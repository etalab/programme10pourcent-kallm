---
title: 3. Infrastructures externalisées pour l'administration
---

## A. Pistes d'infrastructure

Dans beaucoup de cas l'accès à des GPUs est un des principaux freins à l'expérimentaion et la mise en production d'un cas d'usage d'IA générative. L'acquisition d'un cluster GPUs n'est pas toujours une possibilité pour des questions budgétaires ou techniques. Cependant, plusieurs alternatives sont envisageables (ou en cours de construction) à destination des administrations pour externaliser cette infrastructure.

Cette approche est d'ailleurs en phase avec la directive ["cloud au centre"](https://www.legifrance.gouv.fr/download/pdf/circ?id=45446) qui encourage l'utilisation d'infrastructure externalisée pour les projets informatiques de l'administration.

Dans ce cadre, la principale variable a prendre en compte sont les contraintes de sécurité de l'application. Cette question va à la fois déterminer les solutions accessibles et imposer des choix architecturaux. Un exemple d'architecture d'application d'IA générative est donné dans le [rapport de l'ANSSI sur l'IA générative](https://cyber.gouv.fr/sites/default/files/document/Recommandations_de_s%C3%A9curit%C3%A9_pour_un_syst%C3%A8me_d_IA_g%C3%A9n%C3%A9rative.pdf) :

![](../images/anssi_architecture_iagen.png)

Tout ou parti de ces éléments peuvent être externalisé en fonction de la maturité de l'administration, du besoin utilisateur et des contraintes de sécurité.

3 principales solutions d'externalisation sont possibles :

- [Cloud Public](#cloud-public)
- [Cloud externe](#cloud-externe)
- [API inférence](#api-inference)

### a. Cloud Public

Bien que l'état dispose de plusieurs offres cloud internes, la mise à disposition de GPU est encore très peu mature et peu développée

1. [**SSP Cloud**](https://datalab.sspcloud.fr/?lang=fr) : A ce jour, le SSP Cloud via sa plateforme ONYXIA (hébergée et dévelopée par l'INSEE), est la principale plateforme publique mettant à disposition des GPUs à ses utilisateurs. Les ressources sont cependant très limitées et la plateforme est plus orientée autour du développement de projet que de la mise en production. cf [Déploiement d'un LLM sur SSP Cloud](#Déploiement d'un LLM sur SSP Cloud)

2. [**Cloud pi**](https://pi.interieur.rie.gouv.fr/description-de-loffre-cloud/) : Cloud PI est le cloud du ministère de l'intérieur, il ne semble pas proposer à date de provisionnement de GPUs. Il fournit cependant une offre IAAS et PAAS pour l'hébergement d'applications.

3. [**Nubo**](https://portailnubo.dgfip.finances.rie.gouv.fr/) : Nubo est le service cloud du ministère de l'économie et des finances, qui propose un service IAAS. Via sa solution Nubonyxia (implémentation d'Onyxia sur les infrastructures Nubo), il

> Pour définition de ce que recouvre les offres de service PAAS et IAAS, se référer à ce [lien](https://www.redhat.com/fr/topics/cloud-computing/iaas-vs-paas-vs-saas)

### b. Cloud externe

La qualificiation [SecNumCloud](https://cyber.gouv.fr/secnumcloud-pour-les-fournisseurs-de-services-cloud) a été mis en place par l'ANSSI pour assurer des normes de sécurité aux utilisateurs de produits cloud. A ce jour, peu d'entreprises ont acquis cette qualification. Voici quelques exemples de fournisseurs :

1. [Dassault - Outscale](https://fr.outscale.com/) **IAAS** avec accès GPU
2. [Thales - Sens](https://www.s3ns.io/) (Implémentation de GCP sur une infrastructure sécurisée) **PAAS**
3. [Cloud Temple](https://www.cloud-temple.com/) **IAAS **

> Plus d'informations sur ce type de services sont disponibles [ici](https://www.numerique.gouv.fr/services/cloud/cloud-commercial/).

### c. API inférence

1. **API Albert** : Offre fourni à quelques partenaires de la DINUM, avec des capacités d'hébergement limitées.
> (ajouter contact ou moyen d'accès)
2. **Externe non sécurisée** : Ces solutions sont envisageables où les besoins en performance sont importants et les contraintes de sécurité sont faibles. Voici quelques exemples de solutions :
    - [Mistral API](https://mistral.ai/fr/)
    - [Hugging face - inference endpoint](https://huggingface.co/inference-endpoints/dedicated)
    - [Groq](https://wow.groq.com/why-groq/)

> Dans certains cas, il peut être aussi intéressant de mettre en place une architecture hybride Cloud + API d'inférence. Ce qui permet de bénéficier de l'agilité de développement des solutions Cloud, tout en limitant les coûts relatifs à l'approvisionnement de GPUs.

## B. Déploiement d'un LLM sur SSP Cloud

Sur le DataLab SSP Cloud, il est possible de déployer des LLM à des fins d'expérimentation. Plusieurs cas sont possibles :

A. Utiliser des librairies d'API de LLM (vLLM, etc.)
B. Déployer des containers Docker avec Kube et Helm

### a. Déploiement par API

* Vous pouvez lancer un service VSCode avec une GPU et installer une API de LLM

### b. Déploiement par image Docker

* Créer une image Docker et la mettre à disposition (Dockerhub) : exemple applicatif avec Streamlit
* Déployer avec Kube et Helm en utilisant un service VSCode avec les droits d'admin pour Kube

Exemple avec Kubernetes :

```bash
kubectl create deployment mon-deploiement --image=mon-image-docker
```

```bash
kubectl proxy
```

### c. Exemple de déploiements plus complexes par Chart Helm

> Exemple de Caradoc
