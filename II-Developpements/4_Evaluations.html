<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="équipe KALLM">
<meta name="dcterms.date" content="2024-06-07">

<title>PARTIE II. Développements autour des LLMs (pour les data scientists) – Guide d’installation des LLM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../II-Developpements/0_Introduction.html">II-Développements</a></li><li class="breadcrumb-item"><a href="../II-Developpements/4_Evaluations.html">Evaluations</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Guide d’installation des LLM</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accueil</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">I-Accompagnement</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/0_intro.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/1_cas_usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cas d’usage</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/2_Acculturation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acculturation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/3_Impacts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Impacts</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../II-Developpements/0_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">II-Développements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/1_Anatomie_LLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Anatomie et conception d’un modèle de langage</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/2_Utilisation_LLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Techniques d’utilisation d’un LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/3_RAG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Focus sur le RAG (Retrieval Augmented Generation)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/4_Evaluations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Evaluations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">III-Déploiements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/1_Socle_minimal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Socle minimal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/2_Socle_avance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Socle avancé</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/3_Socle_Production.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Socle Production</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/4_Infras_administrations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Infrastructures dans l’administration</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">IV-Exemples</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../IV-Exemples/2_Classification_accords_entreprise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exemple des textes des accords d’entreprise</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Bibliographie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliographie</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#iv.-évaluations-dun-modèle" id="toc-iv.-évaluations-dun-modèle" class="nav-link active" data-scroll-target="#iv.-évaluations-dun-modèle">IV. Évaluations d’un modèle</a>
  <ul class="collapse">
  <li><a href="#objectif" id="toc-objectif" class="nav-link" data-scroll-target="#objectif">1. Objectif</a></li>
  <li><a href="#quelques-concepts-à-connaître" id="toc-quelques-concepts-à-connaître" class="nav-link" data-scroll-target="#quelques-concepts-à-connaître">2. Quelques concepts à connaître</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">3 Datasets</a></li>
  <li><a href="#evaluation-de-modèles-llm-vs.-evaluation-de-systèmes-llm" id="toc-evaluation-de-modèles-llm-vs.-evaluation-de-systèmes-llm" class="nav-link" data-scroll-target="#evaluation-de-modèles-llm-vs.-evaluation-de-systèmes-llm">4 Evaluation de modèles LLM vs.&nbsp;Evaluation de systèmes LLM</a></li>
  <li><a href="#librairies-et-frameworks" id="toc-librairies-et-frameworks" class="nav-link" data-scroll-target="#librairies-et-frameworks">5 Librairies et Frameworks</a></li>
  <li><a href="#méthodologie" id="toc-méthodologie" class="nav-link" data-scroll-target="#méthodologie">6 Méthodologie</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../II-Developpements/0_Introduction.html">II-Développements</a></li><li class="breadcrumb-item"><a href="../II-Developpements/4_Evaluations.html">Evaluations</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">PARTIE II. Développements autour des LLMs (pour les data scientists)</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>équipe KALLM </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="iv.-évaluations-dun-modèle" class="level2">
<h2 class="anchored" data-anchor-id="iv.-évaluations-dun-modèle">IV. Évaluations d’un modèle</h2>
<section id="objectif" class="level3">
<h3 class="anchored" data-anchor-id="objectif">1. Objectif</h3>
<p>Tous les LLM visent le même objectif : maîtriser le langage naturel et par là même, égaler l’humain dans des tâches telles que le résumé, la traduction, la reconnaissance des entités nommées, etc.</p>
<p>Cependant, tous les LLM souffrent des mêmes défauts, de façon plus ou moins prononcée:</p>
<pre><code>* Très grande sensibilité du modèle au prompt utilisé 
* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)
* Les LLM peuvent avoir des comportements inattendus et dangereux suite à l'usage de prompts malveillants, de données 
d'entraînement biaisées, au recours à des agents trop permissifs, etc.</code></pre>
<p>On souhaite donc se doter d’un cadre de comparaison qui permette d’affirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir à différentes métriques pour <strong>mesurer differents aspects</strong> du problème (fiabilité, sécurité, absence de biais…)</p>
<p>Si de nombreux bancs d’essai existent aujourd’hui, permettant de distinguer certains LLM, <strong>il ne faut pas oublier que de bonnes performances dans un banc d’essai ne sont pas suffisantes, et qu’il est primordial de mettre en place un système d’évaluation quasi temps réél du LLM une fois en production.</strong></p>
</section>
<section id="quelques-concepts-à-connaître" class="level3">
<h3 class="anchored" data-anchor-id="quelques-concepts-à-connaître">2. Quelques concepts à connaître</h3>
<p><strong>a) Scenario</strong></p>
<p>Un scénario est un ensemble de conditions dans lesquelles la performance du LLM est évaluée. Il s’agit par exemple de</p>
<ul>
<li>Réponse aux questions</li>
<li>Raisonnement</li>
<li>Traduction</li>
<li>Génération de texte</li>
<li>…</li>
</ul>
<p><strong>b) Tâche</strong></p>
<p>Une tâche constitue une forme plus granulaire d’un scénario. Elle conditionne plus spécifiquement sur quelle base le LLM est évalué. Une tâche peut être une composition de plusieurs sous-tâches.</p>
<ul>
<li><strong>Combinaisons de sous-tâches de difficulté variée</strong></li>
</ul>
<p>Par exemple, l’arithmétique peut être considérée comme une tâche constituée des sous-tâches arithmétique niveau 1er degré, arithmétique niveau collège, arithmétique niveau lycée, etc.</p>
<ul>
<li><strong>Combinaison de sous-tâche de domaines variés</strong></li>
</ul>
<p>La tâche de type QCM peut être vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.</p>
<p><strong>c) Métrique</strong></p>
<p>Une métrique est une mesure qualitative utilisée pour évaluer la performance d’un modèle de langage dans certaines tâches/scénarios. Une métrique peut être :</p>
<ul>
<li>une fonction statistique/mathématique déterministe simple (par exemple, précision ou rappel)</li>
<li>un score produit par un réseau neuronal ou un modèle de Machine Learning (ex. : score BERT)</li>
<li>un score généré à l’aide d’un LLM (ex. : G-Eval)</li>
</ul>
<p>Notons que dans le dernier cas, évaluer un LLM à l’aide d’un LLM peut donner l’impression du serpent qui se mord la queue. Cependant, ce type de ‘dépendances circulaires’ existe couramment et est bien acceptée dans d’autres dommaines. Lors d’un entretien d’embauche par exemple, l’intellect humain évalue le potentiel d’un autre être humain.</p>
<p><strong>d) Benchmarks</strong></p>
<p>Les benchmarks sont des collections standardisées de tests utilisées pour évaluer les LLM sur une tâche ou un scénario donné. On trouvera par exemple :</p>
<ul>
<li><strong>SQuAD</strong> pour le scénario de réponse aux questions de l’utilisateur à partir d’extraction de parties d’un corpus (En anglais)</li>
<li><strong>PIAF</strong> semblable à SQuAD mais en français</li>
<li><strong>IMDB</strong> pour l’analyse des sentiments</li>
<li>…</li>
</ul>
<p>A titre d’exemple, l’image ci-dessous présente le corpus PIAF. Il est composé de paragraphes issus d’articles de Wikipedia, et d’une liste de questions portant sur ces paragraphes.</p>
<p><img src="../images/piaf_example.png" class="img-fluid"></p>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">3 Datasets</h3>
<p>Les performances des modèles peuvent être évaluées qu’en comparaison avec les connaissances existantes. Pour ce faire, il est nécessaire de disposer d’ensembles de données de référence dont les résultats sont connus et vérifiés. Au cours des dernières années, de tels ensembles de données ont été collectés pour un certain nombre d’applications. Pour évaluer les LLM, il existe des “benchmark datasets” qui peuvent être utilisés pour entraîner et pour tester des modèles.</p>
<ul>
<li><strong>CoQA</strong> (Conversational Question Answering) est un set de données avec plus de 127 000 questions-réponses dans 7 domaines sont 5 sont publiques. <a href="https://stanfordnlp.github.io/coqa/">https://stanfordnlp.github.io/coqa/</a> Pour évaluer votre modèle, il suffit de lancer ce script</li>
</ul>
<pre><code>python evaluate-v1.0.py --data-file &lt;chemin_vers_dev-v1.0.json&gt; --pred-file &lt;chemin_vers_predictions&gt;</code></pre>
<ul>
<li><strong>GLUE</strong> (General Language Understanding Evaluation) <a href="https://gluebenchmark.com/">https://gluebenchmark.com/</a> et <em> </em>SuperGLUE** <a href="https://super.gluebenchmark.com/">https://super.gluebenchmark.com/</a> sont des collections des tâches pour évaluer la compréhension du langage naturel. <code>jiant</code> est un PyTorch toolkit qui permet faire cette évaluation. Installez avec <code>pip</code> :</li>
</ul>
<pre><code>pip install jiant</code></pre>
<p>Ici, un exemple d’affinage du modèle RoBERTa sur les données MRPC :</p>
<pre><code>from jiant.proj.simple import runscript as run
import jiant.scripts.download_data.runscript as downloader

EXP_DIR = "/path/to/exp"

# Télécharger les données
downloader.download_data(["mrpc"], f"{EXP_DIR}/tasks")

# Configurer les arguments pour l'API simple
args = run.RunConfiguration(
   run_name="simple",
   exp_dir=EXP_DIR,
   data_dir=f"{EXP_DIR}/tasks",
   hf_pretrained_model_name_or_path="roberta-base",
   tasks="mrpc",
   train_batch_size=16,
   num_train_epochs=3
)

# Lancer
run.run_simple(args)</code></pre>
<ul>
<li><strong>SQuAD</strong> (Stanford Question Answering Dataset) est un set de données pour évaluer la compréhension de la lecture. Il est constitué des questions basées sur un ensemble d’articles de Wikipédia avec 100000 questions avec des réponses, et 50000 questions qui ne peuvent pas être répondues. Les modèles doivent Pour évaluer votre modèle, il suffit de lancer ce script</li>
</ul>
<pre><code>python evaluate-v2.0.py &lt;chemin_vers_dev-v2.0&gt; &lt;chemin_vers_predictions&gt;</code></pre>
<p>Plus d’informations sur Git : <a href="https://github.com/nyu-mll/jiant">https://github.com/nyu-mll/jiant</a></p>
<p><a href="https://github.com/leobeeson/llm_benchmarks">https://github.com/leobeeson/llm_benchmarks</a>.</p>
</section>
<section id="evaluation-de-modèles-llm-vs.-evaluation-de-systèmes-llm" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-de-modèles-llm-vs.-evaluation-de-systèmes-llm">4 Evaluation de modèles LLM vs.&nbsp;Evaluation de systèmes LLM</h3>
<p>L’expression <em>évaluation de LLM</em> peut recouvrir différentes pratiques et différents objectifs. On doit ainsi distinguer l’<strong>évaluations de modèles LLM</strong> de l’<strong>évaluations de systèmes LLM</strong>. Les évaluations de modèles LLM s’intéressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacité sur un ensemble de tâches différentes.</p>
<p>Il existe de nombreux benchmarks qui permettent d’illustrer les performances des modèles sur des aspects précis, comme HellaSwag (qui évalue la capacité d’un LLM à compléter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la véracité des réponses du modèle) et MMLU (qui mesure la capacité de compréhension et de résolution de problèmes ).</p>
<p><img src="../images/llm_model_eval.webp" class="img-fluid"></p>
<p><em>(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)</em></p>
<p>L’évaluation de <strong>systèmes LLM</strong> couvre l’évaluation de tous les composants de la chaîne, pour un modèle donné. En effet, un modèle de LLM est rarement utilisé seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le résultat du modèle. On pourra s’intéresser par exemple à l’effet du prompt sur la politesse de la réponse, le style, le niveau de détail, etc. Un modèle peut également recevoir un contexte (ensemble de documents, tableaux, images…) et son influence doit également être mesurée. On pourrait par exemple s’apercevoir que le modèle produit des résumés de qualité quand on lui fournit des documents littéraires, mais pas des documents techniques.</p>
<p><img src="../images/llm_system_eval.webp" class="img-fluid"></p>
<p><em>(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)</em></p>
<p><strong>En pratique, la comparaison des modèles sur des benchmarks est réalisée par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communauté universitaire. L’évaluation de modèle reste cependant intéressante pour mesurer le gain de performance apporté par un fine-tuning sur un corpus interne par exemple. Cependant, ce sont les évaluations de systèmes LLM qui intéresseront la majorité des équipes souhaitant déployer un LLM dans leur administration.</strong></p>
<section id="détails-des-métriques-utilisées-pour-lévaluation-de-modèles" class="level4">
<h4 class="anchored" data-anchor-id="détails-des-métriques-utilisées-pour-lévaluation-de-modèles">4.1 Détails des métriques utilisées pour l’évaluation de modèles</h4>
<p>Il n’existe pas de réponse simple à la question de savoir quelles métriques utiliser pour évaluer son système LLM. Cela dépendra du type de tâche, de la population cible, de la nature des données, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l’apprentissage machine, on évalue un modèle en se dotant d’un ensemble annoté d’entrées/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes réponses.</p>
<p>La difficulté de l’évaluation en IA générative réside dans le fait que nous ne disposons généralement pas de valeur de référence à laquelle comparer la sortie du modèle. Même dans les cas où l’on disposerait d’un exemple de bonne réponse, les données de sortie étant non structurées (texte en language naturel), il est difficile de comparer la distance entre deux objets.</p>
<p>On reprend ici l’idée de classer les métriques en fonction de leur approche du problème, c’est à dire selon la façon dont elles évaluent la pertinence de la réponse obtenue. Certaines techniques supposent que l’on dispose d’une réponse de référence, et la question est alors de savoir comment elles évaluent la distance entre la réponse obtenue et une réponse de référence. D’autres techniques plus récentes ne font pas cette hypothèse, et cherchent à évaluer la qualité de la réponse dans l’absolu. <img src="../images/schema_metriques.png" class="img-fluid"></p>
<p>On ne détaillera pas toutes les métriques dans le cadre de ce guide, il existe pléthore de documentation disponible sur le sujet (cf.&nbsp;Bibliographie en fin de guide). L’objectif ici est plutôt de fournir une grille d’analyse.</p>
<p><strong>Métriques traditionnelles du machine learning</strong></p>
<p>Dans les métriques classiques, on trouve des métriques générales de classification qui sont couramment utilisées en apprentissage machine et ne sont pas propres aux données textuelles (Accuracy, Precision, Recall, F1…). Ces métriques restent pertinentes pour certaines tâches confiées aux LLM, typiquement l’extraction d’entités nommées. Parmi les classiques, il existe également des métriques spécifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases prédites et les phrases de référence. Le recouvrement peut être calculé au niveau des mots, ou au niveau des lettres. Ces méthodes ont été critiquées pour leur faible corrélation avec le jugement humain, ce qui est n’est pas étonnant dans la mesure où elles ne s’intéressent qu’a la forme de surface du texte.</p>
<p><strong>Introduction de critères sémantiques</strong></p>
<p>Les métriques basées sur le Deep Learning permettent de palier ce problème en introduisant des critères sémantiques. On distingue en premier lieu les métriques qui ont besoin d’une valeur de référence et les autres.</p>
<p>Détenir une valeur de référence peut représenter une grosse contrainte. Tout d’abord, ce n’est pas toujours pertinent; si on demande au LLM d’écrire un poème sur la mer par exemple, il serait vain de chercher à le comparer à un autre poème. Deuxièmement, les textes de références ne sont pas toujours de meilleure qualité que les textes générés par des LLM. Dans le cas du résumé automatique par exemple, [<a href="https://arxiv.org/abs/2007.12626">Fabbri, 2020</a>] illustre les problèmes du dataset CNN/DailyMail [<a href="https://huggingface.co/datasets/abisee/cnn_dailymail">Hermann, 2015</a>] dans lequel les résumés de référence sont pollués par des références et click-baits vers d’autres articles, ou souffrent d’un manque de cohérence suite à la concaténation de résumés sous forme de liste à puces. Ces résumés, évalués par des annotateurs humains, obtiennent parfois de moins bons scores que des résumés générés par LLM. Enfin, même si l’on dispose de références de qualité, il faut s’assurer que la distribution des documents est la même que celle qui sera utilisée en production. Si l’on entraîne un modèle à résumer des articles de presse par exemple, il ne sera pas nécessairement performants sur des documents d’une autre nature. Dès lors se pose la question de comment constituer un dataset d’évaluation pour un modèle de résumé à vocation généraliste.</p>
<p>Toujours est-il que si l’on dispose de valeurs de référence, on peut recourir à des métriques basés sur les embeddings ou des métriques basées sur des modèles fine-tunés.</p>
<p>Les métriques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexité peut les rendre intéressantes. Elles exigent toutes des réponse de référence. On peut citer BERTScore ou MoverScore.</p>
<p>Les métriques basées sur des LLM utilisent un LLM qui joue le rôle de juge pour évaluer les sorties d’un LLM. Elles peuvent fonctionner avec ou sans références, selon des modalités variées (scoring, ranking, classification, réponse à des question fermées, etc.) Ces techniques nécessitent en général de décrire:</p>
<ul>
<li>la tâche confiée au LLM initial (résumé, traduction, …)</li>
<li>les aspects à évaluer (fluidité, factualité, citation des sources…)</li>
<li>eventuellement, les étapes de raisonnement permettant de déterminer le respect des critères</li>
<li>donner quelques exemples</li>
</ul>
<p>Ces méthodes sont aujourd’hui à l’état de l’art pour l’évaluation des LLM, si tant est que l’on utilise des LLM propriétaires comme juge (Constat valable pour le français du moins). L’inconvénient est qu’elles peuvent vite devenir coûteuses, et que le résultat n’est pas forcément reproductible.</p>
<p>Si notre use case n’est pas compatible avec l’usage d’un LLM propriétaire, on peut alors recourir aux métriques basées sur le fine-tuning de LLM de taille moyenne, open-source. Il peut être nécessaire de fine-tuner encore les modèles sur vos propres corpus. De plus, certains de ces modèles ne sont capables de répondre qu’à la question pour laquelle ils ont été entraînés. Par exemple, le modèle <a href="https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model">Lynx</a> (Patronus AI ) est entraîné à détecter les hallucinations dans les environnements RAG. À partir d’un document, d’une question et d’une réponse, le modèle tente d’évaluer si la réponse est fidèle au document. En revanche, il ne peut se prononcer sur d’autres aspects. Le modèle Prometheus-Eval (LG AI Research, KAIST AI) est lui capable de répondre à des questions sur différents aspects, tant que l’utilisateur les définit explicitement dans le prompt.</p>
<p>```code python</p>
<p>rubric_data = { “criteria”:“Is the model proficient in applying empathy and emotional intelligence to its responses when the user conveys emotions or faces challenging circumstances?”, “score1_description”:“The model neglects to identify or react to the emotional tone of user inputs, giving responses that are unfitting or emotionally insensitive.”, “score2_description”:“The model intermittently acknowledges emotional context but often responds without sufficient empathy or emotional understanding.”, “score3_description”:“The model typically identifies emotional context and attempts to answer with empathy, yet the responses might sometimes miss the point or lack emotional profundity.”, “score4_description”:“The model consistently identifies and reacts suitably to emotional context, providing empathetic responses. Nonetheless, there may still be sporadic oversights or deficiencies in emotional depth.”, “score5_description”:“The model excels in identifying emotional context and persistently offers empathetic, emotionally aware responses that demonstrate a profound comprehension of the user’s emotions or situation.” }</p>
<p>```</p>
<p><del><strong>QAG</strong> (Question Answer Generation) Score est un évaluateur qui exploite les forte capacités de raisonnement des LLM pour évaluer de manière fiable les résultats des LLM. Ici, on utilise les réponses à des questions fermées (générées ou prédéfinies) pour calculer un score final. Cette approche est relativement fiable parce qu’elle n’utilise pas les LLM pour générer directement les scores. Par exemple, si vous voulez calculer un score de fidélité (qui mesure si une sortie du LLM a été hallucinée ou non), vous devez :</del></p>
<p><del>* Utiliser un LLM pour extraire toutes les affirmations faites dans une sortie LLM.</del> <del>* Convertir ces affirmations en question.</del> <del>* Pour chaque question, demandez au LLM si la réponse de référence concorde avec l’affirmation faite.</del></p>
<p><del>En novembre 2024, on constate néanmoins que les LLM open-source ne sont pas encore assez puissants pour mener à bien la tâche de découpage du texte en affirmations élémentaires. Si les données à évaluer ne sont pas sensibles, nous conseillons, pour l’heure, de privilégier un LLM propriétaire.</del></p>
</section>
<section id="détails-des-métriques-utilisées-pour-lévaluation-de-systèmes" class="level4">
<h4 class="anchored" data-anchor-id="détails-des-métriques-utilisées-pour-lévaluation-de-systèmes">4.2 Détails des métriques utilisées pour l’évaluation de systèmes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 16%">
<col style="width: 59%">
</colgroup>
<thead>
<tr class="header">
<th>Catégorie</th>
<th>Métriques</th>
<th>Détails</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Métriques d’engagement des utilisateurs et généralités</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Visité</td>
<td>Nombre d’utilisateurs qui ont visité l’application</td>
</tr>
<tr class="odd">
<td></td>
<td>Soumis</td>
<td>Nombre d’utilisateurs qui soumettent des messages</td>
</tr>
<tr class="even">
<td></td>
<td>Répondu</td>
<td>Le LLM génère des réponses sans erreurs</td>
</tr>
<tr class="odd">
<td></td>
<td>Vu</td>
<td>L’utilisateur consulte les réponses du LLM</td>
</tr>
<tr class="even">
<td></td>
<td>Clics</td>
<td>L’utilisateur clique sur la documentation référencée dans la réponse du LLM, le cas échéant</td>
</tr>
<tr class="odd">
<td>Interaction avec l’utilisateur</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Taux d’acceptation</td>
<td>Fréquence d’acceptation des réponses du LLM par l’utilisateur</td>
</tr>
<tr class="odd">
<td></td>
<td>Conversation LLM</td>
<td>Nombre moyen de conversations LLM par utilisateur</td>
</tr>
<tr class="even">
<td></td>
<td>Jours d’activité</td>
<td>Nombre de jours actifs d’utilisation du LLM (par utilisateur)</td>
</tr>
<tr class="odd">
<td></td>
<td>Timing</td>
<td>Durée moyenne entre les prompts et les réponses, et temps consacré à chacune</td>
</tr>
<tr class="even">
<td>Feedback utilisateur et rétention</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Feedback utilisateur</td>
<td>Nombre de réponses avec des commentaires positifs ou négatifs</td>
</tr>
<tr class="even">
<td></td>
<td>Utilisateurs actifs par période</td>
<td>Nombre d’utilisateurs ayant visité l’application LLM au cours d’une période donnée</td>
</tr>
<tr class="odd">
<td></td>
<td>Taux de retour</td>
<td>Pourcentage d’utilisateurs qui ont utilisé cette fonction la semaine/mois précédente et qui continuent à l’utiliser cette semaine/mois.</td>
</tr>
<tr class="even">
<td>Performance</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Requêtes par seconde (Concurrence)</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Tokens par seconde</td>
<td>Compte les tokens générés par seconde lors de la diffusion de la réponse LLM.</td>
</tr>
<tr class="odd">
<td></td>
<td>Délai avant le premier rendu de jeton</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Taux d’erreur</td>
<td>Taux d’erreur pour différents types d’erreurs tels que l’erreur 401, l’erreur 429.</td>
</tr>
<tr class="odd">
<td></td>
<td>Fiabilité</td>
<td>Le pourcentage de demandes satisfaites par rapport au nombre total de demandes, y compris celles qui comportent des erreurs ou des échecs.</td>
</tr>
<tr class="even">
<td></td>
<td>Latence</td>
<td>Durée moyenne du temps de traitement entre la soumission d’une requête et la réception d’une réponse.</td>
</tr>
<tr class="odd">
<td></td>
<td>utilisation GPU/CPU</td>
<td>Utilisation en termes de nombre total de tokens et nombre de code erreur 429 reçus (‘Rate limit reached for requests’ dans l’API OpenAI)</td>
</tr>
<tr class="even">
<td>Coûts</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Coût des appels LLM</td>
<td>Ce qui est facturé par le fournisseur du LLM si vous passez par un LLM hebergé par un tiers</td>
</tr>
<tr class="even">
<td></td>
<td>Coût de l’infrastucture</td>
<td>Coût du stockage, énergie, si vous hébergez votre LLM</td>
</tr>
<tr class="odd">
<td></td>
<td>Coût opérationnel</td>
<td>Coût de la maintenance, du monitoring, de la mise en place des mesures de sécurité, du support, etc si vous hébergez votre LLM</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="librairies-et-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="librairies-et-frameworks">5 Librairies et Frameworks</h3>
<p>A ce jour, il existe des nombreux outils et librairies pour effectuer l’évaluation des modèles LLM. Chaque de ces librairies et frameworks est taillée pour une utilisation de modèle concrète avec des exemples pour vous aider à démarrer l’évaluation de votre modèle.</p>
<p><strong>Hugging Face Transformers</strong> fournissent des API et des outils pour évaluer des modèles pré-entraînés sur différentes tâches en utilisant des métriques telles que la précision, le score F1 ou encore le score BLEU. Ils prennent en charge aussi l’intégration les données de la bibliothèque Hugging Face Datasets. <a href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></p>
<p><strong>Scikit-learn</strong>, c’est un projet Open source avec des librairies principalement axées sur l’apprentissage automatique traditionnel. Elle comprend de nombreux outils de métriques et utilitaires qui peuvent être utilisés pour évaluer les modèles de langage. <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></p>
<p><strong>EvalML</strong> est une bibliothèque spécifique pour l’évaluation des modèles d’apprentissage automatique, y compris les LLM. Elle fournit des métriques, des visualisations et des outils de sélection de modèles. <a href="https://evalml.alteryx.com/en/stable/">https://evalml.alteryx.com/en/stable/</a></p>
<p><strong>NLTK et SpaCy</strong> - ces deux bibliothèques offrent des fonctionnalités pour le traitement du langage naturel et incluent des métriques pour évaluer des tâches telles que la tokenisation, l’analyse syntaxique et l’analyse des sentiments. <a href="https://www.nltk.org/">https://www.nltk.org/</a> <a href="https://spacy.io/">https://spacy.io/</a></p>
<p><strong>AllenNLP</strong> est une bibliothèque conçue pour construire et évaluer des modèles de NLP. Elle fournit des outils pour faciliter la mise en œuvre de mesures d’évaluation et de visualisation personnalisées. <a href="https://docs.allennlp.org/models/main/">https://docs.allennlp.org/models/main/</a></p>
<p><strong>Transformers-Interpret</strong> Une bibliothèque qui se concentre sur l’interprétabilité des modèles, permettant de mieux comprendre les prédictions et les performances des modèles. <a href="https://pypi.org/project/transformers-interpret/0.3.0/">https://pypi.org/project/transformers-interpret/0.3.0/</a></p>
<p><strong>LangChain</strong> est principalement destinée à la construction d’applications avec des LLM. Elle comprend des outils d’évaluation pour évaluer la performance des modèles de langage dans le contexte. <a href="https://www.langchain.com/">https://www.langchain.com/</a></p>
<p><strong>OpenAI Evals</strong> est une boîte à outils d’évaluation de l’OpenAI qui fournit les outils et lignes directrices pour évaluer la performance et la sécurité de leurs modèles. <a href="https://github.com/openai/evals">https://github.com/openai/evals</a></p>
<p>Autres sources : …</p>
<hr>
</section>
<section id="méthodologie" class="level3">
<h3 class="anchored" data-anchor-id="méthodologie">6 Méthodologie</h3>
<p>Un arbre de décision pour l’évaluation des LLM peut vous aider à guider votre processus d’évaluation en fonction de critères et d’objectifs spécifiques de votre modèle et son évaluation. Voici un exemple de tel arbre de décision qui pourrait vous aider en cas de doute. v1 <img src="../images/dev_eval_arbre_decision-v1.png" class="img-fluid"></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/etalab\.github\.io\/programme10pourcent-kallm\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Ce site a été créé avec <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>