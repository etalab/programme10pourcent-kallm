<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="√©quipe KALLM">
<meta name="dcterms.date" content="2024-06-07">

<title>PARTIE II. D√©veloppements autour des LLMs (pour les data scientists) ‚Äì Guide d‚Äôinstallation des LLM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-8ea72dc5fed832574809a9c94082fbbb.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-f7b04f6ffe41e7141457b7c02a5e8ee3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-1c0e6e8b5a1adf3305129ee279cf25e4.min.css" rel="prefetch" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../II-Developpements/0_Introduction.html">II-D√©veloppements</a></li><li class="breadcrumb-item"><a href="../II-Developpements/4_Evaluations.html">Evaluations</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Guide d‚Äôinstallation des LLM</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accueil</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">I-Accompagnement</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/0_Intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/1_cas_usage.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Cas d‚Äôusage</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/2_Acculturation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acculturation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/3_Impacts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Impacts</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../II-Developpements/0_Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">II-D√©veloppements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/1_Anatomie_LLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Anatomie et conception d‚Äôun mod√®le de langage</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/2_Utilisation_LLM.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Techniques d‚Äôutilisation d‚Äôun LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/3_RAG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Focus sur le RAG (Retrieval Augmented Generation)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/4_Evaluations.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Evaluations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../III-Deploiements/0_Introduction_deploiements.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">III-Deploiements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/1_Architecture_projet_llm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="emoji" data-emoji="construction">üöß</span> Architecture d‚Äôun projet LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/2_Service_LLM_avance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="emoji" data-emoji="wrench">üîß</span> Service LLM avanc√©</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/3_Service_LLM_production.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="emoji" data-emoji="factory">üè≠</span> Service LLM √† grande √©chelle</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/4_Deploiement_applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="emoji" data-emoji="computer">üíª</span> D√©ploiement d‚Äôapplications</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">IV-Exemples</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../IV-Exemples/2_Classification_accords_entreprise.qmd" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exemple des textes des accords d‚Äôentreprise</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Bibliographie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliographie</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#√©valuations-dun-mod√®le" id="toc-√©valuations-dun-mod√®le" class="nav-link active" data-scroll-target="#√©valuations-dun-mod√®le">√âvaluations d‚Äôun mod√®le</a>
  <ul class="collapse">
  <li><a href="#objectif" id="toc-objectif" class="nav-link" data-scroll-target="#objectif">Objectif</a></li>
  <li><a href="#quelques-concepts-√†-conna√Ætre" id="toc-quelques-concepts-√†-conna√Ætre" class="nav-link" data-scroll-target="#quelques-concepts-√†-conna√Ætre">Quelques concepts √† conna√Ætre</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  <li><a href="#evaluation-de-mod√®les-llm-vs.-evaluation-de-syst√®mes-llm" id="toc-evaluation-de-mod√®les-llm-vs.-evaluation-de-syst√®mes-llm" class="nav-link" data-scroll-target="#evaluation-de-mod√®les-llm-vs.-evaluation-de-syst√®mes-llm">Evaluation de mod√®les LLM vs.&nbsp;Evaluation de syst√®mes LLM</a></li>
  <li><a href="#librairies-et-frameworks" id="toc-librairies-et-frameworks" class="nav-link" data-scroll-target="#librairies-et-frameworks">Librairies et Frameworks</a></li>
  <li><a href="#m√©thodologie" id="toc-m√©thodologie" class="nav-link" data-scroll-target="#m√©thodologie">M√©thodologie</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../II-Developpements/0_Introduction.html">II-D√©veloppements</a></li><li class="breadcrumb-item"><a href="../II-Developpements/4_Evaluations.html">Evaluations</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>√©quipe KALLM </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="√©valuations-dun-mod√®le" class="level2">
<h2 class="anchored" data-anchor-id="√©valuations-dun-mod√®le">√âvaluations d‚Äôun mod√®le</h2>
<section id="objectif" class="level3">
<h3 class="anchored" data-anchor-id="objectif">Objectif</h3>
<p>Tous les LLM visent le m√™me objectif : ma√Ætriser le langage naturel et par l√† m√™me, √©galer l‚Äôhumain dans des t√¢ches telles que le r√©sum√©, la traduction, la reconnaissance des entit√©s nomm√©es, etc.</p>
<p>Cependant, tous les LLM souffrent des m√™mes d√©fauts, de fa√ßon plus ou moins prononc√©e:</p>
<pre><code>* Tr√®s grande sensibilit√© du mod√®le au prompt utilis√© 
* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)
* Les LLM peuvent avoir des comportements inattendus et dangereux suite √† l'usage de prompts malveillants, de donn√©es 
d'entra√Ænement biais√©es, au recours √† des agents trop permissifs, etc.</code></pre>
<p>On souhaite donc se doter d‚Äôun cadre de comparaison qui permette d‚Äôaffirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir √† diff√©rentes m√©triques pour <strong>mesurer differents aspects</strong> du probl√®me (fiabilit√©, s√©curit√©, absence de biais‚Ä¶)</p>
<p>Si de nombreux bancs d‚Äôessai existent aujourd‚Äôhui, permettant de distinguer certains LLM, <strong>il ne faut pas oublier que de bonnes performances dans un banc d‚Äôessai ne sont pas suffisantes, et qu‚Äôil est primordial de mettre en place un syst√®me d‚Äô√©valuation quasi temps r√©√©l du LLM une fois en production.</strong></p>
</section>
<section id="quelques-concepts-√†-conna√Ætre" class="level3">
<h3 class="anchored" data-anchor-id="quelques-concepts-√†-conna√Ætre">Quelques concepts √† conna√Ætre</h3>
<p><strong>a) Scenario</strong></p>
<p>Un sc√©nario est un ensemble de conditions dans lesquelles la performance du LLM est √©valu√©e. Il s‚Äôagit par exemple de</p>
<ul>
<li>R√©ponse aux questions</li>
<li>Raisonnement</li>
<li>Traduction</li>
<li>G√©n√©ration de texte</li>
<li>‚Ä¶</li>
</ul>
<p><strong>b) T√¢che</strong></p>
<p>Une t√¢che constitue une forme plus granulaire d‚Äôun sc√©nario. Elle conditionne plus sp√©cifiquement sur quelle base le LLM est √©valu√©. Une t√¢che peut √™tre une composition de plusieurs sous-t√¢ches.</p>
<ul>
<li><strong>Combinaisons de sous-t√¢ches de difficult√© vari√©e</strong></li>
</ul>
<p>Par exemple, l‚Äôarithm√©tique peut √™tre consid√©r√©e comme une t√¢che constitu√©e des sous-t√¢ches arithm√©tique niveau 1er degr√©, arithm√©tique niveau coll√®ge, arithm√©tique niveau lyc√©e, etc.</p>
<ul>
<li><strong>Combinaison de sous-t√¢che de domaines vari√©s</strong></li>
</ul>
<p>La t√¢che de type QCM peut √™tre vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.</p>
<p><strong>c) M√©trique</strong></p>
<p>Une m√©trique est une mesure qualitative utilis√©e pour √©valuer la performance d‚Äôun mod√®le de langage dans certaines t√¢ches/sc√©narios. Une m√©trique peut √™tre :</p>
<ul>
<li>une fonction statistique/math√©matique d√©terministe simple (par exemple, pr√©cision ou rappel)</li>
<li>un score produit par un r√©seau neuronal ou un mod√®le de Machine Learning (ex. : score BERT)</li>
<li>un score g√©n√©r√© √† l‚Äôaide d‚Äôun LLM (ex. : G-Eval)</li>
</ul>
<p>Notons que dans le dernier cas, √©valuer un LLM √† l‚Äôaide d‚Äôun LLM peut donner l‚Äôimpression du serpent qui se mord la queue. Cependant, ce type de ‚Äòd√©pendances circulaires‚Äô existe couramment et est bien accept√©e dans d‚Äôautres dommaines. Lors d‚Äôun entretien d‚Äôembauche par exemple, l‚Äôintellect humain √©value le potentiel d‚Äôun autre √™tre humain.</p>
<p><strong>d) Benchmarks</strong></p>
<p>Les benchmarks sont des collections standardis√©es de tests utilis√©es pour √©valuer les LLM sur une t√¢che ou un sc√©nario donn√©. On trouvera par exemple :</p>
<ul>
<li><strong>SQuAD</strong> pour le sc√©nario de r√©ponse aux questions de l‚Äôutilisateur √† partir d‚Äôextraction de parties d‚Äôun corpus (En anglais)</li>
<li><strong>PIAF</strong> semblable √† SQuAD mais en fran√ßais</li>
<li><strong>IMDB</strong> pour l‚Äôanalyse des sentiments</li>
<li>‚Ä¶</li>
</ul>
<p>A titre d‚Äôexemple, l‚Äôimage ci-dessous pr√©sente le corpus PIAF. Il est compos√© de paragraphes issus d‚Äôarticles de Wikipedia, et d‚Äôune liste de questions portant sur ces paragraphes.</p>
<p><img src="../images/piaf_example.png" class="img-fluid"></p>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<p>Les performances des mod√®les peuvent √™tre √©valu√©es qu‚Äôen comparaison avec les connaissances existantes. Pour ce faire, il est n√©cessaire de disposer d‚Äôensembles de donn√©es de r√©f√©rence dont les r√©sultats sont connus et v√©rifi√©s. Au cours des derni√®res ann√©es, de tels ensembles de donn√©es ont √©t√© collect√©s pour un certain nombre d‚Äôapplications. Pour √©valuer les LLM, il existe des ‚Äúbenchmark datasets‚Äù qui peuvent √™tre utilis√©s pour entra√Æner et pour tester des mod√®les.</p>
<ul>
<li><strong>CoQA</strong> (Conversational Question Answering) est un set de donn√©es avec plus de 127 000 questions-r√©ponses dans 7 domaines sont 5 sont publiques. <a href="https://stanfordnlp.github.io/coqa/">https://stanfordnlp.github.io/coqa/</a> Pour √©valuer votre mod√®le, il suffit de lancer ce script</li>
</ul>
<pre><code>python evaluate-v1.0.py --data-file &lt;chemin_vers_dev-v1.0.json&gt; --pred-file &lt;chemin_vers_predictions&gt;</code></pre>
<ul>
<li><strong>GLUE</strong> (General Language Understanding Evaluation) <a href="https://gluebenchmark.com/">https://gluebenchmark.com/</a> et <em> </em>SuperGLUE** <a href="https://super.gluebenchmark.com/">https://super.gluebenchmark.com/</a> sont des collections des t√¢ches pour √©valuer la compr√©hension du langage naturel. <code>jiant</code> est un PyTorch toolkit qui permet faire cette √©valuation. Installez avec <code>pip</code> :</li>
</ul>
<pre><code>pip install jiant</code></pre>
<p>Ici, un exemple d‚Äôaffinage du mod√®le RoBERTa sur les donn√©es MRPC :</p>
<pre><code>from jiant.proj.simple import runscript as run
import jiant.scripts.download_data.runscript as downloader

EXP_DIR = "/path/to/exp"

# T√©l√©charger les donn√©es
downloader.download_data(["mrpc"], f"{EXP_DIR}/tasks")

# Configurer les arguments pour l'API simple
args = run.RunConfiguration(
   run_name="simple",
   exp_dir=EXP_DIR,
   data_dir=f"{EXP_DIR}/tasks",
   hf_pretrained_model_name_or_path="roberta-base",
   tasks="mrpc",
   train_batch_size=16,
   num_train_epochs=3
)

# Lancer
run.run_simple(args)</code></pre>
<ul>
<li><strong>SQuAD</strong> (Stanford Question Answering Dataset) est un set de donn√©es pour √©valuer la compr√©hension de la lecture. Il est constitu√© des questions bas√©es sur un ensemble d‚Äôarticles de Wikip√©dia avec 100000 questions avec des r√©ponses, et 50000 questions qui ne peuvent pas √™tre r√©pondues. Les mod√®les doivent Pour √©valuer votre mod√®le, il suffit de lancer ce script</li>
</ul>
<pre><code>python evaluate-v2.0.py &lt;chemin_vers_dev-v2.0&gt; &lt;chemin_vers_predictions&gt;</code></pre>
<p>Plus d‚Äôinformations sur Git : <a href="https://github.com/nyu-mll/jiant">https://github.com/nyu-mll/jiant</a></p>
<p><a href="https://github.com/leobeeson/llm_benchmarks">https://github.com/leobeeson/llm_benchmarks</a>.</p>
</section>
<section id="evaluation-de-mod√®les-llm-vs.-evaluation-de-syst√®mes-llm" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-de-mod√®les-llm-vs.-evaluation-de-syst√®mes-llm">Evaluation de mod√®les LLM vs.&nbsp;Evaluation de syst√®mes LLM</h3>
<p>L‚Äôexpression <em>√©valuation de LLM</em> peut recouvrir diff√©rentes pratiques et diff√©rents objectifs. On doit ainsi distinguer l‚Äô<strong>√©valuations de mod√®les LLM</strong> de l‚Äô<strong>√©valuations de syst√®mes LLM</strong>. Les √©valuations de mod√®les LLM s‚Äôint√©ressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacit√© sur un ensemble de t√¢ches diff√©rentes.</p>
<p>Il existe de nombreux benchmarks qui permettent d‚Äôillustrer les performances des mod√®les sur des aspects pr√©cis, comme HellaSwag (qui √©value la capacit√© d‚Äôun LLM √† compl√©ter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la v√©racit√© des r√©ponses du mod√®le) et MMLU (qui mesure la capacit√© de compr√©hension et de r√©solution de probl√®mes ).</p>
<p><img src="../images/llm_model_eval.webp" class="img-fluid"></p>
<p><em>(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)</em></p>
<p>L‚Äô√©valuation de <strong>syst√®mes LLM</strong> couvre l‚Äô√©valuation de tous les composants de la cha√Æne, pour un mod√®le donn√©. En effet, un mod√®le de LLM est rarement utilis√© seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le r√©sultat du mod√®le. On pourra s‚Äôint√©resser par exemple √† l‚Äôeffet du prompt sur la politesse de la r√©ponse, le style, le niveau de d√©tail, etc. Un mod√®le peut √©galement recevoir un contexte (ensemble de documents, tableaux, images‚Ä¶) et son influence doit √©galement √™tre mesur√©e. On pourrait par exemple s‚Äôapercevoir que le mod√®le produit des r√©sum√©s de qualit√© quand on lui fournit des documents litt√©raires, mais pas des documents techniques.</p>
<p><img src="../images/llm_system_eval.webp" class="img-fluid"></p>
<p><em>(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)</em></p>
<p><strong>En pratique, la comparaison des mod√®les sur des benchmarks est r√©alis√©e par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communaut√© universitaire. L‚Äô√©valuation de mod√®le reste cependant int√©ressante pour mesurer le gain de performance apport√© par un fine-tuning sur un corpus interne par exemple. Cependant, ce sont les √©valuations de syst√®mes LLM qui int√©resseront la majorit√© des √©quipes souhaitant d√©ployer un LLM dans leur administration.</strong></p>
<section id="d√©tails-des-m√©triques-utilis√©es-pour-l√©valuation-de-mod√®les" class="level4">
<h4 class="anchored" data-anchor-id="d√©tails-des-m√©triques-utilis√©es-pour-l√©valuation-de-mod√®les">D√©tails des m√©triques utilis√©es pour l‚Äô√©valuation de mod√®les</h4>
<p>Il n‚Äôexiste pas de r√©ponse simple √† la question de savoir quelles m√©triques utiliser pour √©valuer son syst√®me LLM. Cela d√©pendra du type de t√¢che, de la population cible, de la nature des donn√©es, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l‚Äôapprentissage machine, on √©value un mod√®le en se dotant d‚Äôun ensemble annot√© d‚Äôentr√©es/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes r√©ponses.</p>
<p>La difficult√© de l‚Äô√©valuation en IA g√©n√©rative r√©side dans le fait que nous ne disposons g√©n√©ralement pas de valeur de r√©f√©rence √† laquelle comparer la sortie du mod√®le. M√™me dans les cas o√π l‚Äôon disposerait d‚Äôun exemple de bonne r√©ponse, les donn√©es de sortie √©tant non structur√©es (texte en language naturel), il est difficile de comparer la distance entre deux objets.</p>
<p>On reprend ici l‚Äôid√©e de classer les m√©triques en fonction de leur approche du probl√®me, c‚Äôest √† dire selon la fa√ßon dont elles √©valuent la pertinence de la r√©ponse obtenue. Certaines techniques supposent que l‚Äôon dispose d‚Äôune r√©ponse de r√©f√©rence, et la question est alors de savoir comment elles √©valuent la distance entre la r√©ponse obtenue et une r√©ponse de r√©f√©rence. D‚Äôautres techniques plus r√©centes ne font pas cette hypoth√®se, et cherchent √† √©valuer la qualit√© de la r√©ponse dans l‚Äôabsolu. <img src="../images/schema_metriques.png" class="img-fluid"></p>
<p>On ne d√©taillera pas toutes les m√©triques dans le cadre de ce guide, il existe pl√©thore de documentation disponible sur le sujet (cf.&nbsp;Bibliographie en fin de guide). L‚Äôobjectif ici est plut√¥t de fournir une grille d‚Äôanalyse.</p>
<p><strong>M√©triques traditionnelles du machine learning</strong></p>
<p>Dans les m√©triques classiques, on trouve des m√©triques g√©n√©rales de classification qui sont couramment utilis√©es en apprentissage machine et ne sont pas propres aux donn√©es textuelles (Accuracy, Precision, Recall, F1‚Ä¶). Ces m√©triques restent pertinentes pour certaines t√¢ches confi√©es aux LLM, typiquement l‚Äôextraction d‚Äôentit√©s nomm√©es. Parmi les classiques, il existe √©galement des m√©triques sp√©cifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases pr√©dites et les phrases de r√©f√©rence. Le recouvrement peut √™tre calcul√© au niveau des mots, ou au niveau des lettres. Ces m√©thodes ont √©t√© critiqu√©es pour leur faible corr√©lation avec le jugement humain, ce qui est n‚Äôest pas √©tonnant dans la mesure o√π elles ne s‚Äôint√©ressent qu‚Äôa la forme de surface du texte.</p>
<p><strong>Introduction de crit√®res s√©mantiques</strong></p>
<p>Les m√©triques bas√©es sur le Deep Learning permettent de palier ce probl√®me en introduisant des crit√®res s√©mantiques. On distingue en premier lieu les m√©triques qui ont besoin d‚Äôune valeur de r√©f√©rence et les autres.</p>
<p>D√©tenir une valeur de r√©f√©rence peut repr√©senter une grosse contrainte. Tout d‚Äôabord, ce n‚Äôest pas toujours pertinent; si on demande au LLM d‚Äô√©crire un po√®me sur la mer par exemple, il serait vain de chercher √† le comparer √† un autre po√®me. Deuxi√®mement, les textes de r√©f√©rences ne sont pas toujours de meilleure qualit√© que les textes g√©n√©r√©s par des LLM. Dans le cas du r√©sum√© automatique par exemple, [<a href="https://arxiv.org/abs/2007.12626">Fabbri, 2020</a>] illustre les probl√®mes du dataset CNN/DailyMail [<a href="https://huggingface.co/datasets/abisee/cnn_dailymail">Hermann, 2015</a>] dans lequel les r√©sum√©s de r√©f√©rence sont pollu√©s par des r√©f√©rences et click-baits vers d‚Äôautres articles, ou souffrent d‚Äôun manque de coh√©rence suite √† la concat√©nation de r√©sum√©s sous forme de liste √† puces. Ces r√©sum√©s, √©valu√©s par des annotateurs humains, obtiennent parfois de moins bons scores que des r√©sum√©s g√©n√©r√©s par LLM. Enfin, m√™me si l‚Äôon dispose de r√©f√©rences de qualit√©, il faut s‚Äôassurer que la distribution des documents est la m√™me que celle qui sera utilis√©e en production. Si l‚Äôon entra√Æne un mod√®le √† r√©sumer des articles de presse par exemple, il ne sera pas n√©cessairement performants sur des documents d‚Äôune autre nature. D√®s lors se pose la question de comment constituer un dataset d‚Äô√©valuation pour un mod√®le de r√©sum√© √† vocation g√©n√©raliste.</p>
<p>Toujours est-il que si l‚Äôon dispose de valeurs de r√©f√©rence, on peut recourir √† des m√©triques bas√©s sur les embeddings ou des m√©triques bas√©es sur des mod√®les fine-tun√©s.</p>
<p>Les m√©triques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexit√© peut les rendre int√©ressantes. Elles exigent toutes des r√©ponse de r√©f√©rence. On peut citer BERTScore ou MoverScore.</p>
<p>Les m√©triques bas√©es sur des LLM utilisent un LLM qui joue le r√¥le de juge pour √©valuer les sorties d‚Äôun LLM. Elles peuvent fonctionner avec ou sans r√©f√©rences, selon des modalit√©s vari√©es (scoring, ranking, classification, r√©ponse √† des question ferm√©es, etc.) Ces techniques n√©cessitent en g√©n√©ral de d√©crire:</p>
<ul>
<li>la t√¢che confi√©e au LLM initial (r√©sum√©, traduction, ‚Ä¶)</li>
<li>les aspects √† √©valuer (fluidit√©, factualit√©, citation des sources‚Ä¶)</li>
<li>eventuellement, les √©tapes de raisonnement permettant de d√©terminer le respect des crit√®res</li>
<li>donner quelques exemples</li>
</ul>
<p>Ces m√©thodes sont aujourd‚Äôhui √† l‚Äô√©tat de l‚Äôart pour l‚Äô√©valuation des LLM, si tant est que l‚Äôon utilise des LLM propri√©taires comme juge (Constat valable pour le fran√ßais du moins). L‚Äôinconv√©nient est qu‚Äôelles peuvent vite devenir co√ªteuses, et que le r√©sultat n‚Äôest pas forc√©ment reproductible.</p>
<p>Si notre use case n‚Äôest pas compatible avec l‚Äôusage d‚Äôun LLM propri√©taire, on peut alors recourir aux m√©triques bas√©es sur le fine-tuning de LLM de taille moyenne, open-source. Il peut √™tre n√©cessaire de fine-tuner encore les mod√®les sur vos propres corpus. De plus, certains de ces mod√®les ne sont capables de r√©pondre qu‚Äô√† la question pour laquelle ils ont √©t√© entra√Æn√©s. Par exemple, le mod√®le <a href="https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model">Lynx</a> (Patronus AI ) est entra√Æn√© √† d√©tecter les hallucinations dans les environnements RAG. √Ä partir d‚Äôun document, d‚Äôune question et d‚Äôune r√©ponse, le mod√®le tente d‚Äô√©valuer si la r√©ponse est fid√®le au document. En revanche, il ne peut se prononcer sur d‚Äôautres aspects. Le mod√®le Prometheus-Eval (LG AI Research, KAIST AI) est lui capable de r√©pondre √† des questions sur diff√©rents aspects, tant que l‚Äôutilisateur les d√©finit explicitement dans le prompt.</p>
<p>```code python</p>
<p>rubric_data = { ‚Äúcriteria‚Äù:‚ÄúIs the model proficient in applying empathy and emotional intelligence to its responses when the user conveys emotions or faces challenging circumstances?‚Äù, ‚Äúscore1_description‚Äù:‚ÄúThe model neglects to identify or react to the emotional tone of user inputs, giving responses that are unfitting or emotionally insensitive.‚Äù, ‚Äúscore2_description‚Äù:‚ÄúThe model intermittently acknowledges emotional context but often responds without sufficient empathy or emotional understanding.‚Äù, ‚Äúscore3_description‚Äù:‚ÄúThe model typically identifies emotional context and attempts to answer with empathy, yet the responses might sometimes miss the point or lack emotional profundity.‚Äù, ‚Äúscore4_description‚Äù:‚ÄúThe model consistently identifies and reacts suitably to emotional context, providing empathetic responses. Nonetheless, there may still be sporadic oversights or deficiencies in emotional depth.‚Äù, ‚Äúscore5_description‚Äù:‚ÄúThe model excels in identifying emotional context and persistently offers empathetic, emotionally aware responses that demonstrate a profound comprehension of the user‚Äôs emotions or situation.‚Äù }</p>
<p>```</p>
<p><del><strong>QAG</strong> (Question Answer Generation) Score est un √©valuateur qui exploite les forte capacit√©s de raisonnement des LLM pour √©valuer de mani√®re fiable les r√©sultats des LLM. Ici, on utilise les r√©ponses √† des questions ferm√©es (g√©n√©r√©es ou pr√©d√©finies) pour calculer un score final. Cette approche est relativement fiable parce qu‚Äôelle n‚Äôutilise pas les LLM pour g√©n√©rer directement les scores. Par exemple, si vous voulez calculer un score de fid√©lit√© (qui mesure si une sortie du LLM a √©t√© hallucin√©e ou non), vous devez :</del></p>
<p><del>* Utiliser un LLM pour extraire toutes les affirmations faites dans une sortie LLM.</del> <del>* Convertir ces affirmations en question.</del> <del>* Pour chaque question, demandez au LLM si la r√©ponse de r√©f√©rence concorde avec l‚Äôaffirmation faite.</del></p>
<p><del>En novembre 2024, on constate n√©anmoins que les LLM open-source ne sont pas encore assez puissants pour mener √† bien la t√¢che de d√©coupage du texte en affirmations √©l√©mentaires. Si les donn√©es √† √©valuer ne sont pas sensibles, nous conseillons, pour l‚Äôheure, de privil√©gier un LLM propri√©taire.</del></p>
</section>
<section id="d√©tails-des-m√©triques-utilis√©es-pour-l√©valuation-de-syst√®mes" class="level4">
<h4 class="anchored" data-anchor-id="d√©tails-des-m√©triques-utilis√©es-pour-l√©valuation-de-syst√®mes">D√©tails des m√©triques utilis√©es pour l‚Äô√©valuation de syst√®mes</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 16%">
<col style="width: 59%">
</colgroup>
<thead>
<tr class="header">
<th>Cat√©gorie</th>
<th>M√©triques</th>
<th>D√©tails</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>M√©triques d‚Äôengagement des utilisateurs et g√©n√©ralit√©s</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Visit√©</td>
<td>Nombre d‚Äôutilisateurs qui ont visit√© l‚Äôapplication</td>
</tr>
<tr class="odd">
<td></td>
<td>Soumis</td>
<td>Nombre d‚Äôutilisateurs qui soumettent des messages</td>
</tr>
<tr class="even">
<td></td>
<td>R√©pondu</td>
<td>Le LLM g√©n√®re des r√©ponses sans erreurs</td>
</tr>
<tr class="odd">
<td></td>
<td>Vu</td>
<td>L‚Äôutilisateur consulte les r√©ponses du LLM</td>
</tr>
<tr class="even">
<td></td>
<td>Clics</td>
<td>L‚Äôutilisateur clique sur la documentation r√©f√©renc√©e dans la r√©ponse du LLM, le cas √©ch√©ant</td>
</tr>
<tr class="odd">
<td>Interaction avec l‚Äôutilisateur</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Taux d‚Äôacceptation</td>
<td>Fr√©quence d‚Äôacceptation des r√©ponses du LLM par l‚Äôutilisateur</td>
</tr>
<tr class="odd">
<td></td>
<td>Conversation LLM</td>
<td>Nombre moyen de conversations LLM par utilisateur</td>
</tr>
<tr class="even">
<td></td>
<td>Jours d‚Äôactivit√©</td>
<td>Nombre de jours actifs d‚Äôutilisation du LLM (par utilisateur)</td>
</tr>
<tr class="odd">
<td></td>
<td>Timing</td>
<td>Dur√©e moyenne entre les prompts et les r√©ponses, et temps consacr√© √† chacune</td>
</tr>
<tr class="even">
<td>Feedback utilisateur et r√©tention</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Feedback utilisateur</td>
<td>Nombre de r√©ponses avec des commentaires positifs ou n√©gatifs</td>
</tr>
<tr class="even">
<td></td>
<td>Utilisateurs actifs par p√©riode</td>
<td>Nombre d‚Äôutilisateurs ayant visit√© l‚Äôapplication LLM au cours d‚Äôune p√©riode donn√©e</td>
</tr>
<tr class="odd">
<td></td>
<td>Taux de retour</td>
<td>Pourcentage d‚Äôutilisateurs qui ont utilis√© cette fonction la semaine/mois pr√©c√©dente et qui continuent √† l‚Äôutiliser cette semaine/mois.</td>
</tr>
<tr class="even">
<td>Performance</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Requ√™tes par seconde (Concurrence)</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Tokens par seconde</td>
<td>Compte les tokens g√©n√©r√©s par seconde lors de la diffusion de la r√©ponse LLM.</td>
</tr>
<tr class="odd">
<td></td>
<td>D√©lai avant le premier rendu de jeton</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>Taux d‚Äôerreur</td>
<td>Taux d‚Äôerreur pour diff√©rents types d‚Äôerreurs tels que l‚Äôerreur 401, l‚Äôerreur 429.</td>
</tr>
<tr class="odd">
<td></td>
<td>Fiabilit√©</td>
<td>Le pourcentage de demandes satisfaites par rapport au nombre total de demandes, y compris celles qui comportent des erreurs ou des √©checs.</td>
</tr>
<tr class="even">
<td></td>
<td>Latence</td>
<td>Dur√©e moyenne du temps de traitement entre la soumission d‚Äôune requ√™te et la r√©ception d‚Äôune r√©ponse.</td>
</tr>
<tr class="odd">
<td></td>
<td>utilisation GPU/CPU</td>
<td>Utilisation en termes de nombre total de tokens et nombre de code erreur 429 re√ßus (‚ÄòRate limit reached for requests‚Äô dans l‚ÄôAPI OpenAI)</td>
</tr>
<tr class="even">
<td>Co√ªts</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Co√ªt des appels LLM</td>
<td>Ce qui est factur√© par le fournisseur du LLM si vous passez par un LLM heberg√© par un tiers</td>
</tr>
<tr class="even">
<td></td>
<td>Co√ªt de l‚Äôinfrastucture</td>
<td>Co√ªt du stockage, √©nergie, si vous h√©bergez votre LLM</td>
</tr>
<tr class="odd">
<td></td>
<td>Co√ªt op√©rationnel</td>
<td>Co√ªt de la maintenance, du monitoring, de la mise en place des mesures de s√©curit√©, du support, etc si vous h√©bergez votre LLM</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="librairies-et-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="librairies-et-frameworks">Librairies et Frameworks</h3>
<p>A ce jour, il existe des nombreux outils et librairies pour effectuer l‚Äô√©valuation des mod√®les LLM. Chaque de ces librairies et frameworks est taill√©e pour une utilisation de mod√®le concr√®te avec des exemples pour vous aider √† d√©marrer l‚Äô√©valuation de votre mod√®le.</p>
<p><strong>Hugging Face Transformers</strong> fournissent des API et des outils pour √©valuer des mod√®les pr√©-entra√Æn√©s sur diff√©rentes t√¢ches en utilisant des m√©triques telles que la pr√©cision, le score F1 ou encore le score BLEU. Ils prennent en charge aussi l‚Äôint√©gration les donn√©es de la biblioth√®que Hugging Face Datasets. <a href="https://huggingface.co/docs/transformers/index">https://huggingface.co/docs/transformers/index</a></p>
<p><strong>Scikit-learn</strong>, c‚Äôest un projet Open source avec des librairies principalement ax√©es sur l‚Äôapprentissage automatique traditionnel. Elle comprend de nombreux outils de m√©triques et utilitaires qui peuvent √™tre utilis√©s pour √©valuer les mod√®les de langage. <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></p>
<p><strong>EvalML</strong> est une biblioth√®que sp√©cifique pour l‚Äô√©valuation des mod√®les d‚Äôapprentissage automatique, y compris les LLM. Elle fournit des m√©triques, des visualisations et des outils de s√©lection de mod√®les. <a href="https://evalml.alteryx.com/en/stable/">https://evalml.alteryx.com/en/stable/</a></p>
<p><strong>NLTK et SpaCy</strong> - ces deux biblioth√®ques offrent des fonctionnalit√©s pour le traitement du langage naturel et incluent des m√©triques pour √©valuer des t√¢ches telles que la tokenisation, l‚Äôanalyse syntaxique et l‚Äôanalyse des sentiments. <a href="https://www.nltk.org/">https://www.nltk.org/</a> <a href="https://spacy.io/">https://spacy.io/</a></p>
<p><strong>AllenNLP</strong> est une biblioth√®que con√ßue pour construire et √©valuer des mod√®les de NLP. Elle fournit des outils pour faciliter la mise en ≈ìuvre de mesures d‚Äô√©valuation et de visualisation personnalis√©es. <a href="https://docs.allennlp.org/models/main/">https://docs.allennlp.org/models/main/</a></p>
<p><strong>Transformers-Interpret</strong> Une biblioth√®que qui se concentre sur l‚Äôinterpr√©tabilit√© des mod√®les, permettant de mieux comprendre les pr√©dictions et les performances des mod√®les. <a href="https://pypi.org/project/transformers-interpret/0.3.0/">https://pypi.org/project/transformers-interpret/0.3.0/</a></p>
<p><strong>LangChain</strong> est principalement destin√©e √† la construction d‚Äôapplications avec des LLM. Elle comprend des outils d‚Äô√©valuation pour √©valuer la performance des mod√®les de langage dans le contexte. <a href="https://www.langchain.com/">https://www.langchain.com/</a></p>
<p><strong>OpenAI Evals</strong> est une bo√Æte √† outils d‚Äô√©valuation de l‚ÄôOpenAI qui fournit les outils et lignes directrices pour √©valuer la performance et la s√©curit√© de leurs mod√®les. <a href="https://github.com/openai/evals">https://github.com/openai/evals</a></p>
<p>Autres sources : ‚Ä¶</p>
<hr>
</section>
<section id="m√©thodologie" class="level3">
<h3 class="anchored" data-anchor-id="m√©thodologie">M√©thodologie</h3>
<p>Un arbre de d√©cision pour l‚Äô√©valuation des LLM peut vous aider √† guider votre processus d‚Äô√©valuation en fonction de crit√®res et d‚Äôobjectifs sp√©cifiques de votre mod√®le et son √©valuation. Voici un exemple de tel arbre de d√©cision qui pourrait vous aider en cas de doute. v1 <img src="../images/dev_eval_arbre_decision-v1.png" class="img-fluid"></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/etalab\.github\.io\/programme10pourcent-kallm\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Ce site a √©t√© cr√©√© avec <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>