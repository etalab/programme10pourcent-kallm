<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="équipe KALLM">
<meta name="dcterms.date" content="2024-06-07">

<title>Guide du LLM – Guide d’installation des LLM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../II-Developpements/1_Revue_Technique_LLM.html">II-Developpements</a></li><li class="breadcrumb-item"><a href="../II-Developpements/1_Revue_Technique_LLM.html">Revue technique des LLM</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Guide d’installation des LLM</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Accueil</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">I-Accompagnement</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/1_Besoins.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Besoins</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/2_Deja_Fait_Admin.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exemples dans l’administration</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/3_Acculturation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acculturation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../I-Accompagnement/4_Impacts.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Impacts</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">II-Developpements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/1_Revue_Technique_LLM.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Revue technique des LLM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/2_RAG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Retrieval Augmented Generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../II-Developpements/3_Evaluations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Evaluations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">III-Deploiements</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/1_Socle_minimal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Socle minimal</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/2_Socle_avance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Socle avancé</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/3_Socle_Production.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Socle Production</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../III-Deploiements/4_Infras_administrations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Infrastructures dans l’administration</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">IV-Exemples</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../IV-Exemples/2_Classification_accords_entreprise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exemple des textes des accords d’entreprise</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Bibliographie.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliographie</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#guide-du-llm" id="toc-guide-du-llm" class="nav-link active" data-scroll-target="#guide-du-llm">Guide du LLM</a>
  <ul class="collapse">
  <li><a href="#partie-ii.-développements-autour-des-llms-pour-les-data-scientists" id="toc-partie-ii.-développements-autour-des-llms-pour-les-data-scientists" class="nav-link" data-scroll-target="#partie-ii.-développements-autour-des-llms-pour-les-data-scientists">PARTIE II. Développements autour des LLMs (pour les data scientists)</a>
  <ul class="collapse">
  <li><a href="#i.-revue-technique-de-létat-de-lart-llm-malo-jérôme" id="toc-i.-revue-technique-de-létat-de-lart-llm-malo-jérôme" class="nav-link" data-scroll-target="#i.-revue-technique-de-létat-de-lart-llm-malo-jérôme">I. Revue technique de l’état de l’art LLM (Malo Jérôme)</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../II-Developpements/1_Revue_Technique_LLM.html">II-Developpements</a></li><li class="breadcrumb-item"><a href="../II-Developpements/1_Revue_Technique_LLM.html">Revue technique des LLM</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Guide du LLM</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>équipe KALLM </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 7, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="guide-du-llm" class="level1">
<h1>Guide du LLM</h1>
<section id="partie-ii.-développements-autour-des-llms-pour-les-data-scientists" class="level2">
<h2 class="anchored" data-anchor-id="partie-ii.-développements-autour-des-llms-pour-les-data-scientists">PARTIE II. Développements autour des LLMs (pour les data scientists)</h2>
<section id="i.-revue-technique-de-létat-de-lart-llm-malo-jérôme" class="level3">
<h3 class="anchored" data-anchor-id="i.-revue-technique-de-létat-de-lart-llm-malo-jérôme">I. Revue technique de l’état de l’art LLM (Malo Jérôme)</h3>
<section id="principe-pré-entraînement-fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="principe-pré-entraînement-fine-tuning">1. Principe pré-entraînement / fine-tuning</h4>
<p>Les LLMs reposent sur un développement en deux voire trois étapes. - <strong>Le pré-entraînement</strong> consiste à entraîner le modèle, en partant de zéro, de façon auto-supervisée, et sur un corpus d’entraînement gigantesque. L’objectif de ce pré-entraînement dépend du type de modèle utilisé (cf.&nbsp;paragraphe suivant), mais la plupart apprennent à prédire le token suivant, à partir d’une suite de tokens. C’est ce qui les rend particulièrement efficaces pour de la génération de texte.</p>
<ul>
<li><p><strong>L’instruction-tuning</strong> permet d’adapter le modèle pré-entraîné à une plus grande diversité de tâches. Dans de nombreux cas (chatbot, résumé de texte, etc.), la prédiction du token suivant n’est pas la bonne stratégie. L’étape d’instruction-tuning permet ainsi, grâce à un entraînement supervisé, de créer une version «&nbsp;chat&nbsp;» du modèle. Pour donner un exemple connu de tous, ChatGPT est la version instruction-tunée de GPT-4.</p></li>
<li><p><strong>Le fine-tuning</strong> (optionnel) peut être utilisé pour adapter le modèle à une tâche et à des données spécifiques. Les LLMs étant des outils multitâches, souvent multilingues et multidomaines, leurs performances peuvent être dégradées lorsqu’il y a des exigences précises et spécifiques. Le fine-tuning est une nouvelle phase d’entraînement supervisé, nécessitant moins de données et de puissance de calcul, qui permet de spécialiser le modèle.</p></li>
</ul>
<p>Par leur taille et les exigences techniques qu’ils impliquent, seules quelques entreprises spécialisées ont les moyens de pré-entraîner et d’instruction-tuner des LLMs. Le fine-tuning, en revanche, peut être abordable pour beaucoup plus d’acteurs, pour peu qu’ils répondent à certaines exigences techniques (cf.&nbsp;partie sur le fine-tuning).</p>
<p>Pour donner des ordres de grandeur, la petite version du dernier modèle de Meta, Llama-3 8B, a été pré-entraîné et instruction-tuné sur un corpus de 15 trillions de tokens. Ces deux phases d’entraînement ont nécessité 1,3 millions d’heures GPU, réparties sur plusieurs milliers de GPU H100.</p>
<ul>
<li><a href="https://www.entrypointai.com/blog/pre-training-vs-fine-tuning-vs-in-context-learning-of-large-language-models/">Article résumant la dualité pré-entraînement/fine-tuning</a></li>
</ul>
</section>
<section id="architectures-principales-llm" class="level4">
<h4 class="anchored" data-anchor-id="architectures-principales-llm">2. Architectures principales LLM</h4>
<section id="a.-larchitecture-transformer" class="level5">
<h5 class="anchored" data-anchor-id="a.-larchitecture-transformer">A. L’architecture Transformer</h5>
<p>Introduite en 2017 dans le papier <strong>Attention Is All You Need</strong>, l’architecture Transformer a révolutionné le domaine du TAL. Par rapport aux RNN, les Transformers permettent un traitement efficace des séquences en parallèle, conduisant à un temps de calcul beaucoup plus court (tant lors de l’entraînement qu’en inférence), tandis que les RNN, par construction, ne peuvent traiter une séquence que séquentiellement, c’est-à-dire token par token. En outre, le mécanisme d’auto-attention, présenté ci-dessous, permet de capturer efficacement les dépendances distantes en atténuant le problème de la disparition et de l’explosion des gradients.</p>
<p><a href="Architecture Transformer"><img src="../images/transformer_architecture.png" width="250"></a></p>
<p>L’auto-attention est le mécanisme central des Transformers. Elle est utilisée pour pondérer, lors de l’examen d’un token en particulier, l’importance, relative à ce token, de chaque autre token de la séquence. Concrètement, trois vecteurs (qui représentent chacun la séquence d’entrée dans un rôle différent) sont déduits de la séquence d’entrée <span class="math inline">\(X\)</span> : les requêtes (<span class="math inline">\(Q\)</span>), les clés (<span class="math inline">\(K\)</span>) et les valeurs (<span class="math inline">\(V\)</span>), par des transformations linéaires comme exprimées dans l’équation suivantes. Les matrices <span class="math inline">\(W_Q\)</span>, <span class="math inline">\(W_K\)</span> et <span class="math inline">\(W_V\)</span> sont des paramètres entraînables du modèle. <span class="math display">\[Q = X \cdot W_Q \qquad K = X \cdot W_K \qquad V = X \cdot W_V \]</span></p>
<p>Les scores d’attention sont ensuite calculés selon l’équation suivante. <span class="math display">\[\text{Attention}(Q, K, V) = \text{softmax} \left( \frac{QK^{T}}{\sqrt{d}} \right) V\]</span></p>
<p>Pour chaque token d’entrée <span class="math inline">\(X_i\)</span>, le résultat <span class="math inline">\(\text{Attention}(Q, K, V)_i\)</span> est une combinaison de tous les autres éléments de la séquence, pondérés selon leur pertinence par rapport à <span class="math inline">\(X_i\)</span>.</p>
<p>L’auto-attention telle que présentée ci-dessus n’est cependant pas directement utilisée dans l’architecture Transformer. A la place, une extension, appelée attention multi-têtes, permet au modèle de capturer plusieurs aspects des relations et des dépendances entre les éléments de la séquence d’entrée. Cela est fait en transformant la séquence d’entrée en plusieurs têtes, i.e.&nbsp;en plusieurs vecteurs de requêtes, de clés et de valeurs, et en appliquant un mécanisme d’auto-attention sur chacune de ces têtes. Les vecteurs d’attention de chaque tête sont ensuite concaténés et réduits linéairement à la taille d’entrée d’origine. Le calcul de l’attention multi-têtes est détaillé dans l’équation suivante.</p>
<p><span class="math display">\[\text{MultiHeadAttention}(Q, K, V) = \text{Concat}(\text{head}_1 , \cdots, \text{head}_h)W^O\]</span></p>
<p>où <span class="math inline">\(\text{head}_i = \text{Attention}(X \cdot W_Q^i, X \cdot W_K^i, X \cdot W_V^i)\)</span> pour <span class="math inline">\(i = 1, \cdot, h\)</span> avec <span class="math inline">\(h\)</span> le nombre de têtes d’attention. Chaque tête d’attention peut donc se spécialiser dans un aspect spécifique des données, et le modèle peut apprendre à combiner ces différents aspects pour une meilleure représentation. La combinaison de ce mécanisme d’attention multi-têtes, de couches de normalisation et de couches à action directes (FNN) forme un bloc Transformer.</p>
<p>Plusieurs blocs (6 dans l’implémentation originale) forment ensuite l’encodeur (qui a accès à la séquence d’entrée dans son intégralité) et le décodeur (qui a accès à la représention encodée de la séquence d’entrée, et à la séquence de sortie générée jusqu’alors). La combinaison de ces deux éléments composent le Transformer encodeur-décodeur original.</p>
<ul>
<li><a href="https://arxiv.org/abs/1706.03762">Papier original <strong>‘Attention Is All You Need’</strong></a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">Explication illustrée et très détaillée</a></li>
</ul>
</section>
<section id="b.-encoder-only-encoder-decoder-decoder-only" class="level5">
<h5 class="anchored" data-anchor-id="b.-encoder-only-encoder-decoder-decoder-only">B. Encoder-only, encoder-decoder, decoder-only</h5>
<p>Les LLMs basés sur des architectures Transformers appartiennent à l’une des 3 catégories suivantes&nbsp;:</p>
<ul>
<li><p><strong>Modèle «&nbsp;encoder-only&nbsp;»</strong>&nbsp;: Ils sont basés uniquement sur la partie décodeur des Transformers. Leur pré-entraînement est souvent basé sur la reconstruction de phrases&nbsp;: à chaque étape, le modèle a accès à une phrase entière, sauf certains mots qui ont été masqués, et apprend à retrouver ces mots masqués. Ces modèles sont adaptés pour des tâches de classification, de reconnaissance d’entités nommées (NER), de réponses aux questions, etc. Ils ont aujourd’hui perdu en popularité, mais leurs représentants les plus connus (BERT, RoBERTa, DistilBERT, CamemBERT, etc.) sont encore très utilisés, et restent un choix intéressant selon la tâche, grâce à leur compréhension fine du langage et à leur petite taille.</p></li>
<li><p><strong>Modèle «&nbsp;decoder-only&nbsp;»</strong>&nbsp;: Ils sont basés uniquement sur la partie décodeur des Transformers. Ces modèles sont aujourd’hui la norme, et l’immense majorité des LLMs actuels utilisent cette architecture. Leur pré-entraînement est basé sur la prédiction du prochain token&nbsp;: à chaque étape, le modèle a accès au début d’une phrase, et apprend à prédire le token suivant. Pour cette raison, ces modèles sont également qualifiés d’«&nbsp;autorégressifs&nbsp;». Les modèles GPT (2, 3, 4), Llama (2, 3), Mistral,&nbsp;Gemini, etc. sont tous des decoder-only.</p></li>
<li><p><strong>Modèle «&nbsp;encoder-decoder&nbsp;»</strong>&nbsp;: Ils utilisent les deux blocs des Transformers. L’encodeur a ainsi accès à l’intégralité de la séquence d’entrée, alors que le décodeur a accès à la représentation cachée de l’entrée et aux tokens générés jusqu’alors. Les modèles les plus connus sont par exemple BART et T5.</p></li>
</ul>
<p>https://medium.com/artificial-corner/discovering-llm-structures-decoder-only-encoder-only-or-decoder-encoder-5036b0e9e88</p>
</section>
<section id="c.-mixture-of-experts-moe" class="level5">
<h5 class="anchored" data-anchor-id="c.-mixture-of-experts-moe">C. Mixture of Experts (MoE)</h5>
<p>Les architectures Mixture of Experts ne sont pas spécifiques aux LLMs, mais elles ont été adaptées avec succès sur des modèles comme Mixtral 8x7B, Mixtral 8x22B ou GPT-4 (supposition). Le principe est de remplacer chaque réseau à propagation directe (présent dans chaque bloc de l’architecture Transformer) par un ensemble de réseaux «&nbsp;experts&nbsp;». Au moment de passer dans cette partie du réseau, un routeur envoie vers un de ces experts uniquement. L’intérêt est double : un seul expert étant utilisé à la fois, le temps d’inférence est naturellement nettement plus court. Par ailleurs, chaque réseau expert est entraîné et donc spécialisé différement des autres : pour un même nombre de paramètres, les performances sont donc supposées être meilleures qu’avec une architecture classique. En revanche, si tous les poids du modèles ne sont pas utilisés systématiquement, c’est uniquement à l’inférence et à chaque couche du réseau que l’expert est choisi : il est donc tout de même nécessaire de charger l’intégralité des poids du modèle en mémoire, ce qui peut être très coûteux en VRAM. Pour une explication plus technique, l’article suivant détaille très bien les MoE en prenant l’exemple de Mixtral.</p>
<p><a href="Une couche d'un réseau MoE"><img src="../images/moe_layer.png" width="500"></a></p>
<p>Explication détaillée des MoE (exemple de Mixtral) : https://huggingface.co/blog/moe</p>
</section>
<section id="d.-nouvelles-architectures-mamba-jamba-etc." class="level5">
<h5 class="anchored" data-anchor-id="d.-nouvelles-architectures-mamba-jamba-etc.">D. Nouvelles architectures : Mamba, Jamba, etc.</h5>
<p>Le principal inconvénient architectural des Transformers est leur complexité quadratique par rapport à la taille de l’entrée (qui vient du calcul quadratique de l’attention). <strong>Mamba</strong> est une architecture récente (Décembre 2023) qui s’affranchit du mécanisme d’attention, au profit de briques SSM (Structured State Space Models). L’intérêt principal de cette architecture est sa complexité linéaire par rapport à la taille de l’entrée.</p>
<p><strong>Jamba</strong> est une nouvelle architecture hybride, à mi-chemin entre le Transformer et Mamba. Cela semble permettre un niveau de performance élevé, une gestion des contextes très longs, un temps d’inférence nettement plus court, et des exigences mémoires bien moindres.</p>
<p>Liens des papiers originaux :</p>
<ul>
<li><a href="https://arxiv.org/abs/2312.00752">Mamba</a></li>
<li><a href="https://arxiv.org/abs/2403.19887">Jamba</a></li>
</ul>
</section>
</section>
<section id="b.-méthodes-de-fine-tuning-conrad" class="level4">
<h4 class="anchored" data-anchor-id="b.-méthodes-de-fine-tuning-conrad">b. Méthodes de fine-tuning (Conrad)</h4>
<p>Les LLM sont des réseaux de neurones de taille importante et font l’objet d’entraînement avec des ressources colossales (<em>e.g</em>: quelques dizaines de milliers de GPUs dernier modèle pendant 3 mois pour <code>GPT-4</code>). L’entraînement permet d’apprendre un jeu de données particulier, en réglant l’ensemble des poids du modèles (<em>e.g</em>: <code>Mixtral 8x22B</code> est une architecture à 141 milliards de poids; 175 milliards pour <code>GPT-3</code>). Les LLM sont entraînés à répondre à plusieurs tâches génériques et ne sont pas forcément pertinent pour des cas d’utilisation particulier.</p>
<p>Pour répondre à ce besoin, plusieurs méthodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste à reprendre un modèle déjà entraîné et à l’adapter sur un jeu de données particulier sur une ou plusieurs tâches spécifiques. En général, il s’agit de modifier une partie ou l’ensemble des poids pour que le modèle soit plus précis pour les tâches voulues. Le fine-tuning garde en grande partie les bénéfices de l’entraînement initial, <em>i.e</em> les connaissances antérieures déjà apprises. Repartir d’un modèle déjà entraîné pourra réduire le temps d’entraînement requis pour le fine-tuning, en fonction de la similarité entre la nouvelle tâche souhaitée et son jeu de données et les entraînements précédents.</p>
<p>Pour des petits modèles de langages, il est possible de ré-entraîner en modifiant l’ensemble des poids. Pour des modèles plus grands, modifier l’ensemble des poids peut s’avérer couteux en temps et en GPUs. Plusieurs approches permettent de ré-entraîner à moindre coût :</p>
<ul>
<li>réentrainer seulement un sous-ensemble de poids</li>
<li>modifier la tête de modélisation de la langue (<code>lm_head</code>) pour certains modèles, soit en réentrainant depuis les poids entraînés, soit en réinitialisant ces poids.</li>
<li>garder l’intégralité du modèle et rajouter des poids à entraîner puis utiliser l’approximation de bas rang avec <code>LORA</code> (<code>Low-Rank Adaptation</code>) pour l’entraînement et l’inférence.</li>
<li>utiliser des versions quantisées, i.e.&nbsp;des modèles où les poids ont été tronqués à une précision inférieure (possibilité de combiner avec la technique précédente, sous le nom de qLORA).</li>
</ul>
<p>Entraînement avec qLORA en pratique :</p>
<p>En plus de la librairie <code>transformers</code> et <code>datasets</code>, les librairies <code>peft</code>, <code>bitsandbytes</code> et <code>trl</code> permettent de simplifier l’entraînement avec qLORA</p>
<p>(inspiré du <a href="https://www.kaggle.com/code/kingabzpro/mistral-7b-instruct-4bit-qlora-fine-tuning">notebook suivant</a> )</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%%</span>capture</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U bitsandbytes</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U transformers</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U peft</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U trl</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U sentencepiece</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install <span class="op">-</span>U protobuf</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForCausalLM, AutoTokenizer,TrainingArguments</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, prepare_model_for_kbit_training, get_peft_model</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> trl <span class="im">import</span> SFTTrainer</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>base_model <span class="op">=</span> <span class="st">"teknium/OpenHermes-2.5-Mistral-7B"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>new_model <span class="op">=</span> <span class="st">"Mistral-7b-instruct-teletravail"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>path_to_training_file<span class="op">=</span><span class="st">"Dataset_public_accords_teletravail_Dares_train.parquet"</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>path_to_test_file<span class="op">=</span><span class="st">"Dataset_public_accords_teletravail_Dares_test.parquet"</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>dataset<span class="op">=</span>load_dataset(<span class="st">"parquet"</span>, data_files<span class="op">=</span>{<span class="st">'train'</span>: path_to_training_file, <span class="st">'test'</span>: path_to_test_file})</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>bnb_config <span class="op">=</span> BitsAndBytesConfig(</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    load_in_4bit<span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_quant_type<span class="op">=</span> <span class="st">"nf4"</span>,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_compute_dtype<span class="op">=</span> torch.bfloat16,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    bnb_4bit_use_double_quant<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        base_model,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        quantization_config<span class="op">=</span>bnb_config,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>model.config.use_cache <span class="op">=</span> <span class="va">False</span> <span class="co"># silence the warnings. Please re-enable for inference!</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>model.config.pretraining_tp <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>model.gradient_checkpointing_enable()</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Load tokenizer</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(base_model, trust_remote_code<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>tokenizer.padding_side <span class="op">=</span> <span class="st">'right'</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>tokenizer.pad_token <span class="op">=</span> tokenizer.eos_token</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>tokenizer.add_eos_token <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>tokenizer.add_bos_token, tokenizer.add_eos_token</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> prepare_model_for_kbit_training(model)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>peft_config <span class="op">=</span> LoraConfig(</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    lora_alpha<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    lora_dropout<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    task_type<span class="op">=</span><span class="st">"CAUSAL_LM"</span>,</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"q_proj"</span>, <span class="st">"k_proj"</span>, <span class="st">"v_proj"</span>, <span class="st">"o_proj"</span>,<span class="st">"gate_proj"</span>]</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, peft_config)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>training_arguments <span class="op">=</span> TrainingArguments(</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    output_dir<span class="op">=</span><span class="st">"./results"</span>,</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    num_train_epochs<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    per_device_train_batch_size<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    gradient_accumulation_steps<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    optim<span class="op">=</span><span class="st">"paged_adamw_32bit"</span>,</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    save_steps<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    logging_steps<span class="op">=</span><span class="dv">25</span>,</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">2e-4</span>,</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">0.001</span>,</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    fp16<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    bf16<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    max_grad_norm<span class="op">=</span><span class="fl">0.3</span>,</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    max_steps<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.03</span>,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>    group_by_length<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    lr_scheduler_type<span class="op">=</span><span class="st">"constant"</span>,</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> SFTTrainer(</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model,</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>    train_dataset<span class="op">=</span>dataset[<span class="st">"train"</span>],</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    peft_config<span class="op">=</span>peft_config,</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>    max_seq_length<span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>    dataset_text_field<span class="op">=</span><span class="st">"text"</span>,</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    args<span class="op">=</span>training_arguments,</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>    packing<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>trainer.train()</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>trainer.model.save_pretrained(new_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="méthodes-de-fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="méthodes-de-fine-tuning">3. Méthodes de fine-tuning</h4>
<section id="a.-fine-tuning-supervisé" class="level5">
<h5 class="anchored" data-anchor-id="a.-fine-tuning-supervisé">A. Fine-tuning supervisé</h5>
<section id="a.-fine-tuning-complet" class="level6">
<h6 class="anchored" data-anchor-id="a.-fine-tuning-complet">a. Fine-tuning complet</h6>
<ul>
<li><a href="https://huggingface.co/docs/transformers/training">Implémentation HuggingFace</a></li>
</ul>
</section>
<section id="b.-fine-tuning-efficace-peft-lora-qlora-dora-etc." class="level6">
<h6 class="anchored" data-anchor-id="b.-fine-tuning-efficace-peft-lora-qlora-dora-etc.">b. Fine-tuning efficace (PEFT) : LoRA, QLoRA, DoRA, etc.</h6>
<p>PEFT = Parameter-Efficient Fine-Tuning | LoRA = Low-Rank Adaptation | QLoRA = Quantized Low-Rank Adaptation | DoRA = Weight-Decomposed Low-Rank Adaptation</p>
<p>Ré-entraîner entièrement un LLM est très coûteux en termes d’infrastructure et de données, et n’est donc pas à la portée de n’importe quelle organisation. Des méthodes « efficaces » ont été créées pour rendre le fine-tuning facilement accessible, dont la plus connue et la plus populaire est LoRA (Low-Rank Adaptation). Son fonctionnement repose sur deux éléments :</p>
<ul>
<li><p><strong>L’adaptation</strong> : Les poids du modèle pré-entraîné sont gelés pendant l’entraînement. Ce sont des poids supplémentaires (ceux de l’adapteur) qui vont être entraînés. Cela permet de garder l’entièreté du modèle pré-entraîné tel quel, et de rajouter uniquement la partie spécifique à chaque tâche. Entre autres, il est ainsi possible, avec un seul modèle de base, d’héberger plusieurs modèles spécialisés à moindre coût. Le papier <a href="https://arxiv.org/abs/2405.00732">LoRA Land</a> explique d’ailleurs comment faire tenir 25 versions de Mistral 7B fine-tunés avec LoRA sur un seul GPU A100.</p></li>
<li><p><strong>Le rang faible</strong> : Les poids additionnels peuvent être choisis de beaucoup de manières. Avec LoRA, certaines couches du modèle (les couches d’attention ou les couches linéaires par exemple) sont sélectionnées, et les poids de ces couches sont exprimés comme une multiplication de deux matrices de rangs faibles, ce qui réduit grandement le nombre de poids à entraîner (la valeur de ce rang étant un hyperparamètre de l’entraînement). En fonction de la valeur de ce rang et des couches sélectionnées, il est ainsi possible d’entraîner uniquement 1 ou 2 % du nombre de paramètres global du modèle pré-entraîné, sans que cela n’affecte trop les performances du fine-tuning.</p></li>
</ul>
<p><a href="Fine-tuning complet VS Fine-Tuning LoRA"><img src="../images/regular_vs_lora_finetuning.png" width="500"></a></p>
<p>D’autres approches de PEFT (Parameter-Efficient Fine-Tuning) ont vu le jour, dont la plupart s’inspirent de LoRA. Parmi les plus connues, QLoRA permet d’appliquer LoRA sur des modèles quantifiés, et DoRA propose un raffinement de l’adapteur de LoRA.</p>
<ul>
<li><a href="https://www.leewayhertz.com/parameter-efficient-fine-tuning/">Guide théorique très clair sur le PEFT (principe, avantages, etc.) avec un focus sur LoRA</a></li>
<li><a href="https://huggingface.co/blog/gemma-peft">Guide pratique / Implémentation HugginFace</a></li>
</ul>
<p>Liens des papiers originaux : - <a href="https://arxiv.org/abs/2106.09685">LoRA</a> - <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> - <a href="https://arxiv.org/abs/2402.09353">DoRA</a></p>
</section>
</section>
<section id="b.-rlhf-et-rlaif" class="level5">
<h5 class="anchored" data-anchor-id="b.-rlhf-et-rlaif">B. RLHF et RLAIF</h5>
<p>Le fine-tuning supervisé est très efficace dans de nombreux cas, mais il présente notamment l’inconvénient de nécessiter une quantité importante de données. La constitution d’une base de questions-réponses attendues par exemple peut se réveler coûteuse. Un autre moyen d’améliorer un modèle est d’utiliser de l’apprentissage par renforcement. La première version utilisée pour ré-entraîner un LLM est le RLHF (Reinforcement Learning from Human Feedback), qui consiste à récolter des retours d’utilisateurs humains (typiquement, entre deux réponses générées par un LLM, l’utilisateur va dire laquelle il préfère), puis à mettre à jour les poids du modèle, par un algorithme d’apprentissage par renforcement, de telle sorte que la réponse préférée par l’utilisateur ait plus de chances d’être générée. Cette approche s’est révélée particulièrement effiace pour «&nbsp;aligner&nbsp;» le modèle aux préférences humaines, en termes de biais, de toxicité, de style, etc.</p>
<p>Bien que la constitution d’une base de retours humains soit moins coûteuse que celle d’une base de questions/réponses, elle reste coûteuse. Une solution aujourd’hui très populaire est de remplacer ces retours humains par des retours générés artificiellement, ce qui donne une approche appelée RLAIF (Reinforcement Learning from Artificial Intelligence Feedback). Typiquement, un LLM plus performant (par exemple GPT-4) va être utilisé pour déterminer la meilleure réponse entre deux ou plusieurs choix, selon des critères donnés. Ce sont ensuite ces retours qui vont être utilisés pour améliorer le modèle grâce à l’algorithme d’apprentissage par renforcement.</p>
<p>RLHF = Reinforcement Learning from Human Feedback | RLAIF = Reinforcement Learning from Artificial Intelligence Feedback</p>
<ul>
<li><a href="https://huggingface.co/blog/rlhf">Introduction au RLHF</a></li>
</ul>
<section id="a.-ppo" class="level6">
<h6 class="anchored" data-anchor-id="a.-ppo">a. PPO</h6>
<p>PPO = Proximal Policy Optimization</p>
<ul>
<li><a href="https://huggingface.co/blog/deep-rl-ppo">Explication théorique</a></li>
<li><a href="https://huggingface.co/docs/trl/main/en/ppo_trainer">Implémentation HuggingFace</a></li>
</ul>
<p>https://medium.com/<span class="citation" data-cites="oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200">@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200</span></p>
</section>
<section id="b.-dpo-kto" class="level6">
<h6 class="anchored" data-anchor-id="b.-dpo-kto">b. DPO, KTO</h6>
<p>DPO = Direct Preference Optimization | KTO = Kahneman-Tversky Optimization</p>
<ul>
<li><a href="https://huggingface.co/blog/pref-tuning">Explication théorique</a></li>
<li><a href="https://huggingface.co/blog/dpo-trl">Guide pratique / Implémentation HugginFace</a></li>
</ul>
<p>Liens des papiers originaux : - <a href="https://arxiv.org/abs/2305.18290">DPO</a> - <a href="https://arxiv.org/abs/2402.01306">KTO</a></p>
</section>
</section>
<section id="c.-fine-tuning-dembeddings" class="level5">
<h5 class="anchored" data-anchor-id="c.-fine-tuning-dembeddings">C. Fine-tuning d’embeddings</h5>
<p>Cf. partie sur la RAG.</p>
</section>
<section id="d.-divers" class="level5">
<h5 class="anchored" data-anchor-id="d.-divers">D. Divers</h5>
<section id="a.-prompt-tuning" class="level6">
<h6 class="anchored" data-anchor-id="a.-prompt-tuning">a. Prompt-tuning</h6>
<ul>
<li><a href="https://arxiv.org/abs/2104.08691">Lien du papier</a></li>
</ul>
</section>
<section id="b.-reft-et-loreft" class="level6">
<h6 class="anchored" data-anchor-id="b.-reft-et-loreft">b. ReFT et LoReFT</h6>
<p>ReFT = Representation Fine-Tuning | LoReFT = Low-Rank Linear Subspace ReFT</p>
<ul>
<li><a href="https://arxiv.org/abs/2404.03592">Lien du papier</a></li>
</ul>
</section>
</section>
</section>
<section id="prompt-engineering" class="level4">
<h4 class="anchored" data-anchor-id="prompt-engineering">4. Prompt engineering</h4>
<section id="a.-bonnes-pratiques" class="level5">
<h5 class="anchored" data-anchor-id="a.-bonnes-pratiques">A. Bonnes pratiques</h5>
<p>Il faut avant tout garder à l’esprit que le prompt engineering est une discipline très empirique, qui demande beaucoup d’itérations pour obtenir le meilleur prompt par rapport au résultat souhaité. Bien qu’il n’existe pas de méthode systématique et efficace pour optimiser un prompt, certaines pratiques sont devenues la norme. Par exemple, voici quelques bonnes pratiques : - <strong>Donner un rôle au modèle</strong> : Par exemple, dire au modèle qu’il est un magistrat honnête et impartial pourra l’aider à générer du texte formel, neutre et juridique. Le rôle est bien sûr à adapter en fonction des exigences de chaque tâche. - <strong>Structurer le prompt</strong> : Il est important de bien différencier le <em>prompt système</em> du <em>prompt utilisateur</em>. Le premier donnera des instructions générales quant au style, à la tâche, au contexte, etc., alors que le second pourra donner des instructions spécifiques ou un texte à analyser. Il est également pertinent d’organiser ou de séparer clairement les instructions. - <strong>Etre le plus précis possible</strong> : - <strong>Contraindre le modèle au maximum</strong> : - <strong>Donner des exemples</strong> : Cf. paragraphe suivant.</p>
<p>Le papier <a href="https://arxiv.org/abs/2312.16171">Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4</a> donne un certains nombre de principes pour améliorer les prompts. Parmi ces principes (très nombreux), on trouve par exemple : - Ne pas etre poli avec le LLM si l’on souhaite une réponse concise. - Décrire l’audience souhaitée dans le prompt (des experts techniques, des enfants, etc.). - Utiliser des directives affirmatives (fais ceci), et éviter les tournures négatives (ne fais pas cela). - Employer des phrases telles que ‘Ta tache est de’ ou ‘Tu DOIS’. - Répéter plusieurs fois certains mots ou phrases essentielles.</p>
</section>
<section id="b.-0-shot-1-shot-few-shot-prompting" class="level5">
<h5 class="anchored" data-anchor-id="b.-0-shot-1-shot-few-shot-prompting">B. 0-shot, 1-shot, few-shot prompting</h5>
<p>La façon la plus intuitive d’adresser une requête à un LLM est de formuler des instructions les plus précises possibles. Ce faisant, on espère que le modèle comprendra ces instructions et répondra en conséquence. Pour des tâches nouvelles, auxquelles le modèle n’a pas nécessairement été confronté durant son (pré)-entraînement, on appelle cette méthode du 0-shot prompting : le modèle n’a pas de référence ou d’exemple de réponse attendue.</p>
<p>Pour pallier ce manque de référence, il est possible (et, en fonction de la tâche, souvent recommandé) d’ajouter des exemples de paires entrée/sortie dans le prompt que l’on adresse au modèle : cela donne du 1-shot (un exemple) ou du few-shot (plusieurs exemples) prompting. Plus les exemples sont proches de la requête initiale, plus le modèle saura précisément comment répondre. Cela permet ainsi au modèle de s’adapter, à moindre coût, à une tâche très spécifique ou particulière.</p>
<ul>
<li><a href="https://www.prompthub.us/blog/the-few-shot-prompting-guide">Guide pratique (avec exemples)</a></li>
</ul>
</section>
<section id="c.-chain-of-thought-cot-reasoning" class="level5">
<h5 class="anchored" data-anchor-id="c.-chain-of-thought-cot-reasoning">C. Chain of Thought (CoT) reasoning</h5>
<p>Sur certaines tâches qui demandent un raisonnement (par exemple la résolution d’un problème mathématique simple), les LLM naturellement ne sont pas très bons. Pour augmenter leurs capacités de raisonnement, une stratégie classique consiste à leur demander de raisonner et de réfléchir étape par étape.</p>
<p>Les modèles les plus récents ayant nettement progressé en raisonnement, il est possible qu’ils raisonnent naturellement étape par étape sur des questions simples. Pour des questions ou des raisonnements plus complexes, il sera cependant probablement plus efficace de proposer une logique de raisonnement au modèle, en explicitant les différentes étapes.</p>
<p>Il est également possible de combiner le CoT reasoning avec du few-shot prompting, <em>i.e.</em> de donner des exemples de raisonnement étape par étape au modèle.</p>
<ul>
<li><a href="https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting">Guide détaillé</a></li>
</ul>
</section>
<section id="d.-rag" class="level5">
<h5 class="anchored" data-anchor-id="d.-rag">D. RAG</h5>
<p>RAG = Retrieval Augmented Generation</p>
<p>Le principe est de rajouter du contexte dans le prompt du LLM, pour lui donner accès à des données spécifiques et pertinentes. Cf. partie sur la RAG.</p>
</section>
<section id="e.-reverse-prompt-engineering" class="level5">
<h5 class="anchored" data-anchor-id="e.-reverse-prompt-engineering">E. Reverse prompt engineering</h5>
<p>Une façon de travailler ses prompts est de profiter des capacités génératives des LLMs pour leur faire créer des prompts. L’idée est de donner au LLM un exemple de sortie souhaitée, et de lui demander de générer le prompt le plus adapté possible pour produire cette sortie.</p>
<ul>
<li><a href="https://bootcamp.uxdesign.cc/why-reverse-prompt-engineering-is-the-magic-key-to-production-ready-prompts-9d4c2c5b2e8b">Guide pratique</a></li>
</ul>
</section>
</section>
<section id="quoi-faire-quand" class="level4">
<h4 class="anchored" data-anchor-id="quoi-faire-quand">5. Quoi faire quand ?</h4>
<section id="a.-utiliser-un-llm" class="level5">
<h5 class="anchored" data-anchor-id="a.-utiliser-un-llm">A. Utiliser un LLM</h5>
<p>La première question à se poser est la nécessité ou non d’utiliser un LLM. Certaines tâches peuvent se résoudre avec un LLM, mais ce n’est pas toujours la solution la plus pertinente. Par exemple, un LLM est normalement capable de parser un fichier xml sans problème, mais un script naïf sera largement aussi efficace, à bien moindre coût (environnemental, humain, financier). L’utilisation d’un LLM doit venir d’un besoin de compréhension fine du langage naturel.</p>
<p><strong>Donner quelques exemples de cas d’usages</strong></p>
</section>
<section id="b.-quels-modèles-utiliser" class="level5">
<h5 class="anchored" data-anchor-id="b.-quels-modèles-utiliser">B. Quel(s) modèle(s) utiliser</h5>
<p>Beaucoup d’éléments sont à prendre en compte lors du choix du modèle à utiliser. Parmi les plus importants&nbsp;:</p>
<ul>
<li><p><strong>Sa taille</strong> : Exprimée généralement en milliards (B) de paramètres (Llama-3 8B possède 8 milliards de paramètres, Mistral 7B en possède 7 milliards, etc.), elle influe fortement sur les performances du modèles et les exigences techniques. Un «&nbsp;petit&nbsp;» LLM de 8 milliards de paramètres pourra tourner sur un GPU modeste avec une VRAM de 32 GB (voire moins si l’on utilise un modèle quantifié, cf.&nbsp;…), tandis qu’un LLM de taille moyenne de 70 milliards de paramètres nécessitera 2 GPU puissants avec 80 GB de VRAM.</p></li>
<li><p><strong>Son multilinguisme</strong>&nbsp;: La plupart des modèles sont entraînés sur une immense majorité de données anglaises (plus de 90&nbsp;% pour Llama-2, contre moins de 0,1&nbsp;% de données françaises). Les modèles incluant plus de français (Mistral ?) dans leurs données d’entraînement sont naturellement plus efficaces sur du français.</p></li>
<li><p><strong>Son temps d’inférence</strong>&nbsp;: Généralement directement lié à la taille du modèle, certaines architectures (MoE) permettent cependant d’avoir un temps d’inférence plus court.</p></li>
<li><p><strong>Ses performances générales</strong>&nbsp;: Beaucoup de benchmarks publics évaluent les LLMs sur des tâches généralistes et variées. Un bon point de départ est de regarder <a href="https://chat.lmsys.org/?leaderboard">le Leaderboard</a> qui recense la plupart des modèles connus.</p></li>
<li><p><strong>Ses performances spécifiques</strong>&nbsp;: Les benchmarks généralistes ne sont pas forcément pertinents pour certains cas d’usages, car ils ne sont pas spécifiques à la tâche, aux données, etc. Il peut être intéressant de développer un pipeline d’évaluation spécifique (cf…).</p></li>
</ul>
<p>En juin 2024, un bon point de départ est de regarder les modèles open-source de Meta (Llama-2 7B/13B/70B, Llama-3 8B/70B) et de Mistral AI (Mistral 7B, Mixtral 8x7B).</p>
</section>
<section id="c.-quand-faire-du-prompt-engineering" class="level5">
<h5 class="anchored" data-anchor-id="c.-quand-faire-du-prompt-engineering">C. Quand faire du prompt engineering</h5>
<p>Si vous êtes dans l’un des cas suivants, le prompt engineering peut être une bonne option :</p>
<ul>
<li>Pas beaucoup de ressources disponibles</li>
<li>Besoin d’un outil laissé à la disposition des utilisateurs, avec une grande liberté</li>
<li>Les réponses requises sont très formattées ou très spécifiques</li>
</ul>
</section>
<section id="d.-quand-faire-de-la-rag" class="level5">
<h5 class="anchored" data-anchor-id="d.-quand-faire-de-la-rag">D. Quand faire de la RAG</h5>
<p>Si vous êtes dans l’un des cas suivants, la RAG peut être une bonne option :</p>
<ul>
<li>Besoin de réponses à jour, régulièrement et facilement actualisées</li>
<li>Besoin de sourcer les réponses ou de diminuer les hallucinations</li>
<li>Besoin d’enrichir les réponses avec des données spécifiques</li>
<li>Besoin d’une application qui ne dépend pas d’un modèle spécifique (généralisabilité), et dont les utilisateurs ne connaissent pas l’IA générative</li>
</ul>
</section>
<section id="e.-quand-faire-du-fine-tuning" class="level5">
<h5 class="anchored" data-anchor-id="e.-quand-faire-du-fine-tuning">E. Quand faire du fine-tuning</h5>
<p>Si vous êtes dans l’un des cas suivants, le fine-tuning peut être une bonne option :</p>
<ul>
<li>Besoin d’une terminologie ou d’un style spécifique</li>
<li>Besoin d’enrichir les réponses avec des données spécifiques</li>
<li>Ressources (GPU, data scientists) disponibles</li>
<li>Données disponibles en quantité et qualité suffisantes</li>
<li>Besoin d’une application qui ne dépend pas d’un modèle spécifique (généralisabilité), et dont les utilisateurs ne connaissent pas l’IA générative</li>
</ul>
</section>
<section id="f.-combiner-plusieurs-techniques" class="level5">
<h5 class="anchored" data-anchor-id="f.-combiner-plusieurs-techniques">F. Combiner plusieurs techniques</h5>
<p>RAG + fine-tuning = <a href="https://arxiv.org/abs/2403.10131">RAFT</a></p>


</section>
</section>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/etalab\.github\.io\/programme10pourcent-kallm\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Ce site a été créé avec <a href="https://quarto.org/">Quarto</a></p>
</div>
  </div>
</footer>




</body></html>