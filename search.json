[
  {
    "objectID": "IV-Exemples/2_Classification_accords_entreprise.html",
    "href": "IV-Exemples/2_Classification_accords_entreprise.html",
    "title": "Classification des thématiques des accords d’entreprise",
    "section": "",
    "text": "Les accords d’entreprise sont des textes négociés entre représentant du personnel et direction, traitant d’une ou plusieurs thématiques. La majeure partie des textes sont publiées sur Légifrance et sont de longueurs variables, d’une page à plusieurs dizaines de pages. Les parties prenantes déposent le texte sur la plateforme numérique TéléAccords auprès de la Direction Générale du Travail en renseignant les thématiques traités dans les textes, parmi une cinquaine de thématique à sélectionner. Ces thématiques déclarées sont ensuite vérifiées et corrigées par l’administration.",
    "crumbs": [
      "IV-Exemples",
      "Exemple des textes des accords d'entreprise"
    ]
  },
  {
    "objectID": "IV-Exemples/2_Classification_accords_entreprise.html#les-accords-dentreprise",
    "href": "IV-Exemples/2_Classification_accords_entreprise.html#les-accords-dentreprise",
    "title": "Classification des thématiques des accords d’entreprise",
    "section": "",
    "text": "Les accords d’entreprise sont des textes négociés entre représentant du personnel et direction, traitant d’une ou plusieurs thématiques. La majeure partie des textes sont publiées sur Légifrance et sont de longueurs variables, d’une page à plusieurs dizaines de pages. Les parties prenantes déposent le texte sur la plateforme numérique TéléAccords auprès de la Direction Générale du Travail en renseignant les thématiques traités dans les textes, parmi une cinquaine de thématique à sélectionner. Ces thématiques déclarées sont ensuite vérifiées et corrigées par l’administration.",
    "crumbs": [
      "IV-Exemples",
      "Exemple des textes des accords d'entreprise"
    ]
  },
  {
    "objectID": "IV-Exemples/2_Classification_accords_entreprise.html#notre-projet",
    "href": "IV-Exemples/2_Classification_accords_entreprise.html#notre-projet",
    "title": "Classification des thématiques des accords d’entreprise",
    "section": "Notre projet",
    "text": "Notre projet\nL’administration souhaite automatiser et fiabiliser ce processus, pour simplifier la démarche d’enregistrement des textes et alléger la charge administrative. Une particularité notable des textes des accords d’entreprises est la longueur variable du texte, allant d’un texte succinct à un document long. Ces documents sont rédigés par les entreprises selon leur propre charte graphique et rédactionnelle.\nPlusieurs expérimentations et projets dans l’administration sont confrontés à des problématiques similaires (classification, analyse de sentiments, recherche d’informations parmi une base documentaire), mais aucun ne propose une solution publique sur des documents longs.\nEn termes d’intégration, la solution devra pouvoir être prise en main par les équipes métiers de l’administration, avec une formation à l’outil proposé.\nEn termes d’impact, la solution devra répondre au besoin de manière légale et sécurisée, tout en ayant un impact environnemental limité",
    "crumbs": [
      "IV-Exemples",
      "Exemple des textes des accords d'entreprise"
    ]
  },
  {
    "objectID": "IV-Exemples/2_Classification_accords_entreprise.html#notre-solution",
    "href": "IV-Exemples/2_Classification_accords_entreprise.html#notre-solution",
    "title": "Classification des thématiques des accords d’entreprise",
    "section": "Notre solution",
    "text": "Notre solution\nPour répondre au besoin, une solution propose ici les différentes étapes :\n\nRécupérer les données\nChoisir et réutiliser un modèle (avec RAG)\nEvaluer les performances du modèles\nMettre en production\n\n\nRécupération des données\nLes données sont disponibles sur Légifrance. Le stock des textes est également publié par le FTP de la DILA et les thématiques déclarées sont à la fois dans les métadonnées XML publié conjointement avec les textes, ou retrouvables sur Légifrance.\nPour des raisons pratiques, nous travaillerons avec une photographie du stock au 1er semestre 2024 et sur un échantillon des 1000 textes d’accords, convertis au format parquet.\nCes données comportent le numéro de dossier de l’accord, identifiant unique, puis le texte et les thématiques déclarées, et enfin suivies des thématiques une à une :\n\n\n\n\n\n\n\n\n\ntexte\ntheme\naccord_methode_penibilite\n...\n\n\nnumdossier\n\n\n\n\n\n\n\n\nT02120002618\nACCORD D’ENTREPRISE FONDANT \\nLE COMITÉ SOCIAL...\nAutre, précisez ;\nFalse\n...\n\n\n\n\n\n\n\n\n\nChoisir et réutiliser un LLM\nA date d’écriture de ce guide, Llama3.1 est un modèle offrant des performances correctes et peut tourner sur la plupart des GPU du marché. Nous utiliserons dans cette exemple Llama3.1, tout en gardant à l’esprit que la méthodologie reste valable pour d’autres modèles de différentes tailles.\nCompte tenu de la variabilité de la longueur du texte et afin d’être économe dans l’utilisation des LLM, le choix a été porté sur une statégie de Retrieval Augmented Generation. Pour ce faire, nous utiliserons les librairies Langchain pour l’implémentation du RAG, ChromaDB pour la vectorisation, Ollama pour déployer un LLM en mode API.\nNous reproduisons ici pas à pas la solution. Le notebook complet se trouve ici\n\nDéployer un LLM avec Ollama\nPour déployer un LLM en local avec Ollama, il faut au préalable installer Ollama, télécharger et lancer le modèle\nEn Linux/Unix :\ncurl -fsSL https://ollama.com/install.sh | sh\nUne fois installé, il faut lancer le serveur Ollama\nollama serve&\nPuis lancer le modèle choisi, ici llama3.1\nollama run llama3.1\nCette dernière commande vous propose un prompt et vous pouvez intéragir avec le modèle en ligne de commande. Dans la suite de cet exemple, nous allons utiliser python pour automatiser nos requêtes vers le LLM à partir de nos données. Mais avant cela, nous allons lire les données et nous devons vectoriser nos textes par morceaux, afin de pouvoir sélectionner les meilleurs morceaux adéquats à la tâche demandé.\n\n\nLecture de données\nVeuillez installer les librairies suivantes\npandas==2.2.2\npyarrow==17.0.0\nollama==0.3.3\nsentence_transformers==3.1.0\nlangchain==0.2.16\nlangchain-community==0.2.16\nlangchain-huggingface==0.0.3\nlangchain-text-splitters==0.2.4\nchromadb==0.5.3\nlangchain-chroma==0.1.3\njupyter==1.1.1\nipykernel==6.29.5\nNous allons dans cet exemple, extraire 10 textes pour des raisons de rapidité :\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport requests\n\nfrom langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\nfrom langchain.llms import Ollama, BaseLLM\nfrom langchain.schema import Document, Generation, LLMResult\nfrom langchain.vectorstores import Chroma\nfrom langchain_chroma import Chroma\nfrom langchain_community.llms import OpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom glob import glob\n\ndf_sample=pd.read_parquet(\"./10p_accords_publics_et_thematiques_240815_sample_of_1000.parquet\")\ndf_sample=df_sample[:10]\n\n\n\nVectoriser nos textes avec ChromaDB\nPour vectoriser nos textes, nous utilisons ChromaDB qui s’intègre avec Langchain. Nous allons découper en morceau des 3000 caractères à chaque saut à ligne, ce qui correspond à un paragraphe. Les morceaux de textes, ici paragraphes, sont stockés dans une boutique de vecteur avec le numéro de dossier et le numéro de paragraphe en métadonnées.\n\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\\n\",\n    chunk_size=3000,\n    chunk_overlap=200,\n    length_function=len,\n    is_separator_regex=False,\n)\n\nmodel_kwargs = {'device': 'cuda'}\nembedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\", model_kwargs=model_kwargs,show_progress=False)\n\nvector_store = Chroma(embedding_function=embedder, persist_directory=\"./chroma_db\")\nfor index, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n    text=df_sample.texte[index]\n    texts = text_splitter.create_documents([text])\n    i=0\n    for t in texts:\n        t.metadata[\"id\"]=f\"{index}_{i}\"\n        t.metadata[\"index\"]=f\"{index}\"\n        vector_store.add_documents([t])\n        i+=1\n\n\n\nInterroger un LLM en mode API\nPour interroger le LLM, nous construisons une classe qui permet de générer les requêtes et de traiter les réponses :\n\nMODEL=\"llama3.1\"\n\n\nclass LocalOllamaLLM(BaseLLM):\n    api_url : str\n    def _generate(self, prompt, stop):\n        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": MODEL , \"prompt\": str(prompt) })\n        response.raise_for_status()\n        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n        generations=[]\n        generations.append([Generation(text=response_text)])\n        return LLMResult(generations=generations)\n\n\n    def _llm_type(self):\n        return \"local\"  \n\n    llm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n\nNous définissons également un prompt de base, améliorable par la suite, et une chaîne LangChain entre le prompt et le LLM :\n\nsystem_prompt = (\n    \" Répondez à la question posée \"\n    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question \"\n    \" Si la réponse ne se trouve pas dans le contexte, répondez par 'Non'\"\n    \" Contexte : {context}  \"\n)\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        (\"human\", \"{input}\"),\n    ]\n)\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\n\nNous définissons une fonction pour effectuer le RAG, avec à la fois la recherche de similarité par rapport à la question, et la soumission augmentée pour une réponse du LLM :\n\ndef search_and_invoke_llm(vector_store,index,query,k=5):\n    if k==0:\n        print(f\"bug with {index}\")\n        return None\n    else:\n        pass\n    try:\n        retriever=vector_store.as_retriever(\n        search_kwargs={\n                \"k\": k, \n                \"filter\": {'index': index}\n            }\n        )\n        chain = create_retrieval_chain(retriever, question_answer_chain)\n        result=chain.invoke({\"input\": query})\n        return result\n    except:\n        search_and_invoke_llm(vector_store,index,query,k=k-1)\n    return None\n\n\n\nAutomatiser la classification sur l’ensemble des thématiques\nNous automatisons ici la classification sous forme de classification binaire pour chaque thématique, en posant une question “oui ou non” et en inférant oui si la réponse commence par oui, non sinon.\n\nTHEMATIQUES={\n    \"accord_methode_penibilite\":\"Accords de méthode (pénibilité)\",\n\"accord_methode_pse\":\"Accords de méthode (PSE)\",\n\"amenagement_temps_travail\":\"Aménagement du temps de travail (modulation, annualisation, cycles)\",\n\"autres\":\"Autre, précisez\",\n\"autres_condition_travail\":\"Autres dispositions de conditions de travail (CHSCT, médecine du travail, politique générale de prévention)\",\n\"autres_dispositions_duree\":\"Autres dispositions durée et aménagement du temps de travail \",\n\"autres_dispositions_egalite\":\"Autres dispositions Egalité professionnelle\",\n\"autres_dispositions_emploi\":\"Autres dispositions emploi\",\n\"calendrier_negociation\":\"Calendrier des négociations\",\n\"classifications\":\"Classifications\",\n\"commision_paritaire\":\"Commissions paritaires\",\n\"cet\":\"Compte épargne temps\",\n\"couverture_complementaire\":\"Couverture complémentaire santé - maladie\",\n\"don_jour\":\"Dispositifs don de jour et jour de solidarité\",\n\"distribution_actions_gratuites\":\"Distribution d'actions gratuites\",\n\"droit_deconnexion\":\"Droit à la déconnexion et outils numériques\",\n\"droit_syndical\":\"Droit syndical, IRP, expression des salariés\",\n\"duree_collective_temps_travail\":\"Durée collective du temps de travail\",\n\"egalite_salariale\":\"Egalité salariale F/H\",\n\"election_pro\":\"Elections professionnelles, prorogations de mandat et vote électronique\",\n\"evolution_prime\":\"Evolution des primes\",\n\"evolution_salariale\":\"Evolution des salaires (augmentation, gel, diminution)\",\n\"fin_conflit\":\"Fin de conflit\",\n\"conges\":\"Fixation des congés (jours fériés, ponts, RTT)\",\n\"forfait\":\"Forfaits (en heures, en jours)\",\n\"formation_pro\":\"Formation professionnelle\",\n\"gpec\":\"GPEC\",\n\"heures_supp\":\"Heures supplémentaires (contingent, majoration)\",\n\"indemnites\":\"Indemnités (dont kilométrique)\",\n\"interessement\":\"Intéressement\",\n\"mesure_age\":\"Mesures d'âge (seniors, contrat de génération...)\",\n\"mobilite\":\"Mobilité (géographique, professionnelle - promotions)\",\n\"diversite\":\"Non discrimination - Diversité\",\n\"participation\":\"Participation\",\n\"pee_peg\":\"PEE ou PEG\",\n\"pei\":\"PEI\",\n\"penibilite\":\"Pénibilité du travail (1% pénibilité, prévention, compensation/réparation)\",\n\"perco_percoi\":\"PERCO et PERCOI\",\n\"performance_collecte\":\"Performance collective (accord de compétitivité)\",\n\"prevoyance_collective\":\"Prévoyance collective, autre que santé maladie\",\n\"prime_partage_profit\":\"Prime de partage des profits\",\n\"qvt\":\"QVT, conciliation vie personnelle/vie professionnelle\",\n\"reprise_des_donnees\":\"Reprise des données\",\n\"retraite_complementaire\":\"Retraite complémentaire - supplémentaire\",\n\"rupture_conventionnelle_collective\":\"Rupture conventionnelle collective\",\n\"stress_rps\":\"Stress, risques psycho-sociaux\",\n\"supplement_participation\":\"Supplément de participation\",\n\"supplement_interessement\":\"Supplément d'intéressement\",\n\"systeme_prime\":\"Système de prime (autre qu'évolution)\",\n\"système_de_remuneration\":\"Système de rémunération (autres qu'évolution)\",\n\"teletravail\":\"Télétravail\",\n\"travail_temps_partiel\":\"Travail à temps partiel\",\n\"travail_nuit\":\"Travail de nuit\",\n\"travail_dimanche\":\"Travail du dimanche\",\n\"travailleurs_handicapes\":\"Travailleurs handicapés\"}\n\n\nalready_done={el.split(\"/\")[1].split(\".\")[0] for el in glob(\"results/*.answer\")}\nnew_dir = Path('results').mkdir(exist_ok=True)\n\nlist_of_df=[]\nfor index, row in df_sample.iterrows():\n    dict_answer=dict()\n    answer=\"\"\n    if index not in already_done:\n        for (k,v) in THEMATIQUES.items():\n            Q0=f\"Oui ou non : est-ce qu'il y a un article sur : {v}?\"\n            if ans:=search_and_invoke_llm(vector_store,index,Q0,k=2):\n                answer_txt=ans['answer']\n                reponse=0\n                if answer_txt.lower().startswith(\"oui\") :\n                    reponse=1\n                dict_answer[k]=reponse\n                answer_k = f\"{k} : {answer_txt}\"\n                answer += answer_k\n            answer += \"\\n-----\\n\"\n            \n        if answer:\n            with open(f\"results/{index}.answer\",\"w\") as f:\n                f.write(answer)\n        list_of_df.append(pd.DataFrame(dict_answer, index=[index]))\n\ndf_results=pd.concat(list_of_df)\n\n\n\n\nEvaluation\nNous évaluons les performances de cette solution simple, en affichant la matrice de confusion et les différentes métriques, pour chaque thématique :\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\nfor (k,v) in THEMATIQUES.items():\n    df=pd.DataFrame(df_sample[k].astype(int)).merge(df_results[k],how=\"inner\",left_index=True,right_index=True,suffixes=[\"_expected\",\"_predicted\"])\n    y_true, y_pred=df[f\"{k}_expected\"], df[f\"{k}_predicted\"]\n    cm = confusion_matrix(y_true, y_pred)\n    print(k)\n    print(cm)\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    report = classification_report(y_true, y_pred)\n    \n    print(f'Accuracy: {accuracy}')\n    print(f'Precision (macro): {precision}')\n    print(f'Recall (macro): {recall}')\n    print(f'F1 Score (macro): {f1}')\n    print(\"-\"*10)\n    print('Classification Report:')\n    print(report)",
    "crumbs": [
      "IV-Exemples",
      "Exemple des textes des accords d'entreprise"
    ]
  },
  {
    "objectID": "I-Accompagnement/4_Impacts.html",
    "href": "I-Accompagnement/4_Impacts.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Environnementaux (poids de l’entraînement, poids de l’utilisation)\nLe numérique est responsable de 2,5% de l’empreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de déchets) selon l’étude ARCEP & ADEME de 2023. Par contre, il n’existe aucun référentiel à ce jour pour mesurer l’impact environnemental des projets d’intelligence artificielle. À titre d’exemple, les émissions liées à l’entraînement de GPT-3 sont estimées à 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 représenterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, métaux et d’autres matériaux pour la fabrication et opération des infrastructures sont également conséquents.\nAfin de permettre aux acteurs du numérique d’évaluer l’impact environnemental de leurs projets d’intelligence artificielle, et de communiquer sur le caractère frugal de ces derniers, l’Ecolab du MTECT prépare avec l’AFNOR un document de référence, qui devra être disponible en juillet.\nÀ l’heure actuelle, pour estimer la consommation énergétique et les émissions de CO2 liées à l’exécution du code, les data-scientists peuvent utiliser la librairie CodeCarbon, à mettre en place avant l’usage, et/ou Green Algorithms, qui peut être utilisé pour estimer un usage futur ou passé.\nLe coût environnementale lié aux infrastructures de calcul est mis à disposition par le groupe EcoInfo du CNRS à travers l’outil EcoDiag. Des estimations plus précises pour la fabrication de GPUs seront disponibles prochainement.\n[1] https://arxiv.org/pdf/2104.10350.pdf\n[2] Data For Good - Livre Blanc de l’IA Générative\nb. Légaux (RGPD, chartes de l’IA, IA Act, ...)\nLa sécurité des données personnelles et des modèles est un enjeu considérable, que ce soit du point de vue personnel ou à l’échelle de l’administration. Par exemple, quand les modèles ne sont pas auto-hébergés, les entreprises qui les fournissent ont accès aux conversations tenus avec les chatbots. De plus ces données sont réutilisées pour l’entraînement et peuvent ressortir lors de conversations avec d’autres utilisateurs.\nLa CNIL propose une série de recommandations concenrant le développement de système d’IA impliquant un traitement des données personnelles, notamment en insistant sur la définition des finalités du traitement et sur prise en compte de la base légale du RGPD qui autorise à traiter des données personnelles. Dans le cas d’une administration publique, cette base légale pourra être par exemple selon les cas l’obligation légale, la mission d’intérêt public ou l’intérêt légitime.\nAu niveau européen, le règlement (UE) 2024/1689 du Parlement européen et du Conseil du 13 juin 2024 établissant des règles harmonisées concernant l’intelligence artificielle ou “AI Act” est le premier acte législatif européen sur l’IA. Il établit notamment des règles harmonisées concernant la mise sur le marché, mise en service et utilisation de systèmes d’IA dans l’UE, avec l’interdiction de certaines pratiques, comme la notation sociale, l’évaluation des risques de commettre des infractions ou la création de bases de données de reconnaissance faciale non ciblées. Une gradation est déterminée selon le niveau de risque, avec des systèmes d’IA à faible ou moyen risque, des systèmes à haut risque, associés à des exigences spécifiques, par exemple lorsqu’ils traitent des données personnelles, et des pratiques interdites.\nPour aller plus loin : - Guide de la CNIL - Résumé haut niveau de l’AI Act\nc. Sécurité (Renvoyer vers le guide de l’ANSSI)\nEn plus de la sécurisation commune aux applications produites par l’administration, certains sujets sont spécifiques aux modèles d’IA. L’ANSSI a écrit à ce sujet un guide de recommandations de sécurité pour sensibiliser aux risques et promouvoir les bonnes pratiques lors de la création et de la mise en production d’applications comportant des modèles d’IA générative.\nTrois catégories d’attaque spécifiques au système d’IA générative sont identifiées :\n\nles attaques par manipulation, au moyen de requêtes malveillantes;\nles attaques par infection, en contaminant les données lors de la phase d’entraînement (“model poisoning”);\nles attaques par exfiltration, qui visent à obtenir des informations sur le modèle en production, comme les données d’entraînement ou les paramètres.\n\nLes recommandations produites concernent à la fois les phases d’entraînement, de déploiement et de mise en production.\nPour aller plus loin : Guide de l’ANSSI",
    "crumbs": [
      "I-Accompagnement",
      "Impacts"
    ]
  },
  {
    "objectID": "I-Accompagnement/4_Impacts.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/4_Impacts.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "a. Environnementaux (poids de l’entraînement, poids de l’utilisation)\nLe numérique est responsable de 2,5% de l’empreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de déchets) selon l’étude ARCEP & ADEME de 2023. Par contre, il n’existe aucun référentiel à ce jour pour mesurer l’impact environnemental des projets d’intelligence artificielle. À titre d’exemple, les émissions liées à l’entraînement de GPT-3 sont estimées à 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 représenterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, métaux et d’autres matériaux pour la fabrication et opération des infrastructures sont également conséquents.\nAfin de permettre aux acteurs du numérique d’évaluer l’impact environnemental de leurs projets d’intelligence artificielle, et de communiquer sur le caractère frugal de ces derniers, l’Ecolab du MTECT prépare avec l’AFNOR un document de référence, qui devra être disponible en juillet.\nÀ l’heure actuelle, pour estimer la consommation énergétique et les émissions de CO2 liées à l’exécution du code, les data-scientists peuvent utiliser la librairie CodeCarbon, à mettre en place avant l’usage, et/ou Green Algorithms, qui peut être utilisé pour estimer un usage futur ou passé.\nLe coût environnementale lié aux infrastructures de calcul est mis à disposition par le groupe EcoInfo du CNRS à travers l’outil EcoDiag. Des estimations plus précises pour la fabrication de GPUs seront disponibles prochainement.\n[1] https://arxiv.org/pdf/2104.10350.pdf\n[2] Data For Good - Livre Blanc de l’IA Générative\nb. Légaux (RGPD, chartes de l’IA, IA Act, ...)\nLa sécurité des données personnelles et des modèles est un enjeu considérable, que ce soit du point de vue personnel ou à l’échelle de l’administration. Par exemple, quand les modèles ne sont pas auto-hébergés, les entreprises qui les fournissent ont accès aux conversations tenus avec les chatbots. De plus ces données sont réutilisées pour l’entraînement et peuvent ressortir lors de conversations avec d’autres utilisateurs.\nLa CNIL propose une série de recommandations concenrant le développement de système d’IA impliquant un traitement des données personnelles, notamment en insistant sur la définition des finalités du traitement et sur prise en compte de la base légale du RGPD qui autorise à traiter des données personnelles. Dans le cas d’une administration publique, cette base légale pourra être par exemple selon les cas l’obligation légale, la mission d’intérêt public ou l’intérêt légitime.\nAu niveau européen, le règlement (UE) 2024/1689 du Parlement européen et du Conseil du 13 juin 2024 établissant des règles harmonisées concernant l’intelligence artificielle ou “AI Act” est le premier acte législatif européen sur l’IA. Il établit notamment des règles harmonisées concernant la mise sur le marché, mise en service et utilisation de systèmes d’IA dans l’UE, avec l’interdiction de certaines pratiques, comme la notation sociale, l’évaluation des risques de commettre des infractions ou la création de bases de données de reconnaissance faciale non ciblées. Une gradation est déterminée selon le niveau de risque, avec des systèmes d’IA à faible ou moyen risque, des systèmes à haut risque, associés à des exigences spécifiques, par exemple lorsqu’ils traitent des données personnelles, et des pratiques interdites.\nPour aller plus loin : - Guide de la CNIL - Résumé haut niveau de l’AI Act\nc. Sécurité (Renvoyer vers le guide de l’ANSSI)\nEn plus de la sécurisation commune aux applications produites par l’administration, certains sujets sont spécifiques aux modèles d’IA. L’ANSSI a écrit à ce sujet un guide de recommandations de sécurité pour sensibiliser aux risques et promouvoir les bonnes pratiques lors de la création et de la mise en production d’applications comportant des modèles d’IA générative.\nTrois catégories d’attaque spécifiques au système d’IA générative sont identifiées :\n\nles attaques par manipulation, au moyen de requêtes malveillantes;\nles attaques par infection, en contaminant les données lors de la phase d’entraînement (“model poisoning”);\nles attaques par exfiltration, qui visent à obtenir des informations sur le modèle en production, comme les données d’entraînement ou les paramètres.\n\nLes recommandations produites concernent à la fois les phases d’entraînement, de déploiement et de mise en production.\nPour aller plus loin : Guide de l’ANSSI",
    "crumbs": [
      "I-Accompagnement",
      "Impacts"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Deja_Fait_Admin.html",
    "href": "I-Accompagnement/2_Deja_Fait_Admin.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Dans une enquête incluant un champ « Commentaire », celui-ci peut être analysé par des LLMs afin d’identifier les thématiques saillantes exprimées dans ce champ. Ensuite, pour chacune de ces thématiques, les LLMs peuvent être utilisés pour dégager le sentiment prédominant (ex : positif, négatif, neutre) associé à chacune d’entre elles. In fine, grâce aux LLMs, le champ « Commentaire » peut ainsi être divisé en un nombre N de thématiques, et, pour chacune de ces thématiques, un contenu peut être généré afin de faire ressortir le sentiment majoritaire des répondants à l’enquête.\nLes LLMs peuvent être utilisés pour labelliser les textes d’un corpus (articles de presse, par exemple) traitant d’un sujet (ex : décision de politique monétaire), selon certaines catégories (ex : « décision attendue », « décision surprenante », « ne sait pas »).\nInterroger une base de documents en pdf (retrieval augmented generation). Les documents sont découpés en paragraphes (chunks). Les réponses aux questions posées sont générées sur la base de paragraphes idoines. Les paragraphes qui ont servi à l’élaboration de la réponse sont indiqués en regard de celle-ci, et peuvent être consultés.\nRequêter sur des bases en SQL : à une interrogation exprimée en langage naturel sur une base en SQL (exemple : « trouve-moi le ratio R »), un code en SQL servant à la requête est renvoyé.\nChatcoder : sur la base d’une version de code fournie, une discussion en langage naturel est lancée afin soit de corriger une erreur, soit de développer une nouvelle fonction, etc…\n\n\n[Albert - Dinum] : Projet mené par le LabIA de la DINUM\n\nAlbert github : Outils de déploiements des modèles Albert\nModèles Albert\nAlbert France Services : Projet à destination de France Service et visant à appuyer ses conseillers dans la réalisation de leurs missions. Ce projet se base principalement\n\nLLamandement - LLM finetuné permettant d’accélerer le traitement d’amendements et projets de loi (notamment via la synthétisation des textes).\n\n\nPour plus de projets IA (au sens large) dans l’administration se référer au lien : https://grist.numerique.gouv.fr/o/beta-gouv-ia/9wTgwEbwqmwW/Ressources/p/1\n\n\n\n\nAlbert github Albert hugging face\n\n\n\n\n\n\ndes modeles des datasets/open data ?",
    "crumbs": [
      "I-Accompagnement",
      "Exemples dans l'administration"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Deja_Fait_Admin.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/2_Deja_Fait_Admin.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "Dans une enquête incluant un champ « Commentaire », celui-ci peut être analysé par des LLMs afin d’identifier les thématiques saillantes exprimées dans ce champ. Ensuite, pour chacune de ces thématiques, les LLMs peuvent être utilisés pour dégager le sentiment prédominant (ex : positif, négatif, neutre) associé à chacune d’entre elles. In fine, grâce aux LLMs, le champ « Commentaire » peut ainsi être divisé en un nombre N de thématiques, et, pour chacune de ces thématiques, un contenu peut être généré afin de faire ressortir le sentiment majoritaire des répondants à l’enquête.\nLes LLMs peuvent être utilisés pour labelliser les textes d’un corpus (articles de presse, par exemple) traitant d’un sujet (ex : décision de politique monétaire), selon certaines catégories (ex : « décision attendue », « décision surprenante », « ne sait pas »).\nInterroger une base de documents en pdf (retrieval augmented generation). Les documents sont découpés en paragraphes (chunks). Les réponses aux questions posées sont générées sur la base de paragraphes idoines. Les paragraphes qui ont servi à l’élaboration de la réponse sont indiqués en regard de celle-ci, et peuvent être consultés.\nRequêter sur des bases en SQL : à une interrogation exprimée en langage naturel sur une base en SQL (exemple : « trouve-moi le ratio R »), un code en SQL servant à la requête est renvoyé.\nChatcoder : sur la base d’une version de code fournie, une discussion en langage naturel est lancée afin soit de corriger une erreur, soit de développer une nouvelle fonction, etc…\n\n\n[Albert - Dinum] : Projet mené par le LabIA de la DINUM\n\nAlbert github : Outils de déploiements des modèles Albert\nModèles Albert\nAlbert France Services : Projet à destination de France Service et visant à appuyer ses conseillers dans la réalisation de leurs missions. Ce projet se base principalement\n\nLLamandement - LLM finetuné permettant d’accélerer le traitement d’amendements et projets de loi (notamment via la synthétisation des textes).\n\n\nPour plus de projets IA (au sens large) dans l’administration se référer au lien : https://grist.numerique.gouv.fr/o/beta-gouv-ia/9wTgwEbwqmwW/Ressources/p/1\n\n\n\n\nAlbert github Albert hugging face\n\n\n\n\n\n\ndes modeles des datasets/open data ?",
    "crumbs": [
      "I-Accompagnement",
      "Exemples dans l'administration"
    ]
  },
  {
    "objectID": "II-Developpements/1_Revue_Technique_LLM.html",
    "href": "II-Developpements/1_Revue_Technique_LLM.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Les LLMs reposent sur un développement en deux voire trois étapes. - Le pré-entraînement consiste à entraîner le modèle, en partant de zéro, de façon auto-supervisée, et sur un corpus d’entraînement gigantesque. L’objectif de ce pré-entraînement dépend du type de modèle utilisé (cf. paragraphe suivant), mais la plupart apprennent à prédire le token suivant, à partir d’une suite de tokens. C’est ce qui les rend particulièrement efficaces pour de la génération de texte.\n\nL’instruction-tuning permet d’adapter le modèle pré-entraîné à une plus grande diversité de tâches. Dans de nombreux cas (chatbot, résumé de texte, etc.), la prédiction du token suivant n’est pas la bonne stratégie. L’étape d’instruction-tuning permet ainsi, grâce à un entraînement supervisé, de créer une version « chat » du modèle. Pour donner un exemple connu de tous, ChatGPT est la version instruction-tunée de GPT-4.\nLe fine-tuning (optionnel) peut être utilisé pour adapter le modèle à une tâche et à des données spécifiques. Les LLMs étant des outils multitâches, souvent multilingues et multidomaines, leurs performances peuvent être dégradées lorsqu’il y a des exigences précises et spécifiques. Le fine-tuning est une nouvelle phase d’entraînement supervisé, nécessitant moins de données et de puissance de calcul, qui permet de spécialiser le modèle.\n\nPar leur taille et les exigences techniques qu’ils impliquent, seules quelques entreprises spécialisées ont les moyens de pré-entraîner et d’instruction-tuner des LLMs. Le fine-tuning, en revanche, peut être abordable pour beaucoup plus d’acteurs, pour peu qu’ils répondent à certaines exigences techniques (cf. partie sur le fine-tuning).\nPour donner des ordres de grandeur, la petite version du dernier modèle de Meta, Llama-3 8B, a été pré-entraîné et instruction-tuné sur un corpus de 15 trillions de tokens. Ces deux phases d’entraînement ont nécessité 1,3 millions d’heures GPU, réparties sur plusieurs milliers de GPU H100.\n\nArticle résumant la dualité pré-entraînement/fine-tuning\n\n\n\n\n\n\nIntroduite en 2017 dans le papier Attention Is All You Need, l’architecture Transformer a révolutionné le domaine du TAL. Par rapport aux RNN, les Transformers permettent un traitement efficace des séquences en parallèle, conduisant à un temps de calcul beaucoup plus court (tant lors de l’entraînement qu’en inférence), tandis que les RNN, par construction, ne peuvent traiter une séquence que séquentiellement, c’est-à-dire token par token. En outre, le mécanisme d’auto-attention, présenté ci-dessous, permet de capturer efficacement les dépendances distantes en atténuant le problème de la disparition et de l’explosion des gradients.\n\nL’auto-attention est le mécanisme central des Transformers. Elle est utilisée pour pondérer, lors de l’examen d’un token en particulier, l’importance, relative à ce token, de chaque autre token de la séquence. Concrètement, trois vecteurs (qui représentent chacun la séquence d’entrée dans un rôle différent) sont déduits de la séquence d’entrée \\(X\\) : les requêtes (\\(Q\\)), les clés (\\(K\\)) et les valeurs (\\(V\\)), par des transformations linéaires comme exprimées dans l’équation suivantes. Les matrices \\(W_Q\\), \\(W_K\\) et \\(W_V\\) sont des paramètres entraînables du modèle. \\[Q = X \\cdot W_Q \\qquad K = X \\cdot W_K \\qquad V = X \\cdot W_V \\]\nLes scores d’attention sont ensuite calculés selon l’équation suivante. \\[\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^{T}}{\\sqrt{d}} \\right) V\\]\nPour chaque token d’entrée \\(X_i\\), le résultat \\(\\text{Attention}(Q, K, V)_i\\) est une combinaison de tous les autres éléments de la séquence, pondérés selon leur pertinence par rapport à \\(X_i\\).\nL’auto-attention telle que présentée ci-dessus n’est cependant pas directement utilisée dans l’architecture Transformer. A la place, une extension, appelée attention multi-têtes, permet au modèle de capturer plusieurs aspects des relations et des dépendances entre les éléments de la séquence d’entrée. Cela est fait en transformant la séquence d’entrée en plusieurs têtes, i.e. en plusieurs vecteurs de requêtes, de clés et de valeurs, et en appliquant un mécanisme d’auto-attention sur chacune de ces têtes. Les vecteurs d’attention de chaque tête sont ensuite concaténés et réduits linéairement à la taille d’entrée d’origine. Le calcul de l’attention multi-têtes est détaillé dans l’équation suivante.\n\\[\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(\\text{head}_1 , \\cdots, \\text{head}_h)W^O\\]\noù \\(\\text{head}_i = \\text{Attention}(X \\cdot W_Q^i, X \\cdot W_K^i, X \\cdot W_V^i)\\) pour \\(i = 1, \\cdot, h\\) avec \\(h\\) le nombre de têtes d’attention. Chaque tête d’attention peut donc se spécialiser dans un aspect spécifique des données, et le modèle peut apprendre à combiner ces différents aspects pour une meilleure représentation. La combinaison de ce mécanisme d’attention multi-têtes, de couches de normalisation et de couches à action directes (FNN) forme un bloc Transformer.\nPlusieurs blocs (6 dans l’implémentation originale) forment ensuite l’encodeur (qui a accès à la séquence d’entrée dans son intégralité) et le décodeur (qui a accès à la représention encodée de la séquence d’entrée, et à la séquence de sortie générée jusqu’alors). La combinaison de ces deux éléments composent le Transformer encodeur-décodeur original.\n\nPapier original ‘Attention Is All You Need’\nExplication illustrée et très détaillée\n\n\n\n\nLes LLMs basés sur des architectures Transformers appartiennent à l’une des 3 catégories suivantes :\n\nModèle « encoder-only » : Ils sont basés uniquement sur la partie décodeur des Transformers. Leur pré-entraînement est souvent basé sur la reconstruction de phrases : à chaque étape, le modèle a accès à une phrase entière, sauf certains mots qui ont été masqués, et apprend à retrouver ces mots masqués. Ces modèles sont adaptés pour des tâches de classification, de reconnaissance d’entités nommées (NER), de réponses aux questions, etc. Ils ont aujourd’hui perdu en popularité, mais leurs représentants les plus connus (BERT, RoBERTa, DistilBERT, CamemBERT, etc.) sont encore très utilisés, et restent un choix intéressant selon la tâche, grâce à leur compréhension fine du langage et à leur petite taille.\nModèle « decoder-only » : Ils sont basés uniquement sur la partie décodeur des Transformers. Ces modèles sont aujourd’hui la norme, et l’immense majorité des LLMs actuels utilisent cette architecture. Leur pré-entraînement est basé sur la prédiction du prochain token : à chaque étape, le modèle a accès au début d’une phrase, et apprend à prédire le token suivant. Pour cette raison, ces modèles sont également qualifiés d’« autorégressifs ». Les modèles GPT (2, 3, 4), Llama (2, 3), Mistral, Gemini, etc. sont tous des decoder-only.\nModèle « encoder-decoder » : Ils utilisent les deux blocs des Transformers. L’encodeur a ainsi accès à l’intégralité de la séquence d’entrée, alors que le décodeur a accès à la représentation cachée de l’entrée et aux tokens générés jusqu’alors. Les modèles les plus connus sont par exemple BART et T5.\n\nhttps://medium.com/artificial-corner/discovering-llm-structures-decoder-only-encoder-only-or-decoder-encoder-5036b0e9e88\n\n\n\nLes architectures Mixture of Experts ne sont pas spécifiques aux LLMs, mais elles ont été adaptées avec succès sur des modèles comme Mixtral 8x7B, Mixtral 8x22B ou GPT-4 (supposition). Le principe est de remplacer chaque réseau à propagation directe (présent dans chaque bloc de l’architecture Transformer) par un ensemble de réseaux « experts ». Au moment de passer dans cette partie du réseau, un routeur envoie vers un de ces experts uniquement. L’intérêt est double : un seul expert étant utilisé à la fois, le temps d’inférence est naturellement nettement plus court. Par ailleurs, chaque réseau expert est entraîné et donc spécialisé différement des autres : pour un même nombre de paramètres, les performances sont donc supposées être meilleures qu’avec une architecture classique. En revanche, si tous les poids du modèles ne sont pas utilisés systématiquement, c’est uniquement à l’inférence et à chaque couche du réseau que l’expert est choisi : il est donc tout de même nécessaire de charger l’intégralité des poids du modèle en mémoire, ce qui peut être très coûteux en VRAM. Pour une explication plus technique, l’article suivant détaille très bien les MoE en prenant l’exemple de Mixtral.\n\nExplication détaillée des MoE (exemple de Mixtral) : https://huggingface.co/blog/moe\n\n\n\nLe principal inconvénient architectural des Transformers est leur complexité quadratique par rapport à la taille de l’entrée (qui vient du calcul quadratique de l’attention). Mamba est une architecture récente (Décembre 2023) qui s’affranchit du mécanisme d’attention, au profit de briques SSM (Structured State Space Models). L’intérêt principal de cette architecture est sa complexité linéaire par rapport à la taille de l’entrée.\nJamba est une nouvelle architecture hybride, à mi-chemin entre le Transformer et Mamba. Cela semble permettre un niveau de performance élevé, une gestion des contextes très longs, un temps d’inférence nettement plus court, et des exigences mémoires bien moindres.\nLiens des papiers originaux :\n\nMamba\nJamba\n\n\n\n\n\nLes LLM sont des réseaux de neurones de taille importante et font l’objet d’entraînement avec des ressources colossales (e.g: quelques dizaines de milliers de GPUs dernier modèle pendant 3 mois pour GPT-4). L’entraînement permet d’apprendre un jeu de données particulier, en réglant l’ensemble des poids du modèles (e.g: Mixtral 8x22B est une architecture à 141 milliards de poids; 175 milliards pour GPT-3). Les LLM sont entraînés à répondre à plusieurs tâches génériques et ne sont pas forcément pertinent pour des cas d’utilisation particulier.\nPour répondre à ce besoin, plusieurs méthodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste à reprendre un modèle déjà entraîné et à l’adapter sur un jeu de données particulier sur une ou plusieurs tâches spécifiques. En général, il s’agit de modifier une partie ou l’ensemble des poids pour que le modèle soit plus précis pour les tâches voulues. Le fine-tuning garde en grande partie les bénéfices de l’entraînement initial, i.e les connaissances antérieures déjà apprises. Repartir d’un modèle déjà entraîné pourra réduire le temps d’entraînement requis pour le fine-tuning, en fonction de la similarité entre la nouvelle tâche souhaitée et son jeu de données et les entraînements précédents.\nPour des petits modèles de langages, il est possible de ré-entraîner en modifiant l’ensemble des poids. Pour des modèles plus grands, modifier l’ensemble des poids peut s’avérer couteux en temps et en GPUs. Plusieurs approches permettent de ré-entraîner à moindre coût :\n\nréentrainer seulement un sous-ensemble de poids\nmodifier la tête de modélisation de la langue (lm_head) pour certains modèles, soit en réentrainant depuis les poids entraînés, soit en réinitialisant ces poids.\ngarder l’intégralité du modèle et rajouter des poids à entraîner puis utiliser l’approximation de bas rang avec LORA (Low-Rank Adaptation) pour l’entraînement et l’inférence.\nutiliser des versions quantisées, i.e. des modèles où les poids ont été tronqués à une précision inférieure (possibilité de combiner avec la technique précédente, sous le nom de qLORA).\n\nEntraînement avec qLORA en pratique :\nEn plus de la librairie transformers et datasets, les librairies peft, bitsandbytes et trl permettent de simplifier l’entraînement avec qLORA\n(inspiré du notebook suivant )\n%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U trl\n%pip install -U sentencepiece\n%pip install -U protobuf\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import load_dataset\nimport torch\nfrom trl import SFTTrainer\n\nbase_model = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"Mistral-7b-instruct-teletravail\"\n\npath_to_training_file=\"Dataset_public_accords_teletravail_Dares_train.parquet\"\npath_to_test_file=\"Dataset_public_accords_teletravail_Dares_test.parquet\"\n\n\ndataset=load_dataset(\"parquet\", data_files={'train': path_to_training_file, 'test': path_to_test_file})\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n\n\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()\n\ntrainer.model.save_pretrained(new_model)\n\n\n\n\n\n\n\n\nImplémentation HuggingFace\n\n\n\n\nPEFT = Parameter-Efficient Fine-Tuning | LoRA = Low-Rank Adaptation | QLoRA = Quantized Low-Rank Adaptation | DoRA = Weight-Decomposed Low-Rank Adaptation\nRé-entraîner entièrement un LLM est très coûteux en termes d’infrastructure et de données, et n’est donc pas à la portée de n’importe quelle organisation. Des méthodes « efficaces » ont été créées pour rendre le fine-tuning facilement accessible, dont la plus connue et la plus populaire est LoRA (Low-Rank Adaptation). Son fonctionnement repose sur deux éléments :\n\nL’adaptation : Les poids du modèle pré-entraîné sont gelés pendant l’entraînement. Ce sont des poids supplémentaires (ceux de l’adapteur) qui vont être entraînés. Cela permet de garder l’entièreté du modèle pré-entraîné tel quel, et de rajouter uniquement la partie spécifique à chaque tâche. Entre autres, il est ainsi possible, avec un seul modèle de base, d’héberger plusieurs modèles spécialisés à moindre coût. Le papier LoRA Land explique d’ailleurs comment faire tenir 25 versions de Mistral 7B fine-tunés avec LoRA sur un seul GPU A100.\nLe rang faible : Les poids additionnels peuvent être choisis de beaucoup de manières. Avec LoRA, certaines couches du modèle (les couches d’attention ou les couches linéaires par exemple) sont sélectionnées, et les poids de ces couches sont exprimés comme une multiplication de deux matrices de rangs faibles, ce qui réduit grandement le nombre de poids à entraîner (la valeur de ce rang étant un hyperparamètre de l’entraînement). En fonction de la valeur de ce rang et des couches sélectionnées, il est ainsi possible d’entraîner uniquement 1 ou 2 % du nombre de paramètres global du modèle pré-entraîné, sans que cela n’affecte trop les performances du fine-tuning.\n\n\nD’autres approches de PEFT (Parameter-Efficient Fine-Tuning) ont vu le jour, dont la plupart s’inspirent de LoRA. Parmi les plus connues, QLoRA permet d’appliquer LoRA sur des modèles quantifiés, et DoRA propose un raffinement de l’adapteur de LoRA.\n\nGuide théorique très clair sur le PEFT (principe, avantages, etc.) avec un focus sur LoRA\nGuide pratique / Implémentation HugginFace\n\nLiens des papiers originaux : - LoRA - QLoRA - DoRA\n\n\n\n\nLe fine-tuning supervisé est très efficace dans de nombreux cas, mais il présente notamment l’inconvénient de nécessiter une quantité importante de données. La constitution d’une base de questions-réponses attendues par exemple peut se réveler coûteuse. Un autre moyen d’améliorer un modèle est d’utiliser de l’apprentissage par renforcement. La première version utilisée pour ré-entraîner un LLM est le RLHF (Reinforcement Learning from Human Feedback), qui consiste à récolter des retours d’utilisateurs humains (typiquement, entre deux réponses générées par un LLM, l’utilisateur va dire laquelle il préfère), puis à mettre à jour les poids du modèle, par un algorithme d’apprentissage par renforcement, de telle sorte que la réponse préférée par l’utilisateur ait plus de chances d’être générée. Cette approche s’est révélée particulièrement effiace pour « aligner » le modèle aux préférences humaines, en termes de biais, de toxicité, de style, etc.\nBien que la constitution d’une base de retours humains soit moins coûteuse que celle d’une base de questions/réponses, elle reste coûteuse. Une solution aujourd’hui très populaire est de remplacer ces retours humains par des retours générés artificiellement, ce qui donne une approche appelée RLAIF (Reinforcement Learning from Artificial Intelligence Feedback). Typiquement, un LLM plus performant (par exemple GPT-4) va être utilisé pour déterminer la meilleure réponse entre deux ou plusieurs choix, selon des critères donnés. Ce sont ensuite ces retours qui vont être utilisés pour améliorer le modèle grâce à l’algorithme d’apprentissage par renforcement.\nRLHF = Reinforcement Learning from Human Feedback | RLAIF = Reinforcement Learning from Artificial Intelligence Feedback\n\nIntroduction au RLHF\n\n\n\nPPO = Proximal Policy Optimization\n\nExplication théorique\nImplémentation HuggingFace\n\nhttps://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200\n\n\n\nDPO = Direct Preference Optimization | KTO = Kahneman-Tversky Optimization\n\nExplication théorique\nGuide pratique / Implémentation HugginFace\n\nLiens des papiers originaux : - DPO - KTO\n\n\n\n\nCf. partie sur la RAG.\n\n\n\n\n\n\nLien du papier\n\n\n\n\nReFT = Representation Fine-Tuning | LoReFT = Low-Rank Linear Subspace ReFT\n\nLien du papier\n\n\n\n\n\n\n\n\nIl faut avant tout garder à l’esprit que le prompt engineering est une discipline très empirique, qui demande beaucoup d’itérations pour obtenir le meilleur prompt par rapport au résultat souhaité. Bien qu’il n’existe pas de méthode systématique et efficace pour optimiser un prompt, certaines pratiques sont devenues la norme. Par exemple, voici quelques bonnes pratiques : - Donner un rôle au modèle : Par exemple, dire au modèle qu’il est un magistrat honnête et impartial pourra l’aider à générer du texte formel, neutre et juridique. Le rôle est bien sûr à adapter en fonction des exigences de chaque tâche. - Structurer le prompt : Il est important de bien différencier le prompt système du prompt utilisateur. Le premier donnera des instructions générales quant au style, à la tâche, au contexte, etc., alors que le second pourra donner des instructions spécifiques ou un texte à analyser. Il est également pertinent d’organiser ou de séparer clairement les instructions. - Etre le plus précis possible : - Contraindre le modèle au maximum : - Donner des exemples : Cf. paragraphe suivant.\nLe papier Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 donne un certains nombre de principes pour améliorer les prompts. Parmi ces principes (très nombreux), on trouve par exemple : - Ne pas etre poli avec le LLM si l’on souhaite une réponse concise. - Décrire l’audience souhaitée dans le prompt (des experts techniques, des enfants, etc.). - Utiliser des directives affirmatives (fais ceci), et éviter les tournures négatives (ne fais pas cela). - Employer des phrases telles que ‘Ta tache est de’ ou ‘Tu DOIS’. - Répéter plusieurs fois certains mots ou phrases essentielles.\n\n\n\nLa façon la plus intuitive d’adresser une requête à un LLM est de formuler des instructions les plus précises possibles. Ce faisant, on espère que le modèle comprendra ces instructions et répondra en conséquence. Pour des tâches nouvelles, auxquelles le modèle n’a pas nécessairement été confronté durant son (pré)-entraînement, on appelle cette méthode du 0-shot prompting : le modèle n’a pas de référence ou d’exemple de réponse attendue.\nPour pallier ce manque de référence, il est possible (et, en fonction de la tâche, souvent recommandé) d’ajouter des exemples de paires entrée/sortie dans le prompt que l’on adresse au modèle : cela donne du 1-shot (un exemple) ou du few-shot (plusieurs exemples) prompting. Plus les exemples sont proches de la requête initiale, plus le modèle saura précisément comment répondre. Cela permet ainsi au modèle de s’adapter, à moindre coût, à une tâche très spécifique ou particulière.\n\nGuide pratique (avec exemples)\n\n\n\n\nSur certaines tâches qui demandent un raisonnement (par exemple la résolution d’un problème mathématique simple), les LLM naturellement ne sont pas très bons. Pour augmenter leurs capacités de raisonnement, une stratégie classique consiste à leur demander de raisonner et de réfléchir étape par étape.\nLes modèles les plus récents ayant nettement progressé en raisonnement, il est possible qu’ils raisonnent naturellement étape par étape sur des questions simples. Pour des questions ou des raisonnements plus complexes, il sera cependant probablement plus efficace de proposer une logique de raisonnement au modèle, en explicitant les différentes étapes.\nIl est également possible de combiner le CoT reasoning avec du few-shot prompting, i.e. de donner des exemples de raisonnement étape par étape au modèle.\n\nGuide détaillé\n\n\n\n\nRAG = Retrieval Augmented Generation\nLe principe est de rajouter du contexte dans le prompt du LLM, pour lui donner accès à des données spécifiques et pertinentes. Cf. partie sur la RAG.\n\n\n\nUne façon de travailler ses prompts est de profiter des capacités génératives des LLMs pour leur faire créer des prompts. L’idée est de donner au LLM un exemple de sortie souhaitée, et de lui demander de générer le prompt le plus adapté possible pour produire cette sortie.\n\nGuide pratique\n\n\n\n\n\n\n\nLa première question à se poser est la nécessité ou non d’utiliser un LLM. Certaines tâches peuvent se résoudre avec un LLM, mais ce n’est pas toujours la solution la plus pertinente. Par exemple, un LLM est normalement capable de parser un fichier xml sans problème, mais un script naïf sera largement aussi efficace, à bien moindre coût (environnemental, humain, financier). L’utilisation d’un LLM doit venir d’un besoin de compréhension fine du langage naturel.\nDonner quelques exemples de cas d’usages\n\n\n\nBeaucoup d’éléments sont à prendre en compte lors du choix du modèle à utiliser. Parmi les plus importants :\n\nSa taille : Exprimée généralement en milliards (B) de paramètres (Llama-3 8B possède 8 milliards de paramètres, Mistral 7B en possède 7 milliards, etc.), elle influe fortement sur les performances du modèles et les exigences techniques. Un « petit » LLM de 8 milliards de paramètres pourra tourner sur un GPU modeste avec une VRAM de 32 GB (voire moins si l’on utilise un modèle quantifié, cf. …), tandis qu’un LLM de taille moyenne de 70 milliards de paramètres nécessitera 2 GPU puissants avec 80 GB de VRAM.\nSon multilinguisme : La plupart des modèles sont entraînés sur une immense majorité de données anglaises (plus de 90 % pour Llama-2, contre moins de 0,1 % de données françaises). Les modèles incluant plus de français (Mistral ?) dans leurs données d’entraînement sont naturellement plus efficaces sur du français.\nSon temps d’inférence : Généralement directement lié à la taille du modèle, certaines architectures (MoE) permettent cependant d’avoir un temps d’inférence plus court.\nSes performances générales : Beaucoup de benchmarks publics évaluent les LLMs sur des tâches généralistes et variées. Un bon point de départ est de regarder le Leaderboard qui recense la plupart des modèles connus.\nSes performances spécifiques : Les benchmarks généralistes ne sont pas forcément pertinents pour certains cas d’usages, car ils ne sont pas spécifiques à la tâche, aux données, etc. Il peut être intéressant de développer un pipeline d’évaluation spécifique (cf…).\n\nEn juin 2024, un bon point de départ est de regarder les modèles open-source de Meta (Llama-2 7B/13B/70B, Llama-3 8B/70B) et de Mistral AI (Mistral 7B, Mixtral 8x7B).\n\n\n\nSi vous êtes dans l’un des cas suivants, le prompt engineering peut être une bonne option :\n\nPas beaucoup de ressources disponibles\nBesoin d’un outil laissé à la disposition des utilisateurs, avec une grande liberté\nLes réponses requises sont très formattées ou très spécifiques\n\n\n\n\nSi vous êtes dans l’un des cas suivants, la RAG peut être une bonne option :\n\nBesoin de réponses à jour, régulièrement et facilement actualisées\nBesoin de sourcer les réponses ou de diminuer les hallucinations\nBesoin d’enrichir les réponses avec des données spécifiques\nBesoin d’une application qui ne dépend pas d’un modèle spécifique (généralisabilité), et dont les utilisateurs ne connaissent pas l’IA générative\n\n\n\n\nSi vous êtes dans l’un des cas suivants, le fine-tuning peut être une bonne option :\n\nBesoin d’une terminologie ou d’un style spécifique\nBesoin d’enrichir les réponses avec des données spécifiques\nRessources (GPU, data scientists) disponibles\nDonnées disponibles en quantité et qualité suffisantes\nBesoin d’une application qui ne dépend pas d’un modèle spécifique (généralisabilité), et dont les utilisateurs ne connaissent pas l’IA générative\n\n\n\n\nRAG + fine-tuning = RAFT",
    "crumbs": [
      "II-Developpements",
      "Revue technique des LLM"
    ]
  },
  {
    "objectID": "II-Developpements/1_Revue_Technique_LLM.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "href": "II-Developpements/1_Revue_Technique_LLM.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "title": "Guide du LLM",
    "section": "",
    "text": "Les LLMs reposent sur un développement en deux voire trois étapes. - Le pré-entraînement consiste à entraîner le modèle, en partant de zéro, de façon auto-supervisée, et sur un corpus d’entraînement gigantesque. L’objectif de ce pré-entraînement dépend du type de modèle utilisé (cf. paragraphe suivant), mais la plupart apprennent à prédire le token suivant, à partir d’une suite de tokens. C’est ce qui les rend particulièrement efficaces pour de la génération de texte.\n\nL’instruction-tuning permet d’adapter le modèle pré-entraîné à une plus grande diversité de tâches. Dans de nombreux cas (chatbot, résumé de texte, etc.), la prédiction du token suivant n’est pas la bonne stratégie. L’étape d’instruction-tuning permet ainsi, grâce à un entraînement supervisé, de créer une version « chat » du modèle. Pour donner un exemple connu de tous, ChatGPT est la version instruction-tunée de GPT-4.\nLe fine-tuning (optionnel) peut être utilisé pour adapter le modèle à une tâche et à des données spécifiques. Les LLMs étant des outils multitâches, souvent multilingues et multidomaines, leurs performances peuvent être dégradées lorsqu’il y a des exigences précises et spécifiques. Le fine-tuning est une nouvelle phase d’entraînement supervisé, nécessitant moins de données et de puissance de calcul, qui permet de spécialiser le modèle.\n\nPar leur taille et les exigences techniques qu’ils impliquent, seules quelques entreprises spécialisées ont les moyens de pré-entraîner et d’instruction-tuner des LLMs. Le fine-tuning, en revanche, peut être abordable pour beaucoup plus d’acteurs, pour peu qu’ils répondent à certaines exigences techniques (cf. partie sur le fine-tuning).\nPour donner des ordres de grandeur, la petite version du dernier modèle de Meta, Llama-3 8B, a été pré-entraîné et instruction-tuné sur un corpus de 15 trillions de tokens. Ces deux phases d’entraînement ont nécessité 1,3 millions d’heures GPU, réparties sur plusieurs milliers de GPU H100.\n\nArticle résumant la dualité pré-entraînement/fine-tuning\n\n\n\n\n\n\nIntroduite en 2017 dans le papier Attention Is All You Need, l’architecture Transformer a révolutionné le domaine du TAL. Par rapport aux RNN, les Transformers permettent un traitement efficace des séquences en parallèle, conduisant à un temps de calcul beaucoup plus court (tant lors de l’entraînement qu’en inférence), tandis que les RNN, par construction, ne peuvent traiter une séquence que séquentiellement, c’est-à-dire token par token. En outre, le mécanisme d’auto-attention, présenté ci-dessous, permet de capturer efficacement les dépendances distantes en atténuant le problème de la disparition et de l’explosion des gradients.\n\nL’auto-attention est le mécanisme central des Transformers. Elle est utilisée pour pondérer, lors de l’examen d’un token en particulier, l’importance, relative à ce token, de chaque autre token de la séquence. Concrètement, trois vecteurs (qui représentent chacun la séquence d’entrée dans un rôle différent) sont déduits de la séquence d’entrée \\(X\\) : les requêtes (\\(Q\\)), les clés (\\(K\\)) et les valeurs (\\(V\\)), par des transformations linéaires comme exprimées dans l’équation suivantes. Les matrices \\(W_Q\\), \\(W_K\\) et \\(W_V\\) sont des paramètres entraînables du modèle. \\[Q = X \\cdot W_Q \\qquad K = X \\cdot W_K \\qquad V = X \\cdot W_V \\]\nLes scores d’attention sont ensuite calculés selon l’équation suivante. \\[\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^{T}}{\\sqrt{d}} \\right) V\\]\nPour chaque token d’entrée \\(X_i\\), le résultat \\(\\text{Attention}(Q, K, V)_i\\) est une combinaison de tous les autres éléments de la séquence, pondérés selon leur pertinence par rapport à \\(X_i\\).\nL’auto-attention telle que présentée ci-dessus n’est cependant pas directement utilisée dans l’architecture Transformer. A la place, une extension, appelée attention multi-têtes, permet au modèle de capturer plusieurs aspects des relations et des dépendances entre les éléments de la séquence d’entrée. Cela est fait en transformant la séquence d’entrée en plusieurs têtes, i.e. en plusieurs vecteurs de requêtes, de clés et de valeurs, et en appliquant un mécanisme d’auto-attention sur chacune de ces têtes. Les vecteurs d’attention de chaque tête sont ensuite concaténés et réduits linéairement à la taille d’entrée d’origine. Le calcul de l’attention multi-têtes est détaillé dans l’équation suivante.\n\\[\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(\\text{head}_1 , \\cdots, \\text{head}_h)W^O\\]\noù \\(\\text{head}_i = \\text{Attention}(X \\cdot W_Q^i, X \\cdot W_K^i, X \\cdot W_V^i)\\) pour \\(i = 1, \\cdot, h\\) avec \\(h\\) le nombre de têtes d’attention. Chaque tête d’attention peut donc se spécialiser dans un aspect spécifique des données, et le modèle peut apprendre à combiner ces différents aspects pour une meilleure représentation. La combinaison de ce mécanisme d’attention multi-têtes, de couches de normalisation et de couches à action directes (FNN) forme un bloc Transformer.\nPlusieurs blocs (6 dans l’implémentation originale) forment ensuite l’encodeur (qui a accès à la séquence d’entrée dans son intégralité) et le décodeur (qui a accès à la représention encodée de la séquence d’entrée, et à la séquence de sortie générée jusqu’alors). La combinaison de ces deux éléments composent le Transformer encodeur-décodeur original.\n\nPapier original ‘Attention Is All You Need’\nExplication illustrée et très détaillée\n\n\n\n\nLes LLMs basés sur des architectures Transformers appartiennent à l’une des 3 catégories suivantes :\n\nModèle « encoder-only » : Ils sont basés uniquement sur la partie décodeur des Transformers. Leur pré-entraînement est souvent basé sur la reconstruction de phrases : à chaque étape, le modèle a accès à une phrase entière, sauf certains mots qui ont été masqués, et apprend à retrouver ces mots masqués. Ces modèles sont adaptés pour des tâches de classification, de reconnaissance d’entités nommées (NER), de réponses aux questions, etc. Ils ont aujourd’hui perdu en popularité, mais leurs représentants les plus connus (BERT, RoBERTa, DistilBERT, CamemBERT, etc.) sont encore très utilisés, et restent un choix intéressant selon la tâche, grâce à leur compréhension fine du langage et à leur petite taille.\nModèle « decoder-only » : Ils sont basés uniquement sur la partie décodeur des Transformers. Ces modèles sont aujourd’hui la norme, et l’immense majorité des LLMs actuels utilisent cette architecture. Leur pré-entraînement est basé sur la prédiction du prochain token : à chaque étape, le modèle a accès au début d’une phrase, et apprend à prédire le token suivant. Pour cette raison, ces modèles sont également qualifiés d’« autorégressifs ». Les modèles GPT (2, 3, 4), Llama (2, 3), Mistral, Gemini, etc. sont tous des decoder-only.\nModèle « encoder-decoder » : Ils utilisent les deux blocs des Transformers. L’encodeur a ainsi accès à l’intégralité de la séquence d’entrée, alors que le décodeur a accès à la représentation cachée de l’entrée et aux tokens générés jusqu’alors. Les modèles les plus connus sont par exemple BART et T5.\n\nhttps://medium.com/artificial-corner/discovering-llm-structures-decoder-only-encoder-only-or-decoder-encoder-5036b0e9e88\n\n\n\nLes architectures Mixture of Experts ne sont pas spécifiques aux LLMs, mais elles ont été adaptées avec succès sur des modèles comme Mixtral 8x7B, Mixtral 8x22B ou GPT-4 (supposition). Le principe est de remplacer chaque réseau à propagation directe (présent dans chaque bloc de l’architecture Transformer) par un ensemble de réseaux « experts ». Au moment de passer dans cette partie du réseau, un routeur envoie vers un de ces experts uniquement. L’intérêt est double : un seul expert étant utilisé à la fois, le temps d’inférence est naturellement nettement plus court. Par ailleurs, chaque réseau expert est entraîné et donc spécialisé différement des autres : pour un même nombre de paramètres, les performances sont donc supposées être meilleures qu’avec une architecture classique. En revanche, si tous les poids du modèles ne sont pas utilisés systématiquement, c’est uniquement à l’inférence et à chaque couche du réseau que l’expert est choisi : il est donc tout de même nécessaire de charger l’intégralité des poids du modèle en mémoire, ce qui peut être très coûteux en VRAM. Pour une explication plus technique, l’article suivant détaille très bien les MoE en prenant l’exemple de Mixtral.\n\nExplication détaillée des MoE (exemple de Mixtral) : https://huggingface.co/blog/moe\n\n\n\nLe principal inconvénient architectural des Transformers est leur complexité quadratique par rapport à la taille de l’entrée (qui vient du calcul quadratique de l’attention). Mamba est une architecture récente (Décembre 2023) qui s’affranchit du mécanisme d’attention, au profit de briques SSM (Structured State Space Models). L’intérêt principal de cette architecture est sa complexité linéaire par rapport à la taille de l’entrée.\nJamba est une nouvelle architecture hybride, à mi-chemin entre le Transformer et Mamba. Cela semble permettre un niveau de performance élevé, une gestion des contextes très longs, un temps d’inférence nettement plus court, et des exigences mémoires bien moindres.\nLiens des papiers originaux :\n\nMamba\nJamba\n\n\n\n\n\nLes LLM sont des réseaux de neurones de taille importante et font l’objet d’entraînement avec des ressources colossales (e.g: quelques dizaines de milliers de GPUs dernier modèle pendant 3 mois pour GPT-4). L’entraînement permet d’apprendre un jeu de données particulier, en réglant l’ensemble des poids du modèles (e.g: Mixtral 8x22B est une architecture à 141 milliards de poids; 175 milliards pour GPT-3). Les LLM sont entraînés à répondre à plusieurs tâches génériques et ne sont pas forcément pertinent pour des cas d’utilisation particulier.\nPour répondre à ce besoin, plusieurs méthodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste à reprendre un modèle déjà entraîné et à l’adapter sur un jeu de données particulier sur une ou plusieurs tâches spécifiques. En général, il s’agit de modifier une partie ou l’ensemble des poids pour que le modèle soit plus précis pour les tâches voulues. Le fine-tuning garde en grande partie les bénéfices de l’entraînement initial, i.e les connaissances antérieures déjà apprises. Repartir d’un modèle déjà entraîné pourra réduire le temps d’entraînement requis pour le fine-tuning, en fonction de la similarité entre la nouvelle tâche souhaitée et son jeu de données et les entraînements précédents.\nPour des petits modèles de langages, il est possible de ré-entraîner en modifiant l’ensemble des poids. Pour des modèles plus grands, modifier l’ensemble des poids peut s’avérer couteux en temps et en GPUs. Plusieurs approches permettent de ré-entraîner à moindre coût :\n\nréentrainer seulement un sous-ensemble de poids\nmodifier la tête de modélisation de la langue (lm_head) pour certains modèles, soit en réentrainant depuis les poids entraînés, soit en réinitialisant ces poids.\ngarder l’intégralité du modèle et rajouter des poids à entraîner puis utiliser l’approximation de bas rang avec LORA (Low-Rank Adaptation) pour l’entraînement et l’inférence.\nutiliser des versions quantisées, i.e. des modèles où les poids ont été tronqués à une précision inférieure (possibilité de combiner avec la technique précédente, sous le nom de qLORA).\n\nEntraînement avec qLORA en pratique :\nEn plus de la librairie transformers et datasets, les librairies peft, bitsandbytes et trl permettent de simplifier l’entraînement avec qLORA\n(inspiré du notebook suivant )\n%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U trl\n%pip install -U sentencepiece\n%pip install -U protobuf\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import load_dataset\nimport torch\nfrom trl import SFTTrainer\n\nbase_model = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"Mistral-7b-instruct-teletravail\"\n\npath_to_training_file=\"Dataset_public_accords_teletravail_Dares_train.parquet\"\npath_to_test_file=\"Dataset_public_accords_teletravail_Dares_test.parquet\"\n\n\ndataset=load_dataset(\"parquet\", data_files={'train': path_to_training_file, 'test': path_to_test_file})\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n\n\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()\n\ntrainer.model.save_pretrained(new_model)\n\n\n\n\n\n\n\n\nImplémentation HuggingFace\n\n\n\n\nPEFT = Parameter-Efficient Fine-Tuning | LoRA = Low-Rank Adaptation | QLoRA = Quantized Low-Rank Adaptation | DoRA = Weight-Decomposed Low-Rank Adaptation\nRé-entraîner entièrement un LLM est très coûteux en termes d’infrastructure et de données, et n’est donc pas à la portée de n’importe quelle organisation. Des méthodes « efficaces » ont été créées pour rendre le fine-tuning facilement accessible, dont la plus connue et la plus populaire est LoRA (Low-Rank Adaptation). Son fonctionnement repose sur deux éléments :\n\nL’adaptation : Les poids du modèle pré-entraîné sont gelés pendant l’entraînement. Ce sont des poids supplémentaires (ceux de l’adapteur) qui vont être entraînés. Cela permet de garder l’entièreté du modèle pré-entraîné tel quel, et de rajouter uniquement la partie spécifique à chaque tâche. Entre autres, il est ainsi possible, avec un seul modèle de base, d’héberger plusieurs modèles spécialisés à moindre coût. Le papier LoRA Land explique d’ailleurs comment faire tenir 25 versions de Mistral 7B fine-tunés avec LoRA sur un seul GPU A100.\nLe rang faible : Les poids additionnels peuvent être choisis de beaucoup de manières. Avec LoRA, certaines couches du modèle (les couches d’attention ou les couches linéaires par exemple) sont sélectionnées, et les poids de ces couches sont exprimés comme une multiplication de deux matrices de rangs faibles, ce qui réduit grandement le nombre de poids à entraîner (la valeur de ce rang étant un hyperparamètre de l’entraînement). En fonction de la valeur de ce rang et des couches sélectionnées, il est ainsi possible d’entraîner uniquement 1 ou 2 % du nombre de paramètres global du modèle pré-entraîné, sans que cela n’affecte trop les performances du fine-tuning.\n\n\nD’autres approches de PEFT (Parameter-Efficient Fine-Tuning) ont vu le jour, dont la plupart s’inspirent de LoRA. Parmi les plus connues, QLoRA permet d’appliquer LoRA sur des modèles quantifiés, et DoRA propose un raffinement de l’adapteur de LoRA.\n\nGuide théorique très clair sur le PEFT (principe, avantages, etc.) avec un focus sur LoRA\nGuide pratique / Implémentation HugginFace\n\nLiens des papiers originaux : - LoRA - QLoRA - DoRA\n\n\n\n\nLe fine-tuning supervisé est très efficace dans de nombreux cas, mais il présente notamment l’inconvénient de nécessiter une quantité importante de données. La constitution d’une base de questions-réponses attendues par exemple peut se réveler coûteuse. Un autre moyen d’améliorer un modèle est d’utiliser de l’apprentissage par renforcement. La première version utilisée pour ré-entraîner un LLM est le RLHF (Reinforcement Learning from Human Feedback), qui consiste à récolter des retours d’utilisateurs humains (typiquement, entre deux réponses générées par un LLM, l’utilisateur va dire laquelle il préfère), puis à mettre à jour les poids du modèle, par un algorithme d’apprentissage par renforcement, de telle sorte que la réponse préférée par l’utilisateur ait plus de chances d’être générée. Cette approche s’est révélée particulièrement effiace pour « aligner » le modèle aux préférences humaines, en termes de biais, de toxicité, de style, etc.\nBien que la constitution d’une base de retours humains soit moins coûteuse que celle d’une base de questions/réponses, elle reste coûteuse. Une solution aujourd’hui très populaire est de remplacer ces retours humains par des retours générés artificiellement, ce qui donne une approche appelée RLAIF (Reinforcement Learning from Artificial Intelligence Feedback). Typiquement, un LLM plus performant (par exemple GPT-4) va être utilisé pour déterminer la meilleure réponse entre deux ou plusieurs choix, selon des critères donnés. Ce sont ensuite ces retours qui vont être utilisés pour améliorer le modèle grâce à l’algorithme d’apprentissage par renforcement.\nRLHF = Reinforcement Learning from Human Feedback | RLAIF = Reinforcement Learning from Artificial Intelligence Feedback\n\nIntroduction au RLHF\n\n\n\nPPO = Proximal Policy Optimization\n\nExplication théorique\nImplémentation HuggingFace\n\nhttps://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200\n\n\n\nDPO = Direct Preference Optimization | KTO = Kahneman-Tversky Optimization\n\nExplication théorique\nGuide pratique / Implémentation HugginFace\n\nLiens des papiers originaux : - DPO - KTO\n\n\n\n\nCf. partie sur la RAG.\n\n\n\n\n\n\nLien du papier\n\n\n\n\nReFT = Representation Fine-Tuning | LoReFT = Low-Rank Linear Subspace ReFT\n\nLien du papier\n\n\n\n\n\n\n\n\nIl faut avant tout garder à l’esprit que le prompt engineering est une discipline très empirique, qui demande beaucoup d’itérations pour obtenir le meilleur prompt par rapport au résultat souhaité. Bien qu’il n’existe pas de méthode systématique et efficace pour optimiser un prompt, certaines pratiques sont devenues la norme. Par exemple, voici quelques bonnes pratiques : - Donner un rôle au modèle : Par exemple, dire au modèle qu’il est un magistrat honnête et impartial pourra l’aider à générer du texte formel, neutre et juridique. Le rôle est bien sûr à adapter en fonction des exigences de chaque tâche. - Structurer le prompt : Il est important de bien différencier le prompt système du prompt utilisateur. Le premier donnera des instructions générales quant au style, à la tâche, au contexte, etc., alors que le second pourra donner des instructions spécifiques ou un texte à analyser. Il est également pertinent d’organiser ou de séparer clairement les instructions. - Etre le plus précis possible : - Contraindre le modèle au maximum : - Donner des exemples : Cf. paragraphe suivant.\nLe papier Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 donne un certains nombre de principes pour améliorer les prompts. Parmi ces principes (très nombreux), on trouve par exemple : - Ne pas etre poli avec le LLM si l’on souhaite une réponse concise. - Décrire l’audience souhaitée dans le prompt (des experts techniques, des enfants, etc.). - Utiliser des directives affirmatives (fais ceci), et éviter les tournures négatives (ne fais pas cela). - Employer des phrases telles que ‘Ta tache est de’ ou ‘Tu DOIS’. - Répéter plusieurs fois certains mots ou phrases essentielles.\n\n\n\nLa façon la plus intuitive d’adresser une requête à un LLM est de formuler des instructions les plus précises possibles. Ce faisant, on espère que le modèle comprendra ces instructions et répondra en conséquence. Pour des tâches nouvelles, auxquelles le modèle n’a pas nécessairement été confronté durant son (pré)-entraînement, on appelle cette méthode du 0-shot prompting : le modèle n’a pas de référence ou d’exemple de réponse attendue.\nPour pallier ce manque de référence, il est possible (et, en fonction de la tâche, souvent recommandé) d’ajouter des exemples de paires entrée/sortie dans le prompt que l’on adresse au modèle : cela donne du 1-shot (un exemple) ou du few-shot (plusieurs exemples) prompting. Plus les exemples sont proches de la requête initiale, plus le modèle saura précisément comment répondre. Cela permet ainsi au modèle de s’adapter, à moindre coût, à une tâche très spécifique ou particulière.\n\nGuide pratique (avec exemples)\n\n\n\n\nSur certaines tâches qui demandent un raisonnement (par exemple la résolution d’un problème mathématique simple), les LLM naturellement ne sont pas très bons. Pour augmenter leurs capacités de raisonnement, une stratégie classique consiste à leur demander de raisonner et de réfléchir étape par étape.\nLes modèles les plus récents ayant nettement progressé en raisonnement, il est possible qu’ils raisonnent naturellement étape par étape sur des questions simples. Pour des questions ou des raisonnements plus complexes, il sera cependant probablement plus efficace de proposer une logique de raisonnement au modèle, en explicitant les différentes étapes.\nIl est également possible de combiner le CoT reasoning avec du few-shot prompting, i.e. de donner des exemples de raisonnement étape par étape au modèle.\n\nGuide détaillé\n\n\n\n\nRAG = Retrieval Augmented Generation\nLe principe est de rajouter du contexte dans le prompt du LLM, pour lui donner accès à des données spécifiques et pertinentes. Cf. partie sur la RAG.\n\n\n\nUne façon de travailler ses prompts est de profiter des capacités génératives des LLMs pour leur faire créer des prompts. L’idée est de donner au LLM un exemple de sortie souhaitée, et de lui demander de générer le prompt le plus adapté possible pour produire cette sortie.\n\nGuide pratique\n\n\n\n\n\n\n\nLa première question à se poser est la nécessité ou non d’utiliser un LLM. Certaines tâches peuvent se résoudre avec un LLM, mais ce n’est pas toujours la solution la plus pertinente. Par exemple, un LLM est normalement capable de parser un fichier xml sans problème, mais un script naïf sera largement aussi efficace, à bien moindre coût (environnemental, humain, financier). L’utilisation d’un LLM doit venir d’un besoin de compréhension fine du langage naturel.\nDonner quelques exemples de cas d’usages\n\n\n\nBeaucoup d’éléments sont à prendre en compte lors du choix du modèle à utiliser. Parmi les plus importants :\n\nSa taille : Exprimée généralement en milliards (B) de paramètres (Llama-3 8B possède 8 milliards de paramètres, Mistral 7B en possède 7 milliards, etc.), elle influe fortement sur les performances du modèles et les exigences techniques. Un « petit » LLM de 8 milliards de paramètres pourra tourner sur un GPU modeste avec une VRAM de 32 GB (voire moins si l’on utilise un modèle quantifié, cf. …), tandis qu’un LLM de taille moyenne de 70 milliards de paramètres nécessitera 2 GPU puissants avec 80 GB de VRAM.\nSon multilinguisme : La plupart des modèles sont entraînés sur une immense majorité de données anglaises (plus de 90 % pour Llama-2, contre moins de 0,1 % de données françaises). Les modèles incluant plus de français (Mistral ?) dans leurs données d’entraînement sont naturellement plus efficaces sur du français.\nSon temps d’inférence : Généralement directement lié à la taille du modèle, certaines architectures (MoE) permettent cependant d’avoir un temps d’inférence plus court.\nSes performances générales : Beaucoup de benchmarks publics évaluent les LLMs sur des tâches généralistes et variées. Un bon point de départ est de regarder le Leaderboard qui recense la plupart des modèles connus.\nSes performances spécifiques : Les benchmarks généralistes ne sont pas forcément pertinents pour certains cas d’usages, car ils ne sont pas spécifiques à la tâche, aux données, etc. Il peut être intéressant de développer un pipeline d’évaluation spécifique (cf…).\n\nEn juin 2024, un bon point de départ est de regarder les modèles open-source de Meta (Llama-2 7B/13B/70B, Llama-3 8B/70B) et de Mistral AI (Mistral 7B, Mixtral 8x7B).\n\n\n\nSi vous êtes dans l’un des cas suivants, le prompt engineering peut être une bonne option :\n\nPas beaucoup de ressources disponibles\nBesoin d’un outil laissé à la disposition des utilisateurs, avec une grande liberté\nLes réponses requises sont très formattées ou très spécifiques\n\n\n\n\nSi vous êtes dans l’un des cas suivants, la RAG peut être une bonne option :\n\nBesoin de réponses à jour, régulièrement et facilement actualisées\nBesoin de sourcer les réponses ou de diminuer les hallucinations\nBesoin d’enrichir les réponses avec des données spécifiques\nBesoin d’une application qui ne dépend pas d’un modèle spécifique (généralisabilité), et dont les utilisateurs ne connaissent pas l’IA générative\n\n\n\n\nSi vous êtes dans l’un des cas suivants, le fine-tuning peut être une bonne option :\n\nBesoin d’une terminologie ou d’un style spécifique\nBesoin d’enrichir les réponses avec des données spécifiques\nRessources (GPU, data scientists) disponibles\nDonnées disponibles en quantité et qualité suffisantes\nBesoin d’une application qui ne dépend pas d’un modèle spécifique (généralisabilité), et dont les utilisateurs ne connaissent pas l’IA générative\n\n\n\n\nRAG + fine-tuning = RAFT",
    "crumbs": [
      "II-Developpements",
      "Revue technique des LLM"
    ]
  },
  {
    "objectID": "II-Developpements/3_Evaluations.html",
    "href": "II-Developpements/3_Evaluations.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Tous les LLM visent le même objectif : maîtriser le langage naturel et par là même, égaler l’humain dans des tâches telles que le résumé, la traduction, la reconnaissance des entités nommées, etc.\nCependant, tous les LLM souffrent des mêmes défauts, de façon plus ou moins prononcée:\n* Très grande sensibilité du modèle au prompt utilisé \n* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)\n* Les LLM peuvent avoir des comportements inattendus et dangereux suite à l'usage de prompts malveillants, de données \nd'entraînement biaisées, au recours à des agents trop permissifs, etc.\nOn souhaite donc se doter d’un cadre de comparaison qui permette d’affirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir à différentes métriques pour mesurer differents aspects du problème (fiabilité, sécurité, absence de biais…)\nSi de nombreux bancs d’essai existent aujourd’hui, permettant de distinguer certains LLM, il ne faut pas oublier que de bonnes performances dans un banc d’essai ne sont pas suffisantes, et qu’il est primordial de mettre en place un système d’évaluation quasi temps réél du LLM une fois en production.\n\n\n\na) Scenario\nUn scénario est un ensemble de conditions dans lesquelles la performance du LLM est évaluée. Il s’agit par exemple de\n\nRéponse aux questions\nRaisonnement\nTraduction\nGénération de texte\n…\n\nb) Tâche\nUne tâche constitue une forme plus granulaire d’un scénario. Elle conditionne plus spécifiquement sur quelle base le LLM est évalué. Une tâche peut être une composition de plusieurs sous-tâches.\n\nCombinaisons de sous-tâches de difficulté variée\n\nPar exemple, l’arithmétique peut être considérée comme une tâche constituée des sous-tâches arithmétique niveau 1er degré, arithmétique niveau collège, arithmétique niveau lycée, etc.\n\nCombinaison de sous-tâche de domaines variés\n\nLa tâche de type QCM peut être vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.\nc) Métrique\nUne métrique est une mesure qualitative utilisée pour évaluer la performance d’un modèle de langage dans certaines tâches/scénarios. Une métrique peut être :\n\nune fonction statistique/mathématique déterministe simple (par exemple, précision ou rappel)\nun score produit par un réseau neuronal ou un modèle de Machine Learning (ex. : score BERT)\nun score généré à l’aide d’un LLM (ex. : G-Eval)\n\nNotons que dans le dernier cas, évaluer un LLM à l’aide d’un LLM peut donner l’impression du serpent qui se mord la queue. Cependant, ce type de ‘dépendances circulaires’ existe couramment et est bien acceptée dans d’autres dommaines. Lors d’un entretien d’embauche par exemple, l’intellect humain évalue le potentiel d’un autre être humain.\nd) Benchmarks\nLes benchmarks sont des collections standardisées de tests utilisées pour évaluer les LLM sur une tâche ou un scénario donné. On trouvera pas exemple :\n\nSQuAD pour le scénario de réponse aux questions de l’utilisateur à partir d’extraction de parties d’un corpus (En anglais)\nPIAF semblable à SQuAD mais en français\nIMDB pour l’analyse des sentiments\n…\n\nA titre d’exemple, l’image ci-dessous présente le corpus PIAF. Il est composé de paragraphes issus d’articles de Wikipedia, et d’une liste de questions portant sur ces paragraphes.\n\n\n\n\nL’expression évaluation de LLM peut recouvrir différentes pratiques et différents objectifs. On doit ainsi distinguer l’évaluations de modèles LLM de l’évaluations de systèmes LLM. Les évaluations de modèles LLM s’intéressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacité sur un ensemble de tâches différentes.\nIl existe de nombreux benchmarks qui permettent d’illustrer les performances des modèles sur des aspects précis, comme HellaSwag (qui évalue la capacité d’un LLM à compléter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la véracité des réponses du modèle) et MMLU (qui mesure la capacité de compréhension et de résolution de problèmes ).\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nL’évaluation de systèmes LLM couvre l’évaluation de tous les composants de la chaîne, pour un modèle donné. En effet, un modèle de LLM est rarement utilisé seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le résultat du modèle. On pourra s’intéresser par exemple à l’effet du prompt sur la politesse de la réponse, le style, le niveau de détail, etc. Un modèle peut également recevoir un contexte (ensemble de documents, tableaux, images…) et son influence doit également être mesurée. On pourrait par exemple s’apercevoir que le modèle produit des résumés de qualité quand on lui fournit des documents littéraires, mais pas des documents techniques.\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nEn pratique, l’évaluation des modèles sur des benchmarks est réalisée par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communauté universitaire. Ce sont les évaluations de systèmes LLM qui intéresseront la majorité des équipes souhaitant déployer un LLM dans leur administration.\n\n\n\nIl n’existe pas de réponse simple à la question de savoir quelles métriques utiliser pour évaluer son système LLM. Cela dépendra du type de tâche, de la population cible, de la nature des données, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l’apprentissage machine, on évalue un modèle en se dotant d’un ensemble annoté d’entrées/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes réponses.\nLa difficulté de l’évaluation en IA générative réside dans le fait que nous ne disposons généralement pas de valeur de référence à laquelle comparer la sortie du modèle. Même dans les cas où l’on disposerait d’un exemple de bonne réponse, les données de sortie étant non structurées (texte en language naturel), il est difficile de comparer la distance entre deux objets.\nOn reprend ici l’idée de classer les métriques en fonction de leur approche du problème, c’est à dire comment elles évaluent la pertinence de la réponse obtenue. Certaines techniques supposent que l’on dispose d’une réponse de référence, et la question est alors de savoir comment elles évaluent la distance entre la réponse obtenue et une réponse de référence. D’autres techniques plus récentes ne font pas cette hypothèse, et cherchent à évaluer la qualité de la réponse dans l’absolu. \nOn ne détaillera pas toutes les métriques dans le cadre de ce guide, il existe pléthore de documentation disponible sur le sujet. L’objectif ici est plutôt de fournir une grille d’analyse.\nDans les métriques classiques, on trouve des métriques générales de classification qui sont couramment utilisées en apprentissage machine et ne sont pas propres aux données textuelles (Accuracy, Precision, Recall, F1…). Parmi les classiques, il existe également des métriques spécifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases prédites et les phrases de référence. Le recouvrement peut être calculé au niveau des mots, ou au niveau des lettres. Ces méthodes ont été critiquées par leur faible corrélation avec le jugement humain, ce qui est n’est pas étonnant dans la mesure où elles ne s’intéressent qu’a la forme de surface des mots.\nLes métriques basées sur le Deep Learning permettent de palier ce problème. Quand vient l’heure de choisir une métrique, on distingue en premier lieu les métriques qui ont besoin d’une valeur de référence et les autres. Détenir une valeur de référence peut représenter une grosse contrainte, et n’est pas toujours pertinent. Si on demande au LLM d’écrire un poème sur la mer par exemple, il serait vain de chercher à le comparer à un autre poème. Toujours est-il que si l’on dispose de valeurs de références, on peut recourir à des métriques basés sur les embeddings ou des métriques basées sur des modèles fine-tunés. Les métriques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexité peut les rendre intéressantes.\na. Métriques (biais, hallucinations, ...)\n\nb. Datasets\n\nc. Librairies/Frameworks (CODE!)\n\nd. Méthodologie (arbre de décision pour le décideur)",
    "crumbs": [
      "II-Developpements",
      "Evaluations"
    ]
  },
  {
    "objectID": "II-Developpements/3_Evaluations.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "href": "II-Developpements/3_Evaluations.html#partie-ii.-développements-autour-des-llms-pour-les-data-scientists",
    "title": "Guide du LLM",
    "section": "",
    "text": "Tous les LLM visent le même objectif : maîtriser le langage naturel et par là même, égaler l’humain dans des tâches telles que le résumé, la traduction, la reconnaissance des entités nommées, etc.\nCependant, tous les LLM souffrent des mêmes défauts, de façon plus ou moins prononcée:\n* Très grande sensibilité du modèle au prompt utilisé \n* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)\n* Les LLM peuvent avoir des comportements inattendus et dangereux suite à l'usage de prompts malveillants, de données \nd'entraînement biaisées, au recours à des agents trop permissifs, etc.\nOn souhaite donc se doter d’un cadre de comparaison qui permette d’affirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir à différentes métriques pour mesurer differents aspects du problème (fiabilité, sécurité, absence de biais…)\nSi de nombreux bancs d’essai existent aujourd’hui, permettant de distinguer certains LLM, il ne faut pas oublier que de bonnes performances dans un banc d’essai ne sont pas suffisantes, et qu’il est primordial de mettre en place un système d’évaluation quasi temps réél du LLM une fois en production.\n\n\n\na) Scenario\nUn scénario est un ensemble de conditions dans lesquelles la performance du LLM est évaluée. Il s’agit par exemple de\n\nRéponse aux questions\nRaisonnement\nTraduction\nGénération de texte\n…\n\nb) Tâche\nUne tâche constitue une forme plus granulaire d’un scénario. Elle conditionne plus spécifiquement sur quelle base le LLM est évalué. Une tâche peut être une composition de plusieurs sous-tâches.\n\nCombinaisons de sous-tâches de difficulté variée\n\nPar exemple, l’arithmétique peut être considérée comme une tâche constituée des sous-tâches arithmétique niveau 1er degré, arithmétique niveau collège, arithmétique niveau lycée, etc.\n\nCombinaison de sous-tâche de domaines variés\n\nLa tâche de type QCM peut être vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.\nc) Métrique\nUne métrique est une mesure qualitative utilisée pour évaluer la performance d’un modèle de langage dans certaines tâches/scénarios. Une métrique peut être :\n\nune fonction statistique/mathématique déterministe simple (par exemple, précision ou rappel)\nun score produit par un réseau neuronal ou un modèle de Machine Learning (ex. : score BERT)\nun score généré à l’aide d’un LLM (ex. : G-Eval)\n\nNotons que dans le dernier cas, évaluer un LLM à l’aide d’un LLM peut donner l’impression du serpent qui se mord la queue. Cependant, ce type de ‘dépendances circulaires’ existe couramment et est bien acceptée dans d’autres dommaines. Lors d’un entretien d’embauche par exemple, l’intellect humain évalue le potentiel d’un autre être humain.\nd) Benchmarks\nLes benchmarks sont des collections standardisées de tests utilisées pour évaluer les LLM sur une tâche ou un scénario donné. On trouvera pas exemple :\n\nSQuAD pour le scénario de réponse aux questions de l’utilisateur à partir d’extraction de parties d’un corpus (En anglais)\nPIAF semblable à SQuAD mais en français\nIMDB pour l’analyse des sentiments\n…\n\nA titre d’exemple, l’image ci-dessous présente le corpus PIAF. Il est composé de paragraphes issus d’articles de Wikipedia, et d’une liste de questions portant sur ces paragraphes.\n\n\n\n\nL’expression évaluation de LLM peut recouvrir différentes pratiques et différents objectifs. On doit ainsi distinguer l’évaluations de modèles LLM de l’évaluations de systèmes LLM. Les évaluations de modèles LLM s’intéressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacité sur un ensemble de tâches différentes.\nIl existe de nombreux benchmarks qui permettent d’illustrer les performances des modèles sur des aspects précis, comme HellaSwag (qui évalue la capacité d’un LLM à compléter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la véracité des réponses du modèle) et MMLU (qui mesure la capacité de compréhension et de résolution de problèmes ).\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nL’évaluation de systèmes LLM couvre l’évaluation de tous les composants de la chaîne, pour un modèle donné. En effet, un modèle de LLM est rarement utilisé seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le résultat du modèle. On pourra s’intéresser par exemple à l’effet du prompt sur la politesse de la réponse, le style, le niveau de détail, etc. Un modèle peut également recevoir un contexte (ensemble de documents, tableaux, images…) et son influence doit également être mesurée. On pourrait par exemple s’apercevoir que le modèle produit des résumés de qualité quand on lui fournit des documents littéraires, mais pas des documents techniques.\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nEn pratique, l’évaluation des modèles sur des benchmarks est réalisée par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communauté universitaire. Ce sont les évaluations de systèmes LLM qui intéresseront la majorité des équipes souhaitant déployer un LLM dans leur administration.\n\n\n\nIl n’existe pas de réponse simple à la question de savoir quelles métriques utiliser pour évaluer son système LLM. Cela dépendra du type de tâche, de la population cible, de la nature des données, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l’apprentissage machine, on évalue un modèle en se dotant d’un ensemble annoté d’entrées/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes réponses.\nLa difficulté de l’évaluation en IA générative réside dans le fait que nous ne disposons généralement pas de valeur de référence à laquelle comparer la sortie du modèle. Même dans les cas où l’on disposerait d’un exemple de bonne réponse, les données de sortie étant non structurées (texte en language naturel), il est difficile de comparer la distance entre deux objets.\nOn reprend ici l’idée de classer les métriques en fonction de leur approche du problème, c’est à dire comment elles évaluent la pertinence de la réponse obtenue. Certaines techniques supposent que l’on dispose d’une réponse de référence, et la question est alors de savoir comment elles évaluent la distance entre la réponse obtenue et une réponse de référence. D’autres techniques plus récentes ne font pas cette hypothèse, et cherchent à évaluer la qualité de la réponse dans l’absolu. \nOn ne détaillera pas toutes les métriques dans le cadre de ce guide, il existe pléthore de documentation disponible sur le sujet. L’objectif ici est plutôt de fournir une grille d’analyse.\nDans les métriques classiques, on trouve des métriques générales de classification qui sont couramment utilisées en apprentissage machine et ne sont pas propres aux données textuelles (Accuracy, Precision, Recall, F1…). Parmi les classiques, il existe également des métriques spécifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases prédites et les phrases de référence. Le recouvrement peut être calculé au niveau des mots, ou au niveau des lettres. Ces méthodes ont été critiquées par leur faible corrélation avec le jugement humain, ce qui est n’est pas étonnant dans la mesure où elles ne s’intéressent qu’a la forme de surface des mots.\nLes métriques basées sur le Deep Learning permettent de palier ce problème. Quand vient l’heure de choisir une métrique, on distingue en premier lieu les métriques qui ont besoin d’une valeur de référence et les autres. Détenir une valeur de référence peut représenter une grosse contrainte, et n’est pas toujours pertinent. Si on demande au LLM d’écrire un poème sur la mer par exemple, il serait vain de chercher à le comparer à un autre poème. Toujours est-il que si l’on dispose de valeurs de références, on peut recourir à des métriques basés sur les embeddings ou des métriques basées sur des modèles fine-tunés. Les métriques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexité peut les rendre intéressantes.\na. Métriques (biais, hallucinations, ...)\n\nb. Datasets\n\nc. Librairies/Frameworks (CODE!)\n\nd. Méthodologie (arbre de décision pour le décideur)",
    "crumbs": [
      "II-Developpements",
      "Evaluations"
    ]
  },
  {
    "objectID": "notebooks/10p_RAG_OLLAMA.html",
    "href": "notebooks/10p_RAG_OLLAMA.html",
    "title": "Notebook exemple",
    "section": "",
    "text": "!sh ./init-env-download.sh\n\n\nrun : ollama serve\nthen : ollama run llama3.1\nor mistral-large or any models\n\n\nprint(\"Avez-vous bien lancer ollama serve et ollama run ? :) \")\n\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport requests\n\nfrom langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\nfrom langchain.llms import Ollama, BaseLLM\nfrom langchain.schema import Document, Generation, LLMResult\nfrom langchain.vectorstores import Chroma\nfrom langchain_chroma import Chroma\nfrom langchain_community.llms import OpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom glob import glob\n\n\ndf_sample=pd.read_parquet(\"./10p_accords_publics_et_thematiques_240815_sample_of_1000.parquet\")\n\n\ndf_sample=df_sample[:10]"
  },
  {
    "objectID": "notebooks/10p_RAG_OLLAMA.html#imports-et-données",
    "href": "notebooks/10p_RAG_OLLAMA.html#imports-et-données",
    "title": "Notebook exemple",
    "section": "",
    "text": "!sh ./init-env-download.sh\n\n\nrun : ollama serve\nthen : ollama run llama3.1\nor mistral-large or any models\n\n\nprint(\"Avez-vous bien lancer ollama serve et ollama run ? :) \")\n\n\nimport json\nimport numpy as np\nimport pandas as pd\nimport requests\n\nfrom langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain, create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings\nfrom langchain.llms import Ollama, BaseLLM\nfrom langchain.schema import Document, Generation, LLMResult\nfrom langchain.vectorstores import Chroma\nfrom langchain_chroma import Chroma\nfrom langchain_community.llms import OpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom glob import glob\n\n\ndf_sample=pd.read_parquet(\"./10p_accords_publics_et_thematiques_240815_sample_of_1000.parquet\")\n\n\ndf_sample=df_sample[:10]"
  },
  {
    "objectID": "notebooks/10p_RAG_OLLAMA.html#vectorisation-des-textes",
    "href": "notebooks/10p_RAG_OLLAMA.html#vectorisation-des-textes",
    "title": "Notebook exemple",
    "section": "2 - Vectorisation des textes",
    "text": "2 - Vectorisation des textes\n\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\\n\",\n    chunk_size=3000,\n    chunk_overlap=200,\n    length_function=len,\n    is_separator_regex=False,\n)\n\nmodel_kwargs = {'device': 'cuda'}\nembedder = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\", model_kwargs=model_kwargs,show_progress=False)\n\n\nvector_store = Chroma(embedding_function=embedder, persist_directory=\"./chroma_db\")\nfor index, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n    text=df_sample.texte[index]\n    texts = text_splitter.create_documents([text])\n    i=0\n    for t in texts:\n        t.metadata[\"id\"]=f\"{index}_{i}\"\n        t.metadata[\"index\"]=f\"{index}\"\n        vector_store.add_documents([t])\n        i+=1"
  },
  {
    "objectID": "notebooks/10p_RAG_OLLAMA.html#inférence-avec-ollama",
    "href": "notebooks/10p_RAG_OLLAMA.html#inférence-avec-ollama",
    "title": "Notebook exemple",
    "section": "3 - Inférence avec Ollama",
    "text": "3 - Inférence avec Ollama\n\n#MODEL=\"mistral-large\"\nMODEL=\"llama3.1\"\n\n\nclass LocalOllamaLLM(BaseLLM):\n    api_url : str\n    def _generate(self, prompt, stop):\n        response = requests.post(f\"{self.api_url}/api/generate\", json={\"model\": MODEL , \"prompt\": str(prompt) })\n        response.raise_for_status()\n        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])\n        generations=[]\n        generations.append([Generation(text=response_text)])\n        return LLMResult(generations=generations)\n\n\n    def _llm_type(self):\n        return \"local\"  \n\n\nllm = LocalOllamaLLM(api_url=\"http://127.0.0.1:11434\")\n\nsystem_prompt = (\n    \" Répondez à la question posée \"\n    \" Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question \"\n    \" Si la réponse ne se trouve pas dans le contexte, répondez par 'Non'\"\n    \" Contexte : {context}  \"\n)\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        (\"human\", \"{input}\"),\n    ]\n)\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\n\n\ndef search_and_invoke_llm(vector_store,index,query,k=5):\n    if k==0:\n        print(f\"bug with {index}\")\n        return None\n    else:\n        pass\n    try:\n        retriever=vector_store.as_retriever(\n        search_kwargs={\n                \"k\": k, \n                \"filter\": {'index': index}\n            }\n        )\n        chain = create_retrieval_chain(retriever, question_answer_chain)\n        result=chain.invoke({\"input\": query})\n        return result\n    except:\n        search_and_invoke_llm(vector_store,index,query,k=k-1)\n    return None\n\n\nTHEMATIQUES={\n    \"accord_methode_penibilite\":\"Accords de méthode (pénibilité)\",\n\"accord_methode_pse\":\"Accords de méthode (PSE)\",\n\"amenagement_temps_travail\":\"Aménagement du temps de travail (modulation, annualisation, cycles)\",\n\"autres\":\"Autre, précisez\",\n\"autres_condition_travail\":\"Autres dispositions de conditions de travail (CHSCT, médecine du travail, politique générale de prévention)\",\n\"autres_dispositions_duree\":\"Autres dispositions durée et aménagement du temps de travail \",\n\"autres_dispositions_egalite\":\"Autres dispositions Egalité professionnelle\",\n\"autres_dispositions_emploi\":\"Autres dispositions emploi\",\n\"calendrier_negociation\":\"Calendrier des négociations\",\n\"classifications\":\"Classifications\",\n\"commision_paritaire\":\"Commissions paritaires\",\n\"cet\":\"Compte épargne temps\",\n\"couverture_complementaire\":\"Couverture complémentaire santé - maladie\",\n\"don_jour\":\"Dispositifs don de jour et jour de solidarité\",\n\"distribution_actions_gratuites\":\"Distribution d'actions gratuites\",\n\"droit_deconnexion\":\"Droit à la déconnexion et outils numériques\",\n\"droit_syndical\":\"Droit syndical, IRP, expression des salariés\",\n\"duree_collective_temps_travail\":\"Durée collective du temps de travail\",\n\"egalite_salariale\":\"Egalité salariale F/H\",\n\"election_pro\":\"Elections professionnelles, prorogations de mandat et vote électronique\",\n\"evolution_prime\":\"Evolution des primes\",\n\"evolution_salariale\":\"Evolution des salaires (augmentation, gel, diminution)\",\n\"fin_conflit\":\"Fin de conflit\",\n\"conges\":\"Fixation des congés (jours fériés, ponts, RTT)\",\n\"forfait\":\"Forfaits (en heures, en jours)\",\n\"formation_pro\":\"Formation professionnelle\",\n\"gpec\":\"GPEC\",\n\"heures_supp\":\"Heures supplémentaires (contingent, majoration)\",\n\"indemnites\":\"Indemnités (dont kilométrique)\",\n\"interessement\":\"Intéressement\",\n\"mesure_age\":\"Mesures d'âge (seniors, contrat de génération...)\",\n\"mobilite\":\"Mobilité (géographique, professionnelle - promotions)\",\n\"diversite\":\"Non discrimination - Diversité\",\n\"participation\":\"Participation\",\n\"pee_peg\":\"PEE ou PEG\",\n\"pei\":\"PEI\",\n\"penibilite\":\"Pénibilité du travail (1% pénibilité, prévention, compensation/réparation)\",\n\"perco_percoi\":\"PERCO et PERCOI\",\n\"performance_collecte\":\"Performance collective (accord de compétitivité)\",\n\"prevoyance_collective\":\"Prévoyance collective, autre que santé maladie\",\n\"prime_partage_profit\":\"Prime de partage des profits\",\n\"qvt\":\"QVT, conciliation vie personnelle/vie professionnelle\",\n\"reprise_des_donnees\":\"Reprise des données\",\n\"retraite_complementaire\":\"Retraite complémentaire - supplémentaire\",\n\"rupture_conventionnelle_collective\":\"Rupture conventionnelle collective\",\n\"stress_rps\":\"Stress, risques psycho-sociaux\",\n\"supplement_participation\":\"Supplément de participation\",\n\"supplement_interessement\":\"Supplément d'intéressement\",\n\"systeme_prime\":\"Système de prime (autre qu'évolution)\",\n\"système_de_remuneration\":\"Système de rémunération (autres qu'évolution)\",\n\"teletravail\":\"Télétravail\",\n\"travail_temps_partiel\":\"Travail à temps partiel\",\n\"travail_nuit\":\"Travail de nuit\",\n\"travail_dimanche\":\"Travail du dimanche\",\n\"travailleurs_handicapes\":\"Travailleurs handicapés\"}\n\n\nalready_done={el.split(\"/\")[1].split(\".\")[0] for el in glob(\"results/*.answer\")}\nnew_dir = Path('results').mkdir(exist_ok=True)\n\nlist_of_df=[]\nfor index, row in df_sample.iterrows():\n    dict_answer=dict()\n    answer=\"\"\n    if index not in already_done:\n        for (k,v) in THEMATIQUES.items():\n            Q0=f\"Oui ou non : est-ce qu'il y a un article sur : {v}?\"\n            if ans:=search_and_invoke_llm(vector_store,index,Q0,k=2):\n                answer_txt=ans['answer']\n                reponse=0\n                if answer_txt.lower().startswith(\"oui\") :\n                    reponse=1\n                dict_answer[k]=reponse\n                answer_k = f\"{k} : {answer_txt}\"\n                answer += answer_k\n            answer += \"\\n-----\\n\"\n            \n        if answer:\n            with open(f\"results/{index}.answer\",\"w\") as f:\n                f.write(answer)\n        list_of_df.append(pd.DataFrame(dict_answer, index=[index]))\n\n\ndf_results=pd.concat(list_of_df)"
  },
  {
    "objectID": "notebooks/10p_RAG_OLLAMA.html#eval",
    "href": "notebooks/10p_RAG_OLLAMA.html#eval",
    "title": "Notebook exemple",
    "section": "Eval",
    "text": "Eval\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\n\nfor (k,v) in THEMATIQUES.items():\n    df=pd.DataFrame(df_sample[k].astype(int)).merge(df_results[k],how=\"inner\",left_index=True,right_index=True,suffixes=[\"_expected\",\"_predicted\"])\n    y_true, y_pred=df[f\"{k}_expected\"], df[f\"{k}_predicted\"]\n    cm = confusion_matrix(y_true, y_pred)\n    print(k)\n    print(cm)\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    report = classification_report(y_true, y_pred)\n    \n    print(f'Accuracy: {accuracy}')\n    print(f'Precision (macro): {precision}')\n    print(f'Recall (macro): {recall}')\n    print(f'F1 Score (macro): {f1}')\n    print(\"-\"*10)\n    print('Classification Report:')\n    print(report)"
  },
  {
    "objectID": "III-Deploiements/1_Socle_minimal.html",
    "href": "III-Deploiements/1_Socle_minimal.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Pour déployer un grand modèle de langage (LLM) dans une infrastructure, il est essentiel de comprendre comment requêter le modèle, les quelques couches techniques immédiates qui l’entourent et les solutions disponibles pour un déploiement efficace.\n\n\nLorsqu’il s’agit de mettre en service des applications basées sur des LLM, il y a 2 composants principaux : le moteur et le serveur. Le moteur gère tout ce qui concerne les modèles et le regroupement des demandes, tandis que le serveur gère l’acheminement des demandes des utilisateurs.\n\n\nLes moteurs sont les composants exécutant les modèles et tout ce que nous avons couvert jusqu’à présent sur le processus de génération avec différents types d’optimisations. À leur cœur, ce sont des bibliothèques Python. Ils gèrent le regroupement des demandes qui proviennent des utilisateurs vers notre chatbot et génèrent la réponse à ces demandes.\n\n\n\nLes serveurs sont responsables de l’orchestration des requêtes HTTP/gRPC entrantes des utilisateurs. Dans les applications du monde réel, nous aurons de nombreux utilisateurs qui posent des questions à notre chatbot à différents moments de la journée. Les serveurs mettent ces demandes en file d’attente et les transfèrent vers le moteur pour la génération de la réponse. Les serveurs apportent également les métriques telles que le débit et la latence, qui sont importantes à suivre pour le service de modèle.\n\n\n\nMoteurs\n\nOptimisation de la mémoire\nOptimisation spécifique au modèle\nPrise en charge du regroupement\n\nServeurs\n\nAPI HTTP/gRPC\nMise en file d’attente des demandes\nMise en service de plusieurs modèles\nPrise en charge de plusieurs moteurs\n\n\n\n\n\nQuels outils sont les mieux adaptés à nos besoins ? Comment choisir ? Voici un survol rapide de grands noms du milieu pour références.\n\nUne recommandation de framework rapide à prendre en main et dont l’utilité a déjà été prouvée dans une de nos administrations se trouve à la fin et est développée dans le prochain paragraphe.\n\nMoteurs\n\nTensorRT-LLM est une bibliothèque open-source qui optimise les performances d’inférence des grands modèles de langage (LLM) en utilisant les GPU NVIDIA Tensor Core. Elle utilise le parallélisme tensoriel, propose une API Python simple et comprend des versions optimisées de LLM populaires. Elle prend en charge le batching en vol et vise à simplifier la construction et l’expérimentation de nouveaux LLM. Cependant, les utilisateurs doivent spécifier la longueur d’entrée/sortie maximale et la taille de lot avant de construire le moteur, et la gestion de la mémoire du cache KV n’est pas open source.\nvLLM est une bibliothèque à hautes performances pour l’inférence et le service LLM, axée sur le débit de service et l’efficacité mémoire grâce à son mécanisme PagedAttention. Il prend en charge le batching continu, le parallélisme GPU et la sortie en streaming, ainsi que la compatibilité OpenAI. Cependant, la mémoire peut devenir un goulot d’étranglement avec des taux de demande élevés et de grandes tailles de lot.\n\nServeurs\n\nRayLLM avec RayServe est construit sur un framework de calcul distribué qui simplifie le développement et le déploiement de modèles d’IA à grande échelle. Il prend en charge les points de terminaison multi-modèles, les fonctionnalités serveur et les optimisations via les intégrations avec vLLM et TGI.\nTriton avec TensorRT-LLM fournit un logiciel d’inférence de serveur pour le déploiement et l’exécution efficaces de LLM avec des techniques telles que le batching en vol et le cache KV paginé.\n\nMoteurs et serveurs\n\nGénération de texte Inférence (TGI) est un serveur Rust, Python et gRPC utilisé chez HuggingFace pour HuggingChat, l’API d’inférence et le point de terminaison d’inférence. Il prend en charge le batching continu, le parallélisme tensoriel, la quantification, les mécanismes d’attention, le recuit simulé des logits et des LLM spécifiques. Cependant, la licence d’utilisation a été modifiée et n’est pas gratuite pour une utilisation commerciale.\nEnfin, Fastchat est une solution auto-hébergée pour héberger des modèles d’IA génératifs et qui propose la gestion des modèles, des API OpenAI-compatibles et une web interface simple.\n\n\nNous allons développer FastChat dans la partie suivante car c’est un outil qui a été testé et qui semble fournir beaucoup des éléments nécessaires pour une utilisation de première intention.\n\n\n\n\n\n\nPour certains cas d’usage, l’enjeu est de traiter de nombreuses données avec le même mode opératoire en un coup de manière ponctuelle. C’est ce qu’on appellera le traitement par batch. Cela consiste à charger un modèle, le requêter sur un tableau de prompt et obtenir la sortie pour pouvoir l’exporter. On peut le faire avec vLLM par exemple avec un morceau de code de ce type :\nfrom vllm import LLM, SamplingParams\nimport re\nimport pandas as pd\nimport re\nimport json\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n\nlist_data = json.load(open(\"Data.json\"))\n\nlist_prompts = [ v for x,v in list_data.items()]\nlist_ids = [ x for x,v in list_data.items() ]\n\nsampling_params = SamplingParams(temperature=0.1, top_p=0.1, max_tokens=4096)\nllm = LLM(model=\"/data/models/hub/models--upstage--Llama-2-70b-instruct-v2/snapshots/36b2a974642846b40fbbafaabad936cd6f8a7632\", tensor_parallel_size=2)\nprint(\"STARTING INFERENCE\")\noutputs = llm.generate(list_prompts, sampling_params)\n\nresume = { idx:output.outputs[0].text for idx, output in zip(list_ids, outputs) }\n\njson.dump(resume, open(\"Sortie.json\", \"w\"))\nMais cette méthodologie a des limites, car cela nécessite de bloquer des gpus, ce qui entraîne des problématiques de gestion et de partage.\n\n\n\nQue ce soit une équipe de plusieurs data-scientists, ou un ensemble d’application, si les besoins sont importants, les GPUs ont tout intérêt à être partagés. Il ne sera donc pas possible que chaque script python charge son modèle en mémoire et bloque des GPUs. Il est également plus rassurant de séparer l’infrastructure GPU des utilisateurs pour que chacun travaille dans son environnement, afin d’éviter les casses accidentelles.\nLa solution qui consiste à mettre à disposition des APIs vient répondre à ces problématiques. Les modèles sont cachés derrière les API, les datascientist et les applications peuvent venir les requêter et n’ont pas besoin de s’occuper de l’infrastructure. Ainsi, plutôt que chaque datascientist déploie un même modèle avec réservation de GPU, l’architecture en API permet la mise en commun du déploiement au même besoin.\n\nDans ce guide, FastChat est présenté comme exemple pour la simplicité mais d’autres solutions existent, avec chacunes leurs avantages et inconvénients.\n\n\n\n\n\nFastChat propose des API OpenAI-compatibles pour ses modèles pris en charge, de sorte que vous puissiez utiliser FastChat comme une alternative locale aux API OpenAI. Cela permet d’utiliser la bibliothèque openai-python et les commandes cURL, ce qui facilite le travail des datascientists.\nLa documentation complète est disponible sur le repo du module : https://github.com/lm-sys/FastChat/tree/main\nNous allons tout de même parcourir les grandes étapes pour pouvoir lancer son installation et ensuite l’utiliser.\n\n\nTout repose sur la complémentarité de trois services : le controller, les modèles et l’API. Il faut commencer par lancer le controller.\npython3 -m fastchat.serve.controller\nEnsuite, les model_workers. (Un modèle vicuna est pris pour l’exemple.)\npython3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5\nEt enfin, l’API.\npython3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n\n\n\nLe but de openai_api_server.py est d’implémenter un serveur d’API entièrement compatible avec OpenAI, de sorte que les modèles puissent être utilisés directement avec la bibliothèque openai-python.\nTout d’abord, installez le package Python OpenAI &gt;= 1.0 :\npip install --upgrade openai\nEnsuite, interagissez avec le modèle Vicuna :\nimport openai\n\nopenai.api_key = \"EMPTY\"\nopenai.base_url = \"http://localhost:8000/v1/\"\n\nmodel = \"vicuna-7b-v1.5\"\nprompt = \"Il était une fois\"\n\n# créer une complétion\ncompletion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)\n# imprimer la complétion\nprint(prompt + completion.choices[0].text)\n\n# créer une complétion de chat\ncompletion = openai.chat.completions.create(\n  model=model,\n  messages=[{\"role\": \"user\", \"content\": \"Bonjour ! Quel est votre nom ?\"}]\n)\n# imprimer la complétion\nprint(completion.choices[0].message.content)\n\n\n\ncURL est un autre bon outil pour observer la sortie de l’API.\nList Models:\ncurl http://localhost:8000/v1/models\nChat Completions:\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n  }'\nText Completions:\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 41,\n    \"temperature\": 0.5\n  }'\nEmbeddings:\ncurl http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"input\": \"Hello world!\"\n  }'\n\n\n\nVous pouvez utiliser vLLM comme une mise en œuvre optimisée d’un travailleur dans FastChat. Il offre une mise en batch continue avancée et un débit beaucoup plus élevé (~10x). Consultez la liste des modèles pris en charge ici : https://docs.vllm.ai/en/latest/models/supported_models.html\nIl suffit de remplacer le model_worker par le vllm_worker\npython3 -m fastchat.serve.vllm_worker --model-path lmsys/vicuna-7b-v1.5\n\n\n\nPour permettre le lancement et l’arrêt de modèles, et pour éviter qu’une erreur dans un des modèles ne déregle l’ensemble du système, une bonne pratique est souvent de conteneuriser les différentes parties. Cela necessite la préparation de quelques fichiers et quelques tests, mais ensuite, cela assure la reproductibilité de votre infrastructure. Une fois que les images sont préparées, on peut les arrêter, les relancer et les reproduire autant de fois que nécessaire.\nUne façon d’implémenter vos services avec FastChat est de faire :\n\nUn conteneur pour le controller\nUn conteneur pour l’API OpenAI like\nUn conteneur par modèle\n\nLes conteneurs pourront tous avoir la même image de base où l’on a installé les packages necessaires, comme vllm et notamment FastChat, que l’on a téléchargé et copié dans notre arbre local :\nDockerfile :\nFROM nvidia/cuda:12.2.0-devel-ubuntu20.04\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 python3.9-distutils curl\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copiez le répertoire FastChat dans le conteneur Docker\nCOPY ./FastChat_k /FastChat\n# COPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.4.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n# nvidia/cuda:12.2.0-runtime-ubuntu20.04 docker pull nvidia/cuda:12.2.0-devel-ubuntu20.04\nEnsuite, il faut lancer les conteneurs docker avec la bonne commande pour que chaque docker remplisse bien sa fonction. Cela se gère avec des docker_compose.yml\nLe fichier de déploiement des deux conteneurs obligatoires ressemblera à cela :\nversion: \"3.9\"\nservices:\n  fastchat-controller:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"21001:21001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n  fastchat-openai:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1,key2,key3\", \"--controller-address\", \"http://0.0.0.0:21001\"]\nEt le fichier de déploiement d’un modèle pourrait ressembler à ceci :\nversion: \"3.9\"\nservices:\n  fastchat-model-mixtral-latest:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - /data/models:/data/models\n      - ./FastChat:/FastChat  \n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n      TRANSFORMERS_OFFLINE: 1\n    image: fastchat:cudadevel-latest \n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: [\"1\", \"5\"]\n              capabilities: [gpu]\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\",\n    \"--worker-address\", \"http://0.0.0.0:26003\", \"--host\", \"0.0.0.0\", \"--port\", \"26003\", \"--controller\", \"http://0.0.0.0:21001\", \n    \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"] #  \"--quantization\", \"awq\" \"--num-gpus\", \"2\"\nPour finir, il suffit de lancer les commandes associées à chaque docker_compose pour lancer tous les services. Par exemple,\n# Define your Docker Compose files\ncompose_openai_service=\"docker-compose_openai.yml\"\ncompose_mixtral=\"docker-compose_mistral.yml\"\n\n# Execute Docker Compose commands\necho \"Executing Docker Compose for $compose_openai_service\"\ndocker compose -f $compose_openai_service up -d\n\necho \"Executing Docker Compose for $compose_mixtral\"\ndocker compose -f $compose_mixtral up -d\nA ce stade, vous avez déjà une installation utilisable par plusieurs personnes (à condition que l’url soit accessible). Voici des exemples de code de cellules notebooks.\nimport openai\nimport requests\nimport json\n# to get proper authentication, make sure to use a valid key that's listed in\n# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.\nopenai.api_key = \"key1\" # 1rentrez l'api key\nopenai.api_base = \"host\" # mettre l'url du serveur\n#eventuellement régler des problèmes de proxy\nmodels = openai.Model.list()\nfor d in models[\"data\"]:\n    print(d[\"id\"])\n# Instruct mode\nprompt = \"\"\"Bonjour toi. Donne moi un pays qui commence par F.\n\"\"\"\n\ncompletion = openai.Completion.create(\n    model=\"mixtral?\", \n    prompt=prompt, \n    max_tokens=25,\n    temperature=0.5,\n    top_p=1\n)\n# print the completion\nprint(completion.choices[0].text)",
    "crumbs": [
      "III-Deploiements",
      "Socle minimal"
    ]
  },
  {
    "objectID": "III-Deploiements/1_Socle_minimal.html#socle-minimal-pour-un-llm-camille-jérôme-conrad",
    "href": "III-Deploiements/1_Socle_minimal.html#socle-minimal-pour-un-llm-camille-jérôme-conrad",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Pour déployer un grand modèle de langage (LLM) dans une infrastructure, il est essentiel de comprendre comment requêter le modèle, les quelques couches techniques immédiates qui l’entourent et les solutions disponibles pour un déploiement efficace.\n\n\nLorsqu’il s’agit de mettre en service des applications basées sur des LLM, il y a 2 composants principaux : le moteur et le serveur. Le moteur gère tout ce qui concerne les modèles et le regroupement des demandes, tandis que le serveur gère l’acheminement des demandes des utilisateurs.\n\n\nLes moteurs sont les composants exécutant les modèles et tout ce que nous avons couvert jusqu’à présent sur le processus de génération avec différents types d’optimisations. À leur cœur, ce sont des bibliothèques Python. Ils gèrent le regroupement des demandes qui proviennent des utilisateurs vers notre chatbot et génèrent la réponse à ces demandes.\n\n\n\nLes serveurs sont responsables de l’orchestration des requêtes HTTP/gRPC entrantes des utilisateurs. Dans les applications du monde réel, nous aurons de nombreux utilisateurs qui posent des questions à notre chatbot à différents moments de la journée. Les serveurs mettent ces demandes en file d’attente et les transfèrent vers le moteur pour la génération de la réponse. Les serveurs apportent également les métriques telles que le débit et la latence, qui sont importantes à suivre pour le service de modèle.\n\n\n\nMoteurs\n\nOptimisation de la mémoire\nOptimisation spécifique au modèle\nPrise en charge du regroupement\n\nServeurs\n\nAPI HTTP/gRPC\nMise en file d’attente des demandes\nMise en service de plusieurs modèles\nPrise en charge de plusieurs moteurs\n\n\n\n\n\nQuels outils sont les mieux adaptés à nos besoins ? Comment choisir ? Voici un survol rapide de grands noms du milieu pour références.\n\nUne recommandation de framework rapide à prendre en main et dont l’utilité a déjà été prouvée dans une de nos administrations se trouve à la fin et est développée dans le prochain paragraphe.\n\nMoteurs\n\nTensorRT-LLM est une bibliothèque open-source qui optimise les performances d’inférence des grands modèles de langage (LLM) en utilisant les GPU NVIDIA Tensor Core. Elle utilise le parallélisme tensoriel, propose une API Python simple et comprend des versions optimisées de LLM populaires. Elle prend en charge le batching en vol et vise à simplifier la construction et l’expérimentation de nouveaux LLM. Cependant, les utilisateurs doivent spécifier la longueur d’entrée/sortie maximale et la taille de lot avant de construire le moteur, et la gestion de la mémoire du cache KV n’est pas open source.\nvLLM est une bibliothèque à hautes performances pour l’inférence et le service LLM, axée sur le débit de service et l’efficacité mémoire grâce à son mécanisme PagedAttention. Il prend en charge le batching continu, le parallélisme GPU et la sortie en streaming, ainsi que la compatibilité OpenAI. Cependant, la mémoire peut devenir un goulot d’étranglement avec des taux de demande élevés et de grandes tailles de lot.\n\nServeurs\n\nRayLLM avec RayServe est construit sur un framework de calcul distribué qui simplifie le développement et le déploiement de modèles d’IA à grande échelle. Il prend en charge les points de terminaison multi-modèles, les fonctionnalités serveur et les optimisations via les intégrations avec vLLM et TGI.\nTriton avec TensorRT-LLM fournit un logiciel d’inférence de serveur pour le déploiement et l’exécution efficaces de LLM avec des techniques telles que le batching en vol et le cache KV paginé.\n\nMoteurs et serveurs\n\nGénération de texte Inférence (TGI) est un serveur Rust, Python et gRPC utilisé chez HuggingFace pour HuggingChat, l’API d’inférence et le point de terminaison d’inférence. Il prend en charge le batching continu, le parallélisme tensoriel, la quantification, les mécanismes d’attention, le recuit simulé des logits et des LLM spécifiques. Cependant, la licence d’utilisation a été modifiée et n’est pas gratuite pour une utilisation commerciale.\nEnfin, Fastchat est une solution auto-hébergée pour héberger des modèles d’IA génératifs et qui propose la gestion des modèles, des API OpenAI-compatibles et une web interface simple.\n\n\nNous allons développer FastChat dans la partie suivante car c’est un outil qui a été testé et qui semble fournir beaucoup des éléments nécessaires pour une utilisation de première intention.\n\n\n\n\n\n\nPour certains cas d’usage, l’enjeu est de traiter de nombreuses données avec le même mode opératoire en un coup de manière ponctuelle. C’est ce qu’on appellera le traitement par batch. Cela consiste à charger un modèle, le requêter sur un tableau de prompt et obtenir la sortie pour pouvoir l’exporter. On peut le faire avec vLLM par exemple avec un morceau de code de ce type :\nfrom vllm import LLM, SamplingParams\nimport re\nimport pandas as pd\nimport re\nimport json\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n\nlist_data = json.load(open(\"Data.json\"))\n\nlist_prompts = [ v for x,v in list_data.items()]\nlist_ids = [ x for x,v in list_data.items() ]\n\nsampling_params = SamplingParams(temperature=0.1, top_p=0.1, max_tokens=4096)\nllm = LLM(model=\"/data/models/hub/models--upstage--Llama-2-70b-instruct-v2/snapshots/36b2a974642846b40fbbafaabad936cd6f8a7632\", tensor_parallel_size=2)\nprint(\"STARTING INFERENCE\")\noutputs = llm.generate(list_prompts, sampling_params)\n\nresume = { idx:output.outputs[0].text for idx, output in zip(list_ids, outputs) }\n\njson.dump(resume, open(\"Sortie.json\", \"w\"))\nMais cette méthodologie a des limites, car cela nécessite de bloquer des gpus, ce qui entraîne des problématiques de gestion et de partage.\n\n\n\nQue ce soit une équipe de plusieurs data-scientists, ou un ensemble d’application, si les besoins sont importants, les GPUs ont tout intérêt à être partagés. Il ne sera donc pas possible que chaque script python charge son modèle en mémoire et bloque des GPUs. Il est également plus rassurant de séparer l’infrastructure GPU des utilisateurs pour que chacun travaille dans son environnement, afin d’éviter les casses accidentelles.\nLa solution qui consiste à mettre à disposition des APIs vient répondre à ces problématiques. Les modèles sont cachés derrière les API, les datascientist et les applications peuvent venir les requêter et n’ont pas besoin de s’occuper de l’infrastructure. Ainsi, plutôt que chaque datascientist déploie un même modèle avec réservation de GPU, l’architecture en API permet la mise en commun du déploiement au même besoin.\n\nDans ce guide, FastChat est présenté comme exemple pour la simplicité mais d’autres solutions existent, avec chacunes leurs avantages et inconvénients.\n\n\n\n\n\nFastChat propose des API OpenAI-compatibles pour ses modèles pris en charge, de sorte que vous puissiez utiliser FastChat comme une alternative locale aux API OpenAI. Cela permet d’utiliser la bibliothèque openai-python et les commandes cURL, ce qui facilite le travail des datascientists.\nLa documentation complète est disponible sur le repo du module : https://github.com/lm-sys/FastChat/tree/main\nNous allons tout de même parcourir les grandes étapes pour pouvoir lancer son installation et ensuite l’utiliser.\n\n\nTout repose sur la complémentarité de trois services : le controller, les modèles et l’API. Il faut commencer par lancer le controller.\npython3 -m fastchat.serve.controller\nEnsuite, les model_workers. (Un modèle vicuna est pris pour l’exemple.)\npython3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5\nEt enfin, l’API.\npython3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n\n\n\nLe but de openai_api_server.py est d’implémenter un serveur d’API entièrement compatible avec OpenAI, de sorte que les modèles puissent être utilisés directement avec la bibliothèque openai-python.\nTout d’abord, installez le package Python OpenAI &gt;= 1.0 :\npip install --upgrade openai\nEnsuite, interagissez avec le modèle Vicuna :\nimport openai\n\nopenai.api_key = \"EMPTY\"\nopenai.base_url = \"http://localhost:8000/v1/\"\n\nmodel = \"vicuna-7b-v1.5\"\nprompt = \"Il était une fois\"\n\n# créer une complétion\ncompletion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)\n# imprimer la complétion\nprint(prompt + completion.choices[0].text)\n\n# créer une complétion de chat\ncompletion = openai.chat.completions.create(\n  model=model,\n  messages=[{\"role\": \"user\", \"content\": \"Bonjour ! Quel est votre nom ?\"}]\n)\n# imprimer la complétion\nprint(completion.choices[0].message.content)\n\n\n\ncURL est un autre bon outil pour observer la sortie de l’API.\nList Models:\ncurl http://localhost:8000/v1/models\nChat Completions:\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n  }'\nText Completions:\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 41,\n    \"temperature\": 0.5\n  }'\nEmbeddings:\ncurl http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"input\": \"Hello world!\"\n  }'\n\n\n\nVous pouvez utiliser vLLM comme une mise en œuvre optimisée d’un travailleur dans FastChat. Il offre une mise en batch continue avancée et un débit beaucoup plus élevé (~10x). Consultez la liste des modèles pris en charge ici : https://docs.vllm.ai/en/latest/models/supported_models.html\nIl suffit de remplacer le model_worker par le vllm_worker\npython3 -m fastchat.serve.vllm_worker --model-path lmsys/vicuna-7b-v1.5\n\n\n\nPour permettre le lancement et l’arrêt de modèles, et pour éviter qu’une erreur dans un des modèles ne déregle l’ensemble du système, une bonne pratique est souvent de conteneuriser les différentes parties. Cela necessite la préparation de quelques fichiers et quelques tests, mais ensuite, cela assure la reproductibilité de votre infrastructure. Une fois que les images sont préparées, on peut les arrêter, les relancer et les reproduire autant de fois que nécessaire.\nUne façon d’implémenter vos services avec FastChat est de faire :\n\nUn conteneur pour le controller\nUn conteneur pour l’API OpenAI like\nUn conteneur par modèle\n\nLes conteneurs pourront tous avoir la même image de base où l’on a installé les packages necessaires, comme vllm et notamment FastChat, que l’on a téléchargé et copié dans notre arbre local :\nDockerfile :\nFROM nvidia/cuda:12.2.0-devel-ubuntu20.04\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 python3.9-distutils curl\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copiez le répertoire FastChat dans le conteneur Docker\nCOPY ./FastChat_k /FastChat\n# COPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.4.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n# nvidia/cuda:12.2.0-runtime-ubuntu20.04 docker pull nvidia/cuda:12.2.0-devel-ubuntu20.04\nEnsuite, il faut lancer les conteneurs docker avec la bonne commande pour que chaque docker remplisse bien sa fonction. Cela se gère avec des docker_compose.yml\nLe fichier de déploiement des deux conteneurs obligatoires ressemblera à cela :\nversion: \"3.9\"\nservices:\n  fastchat-controller:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"21001:21001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n  fastchat-openai:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment: \n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1,key2,key3\", \"--controller-address\", \"http://0.0.0.0:21001\"]\nEt le fichier de déploiement d’un modèle pourrait ressembler à ceci :\nversion: \"3.9\"\nservices:\n  fastchat-model-mixtral-latest:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - /data/models:/data/models\n      - ./FastChat:/FastChat  \n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n      TRANSFORMERS_OFFLINE: 1\n    image: fastchat:cudadevel-latest \n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: [\"1\", \"5\"]\n              capabilities: [gpu]\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\",\n    \"--worker-address\", \"http://0.0.0.0:26003\", \"--host\", \"0.0.0.0\", \"--port\", \"26003\", \"--controller\", \"http://0.0.0.0:21001\", \n    \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"] #  \"--quantization\", \"awq\" \"--num-gpus\", \"2\"\nPour finir, il suffit de lancer les commandes associées à chaque docker_compose pour lancer tous les services. Par exemple,\n# Define your Docker Compose files\ncompose_openai_service=\"docker-compose_openai.yml\"\ncompose_mixtral=\"docker-compose_mistral.yml\"\n\n# Execute Docker Compose commands\necho \"Executing Docker Compose for $compose_openai_service\"\ndocker compose -f $compose_openai_service up -d\n\necho \"Executing Docker Compose for $compose_mixtral\"\ndocker compose -f $compose_mixtral up -d\nA ce stade, vous avez déjà une installation utilisable par plusieurs personnes (à condition que l’url soit accessible). Voici des exemples de code de cellules notebooks.\nimport openai\nimport requests\nimport json\n# to get proper authentication, make sure to use a valid key that's listed in\n# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.\nopenai.api_key = \"key1\" # 1rentrez l'api key\nopenai.api_base = \"host\" # mettre l'url du serveur\n#eventuellement régler des problèmes de proxy\nmodels = openai.Model.list()\nfor d in models[\"data\"]:\n    print(d[\"id\"])\n# Instruct mode\nprompt = \"\"\"Bonjour toi. Donne moi un pays qui commence par F.\n\"\"\"\n\ncompletion = openai.Completion.create(\n    model=\"mixtral?\", \n    prompt=prompt, \n    max_tokens=25,\n    temperature=0.5,\n    top_p=1\n)\n# print the completion\nprint(completion.choices[0].text)",
    "crumbs": [
      "III-Deploiements",
      "Socle minimal"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Socle_Production.html",
    "href": "III-Deploiements/3_Socle_Production.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Les LLM sont des modèles de langage puissants qui nécessitent des ressources informatiques importantes pour fonctionner efficacement. Pour mettre en production une application utilisant des LLM, il est essentiel de choisir le bon matériel et les bons outils pour garantir la disponibilité des applications et des performances optimales.\nPour certains modèles de langage, les processeurs habituels appelés CPU (Central Processing Unit) peuvent suffire. Mais pour la plupart des modèles plus importants, pour que les calculs se fassent dans des temps raisonnables, il est nécessaire de se doter d’unités de traitement graphique (GPU). Nous allons donc nous intéresser ici aux différents critères à étudier pour choisir correctement des GPUs, aux outils qui permettent de suivre leurs performances et enfin le lien avec les essentiels de déploiements en termes de management de ressources (avec l’exemple du lien à Kubernetes).\n\n\nLa sélection des GPU (Graphics Processing Units) pour une installation dans une structure dépend de multiples facteurs. En voici quelques uns :\n\nPuissance de calcul : La puissance de traitement des GPU est mesurée en flops (floating-point operations per second). Un GPU plus puissant permettra d’exécuter des tâches plus rapidement et de gérer des charges de travail plus élevées.\nMémoire vive : La mémoire vive (VRAM) des GPU est essentielle pour les applications nécessitant une grande quantité de mémoire, comme les simulations scientifiques ou les applications de traitement d’image. Assurez-vous de choisir des GPU avec suffisamment de mémoire vive pour répondre aux besoins de vos applications.\nÉnergie et consommation : Les GPU consomment de l’énergie et génèrent de la chaleur. Choisissez des GPU économes en énergie et dotés de systèmes de refroidissement efficaces pour réduire les coûts énergétiques et améliorer la durée de vie des composants.\nCoût et rentabilité : Évaluez le coût total de possession (TCO) des GPU, en tenant compte des coûts d’achat, de maintenance et d’énergie. Choisissez des GPU qui offrent une bonne rentabilité pour votre administration.\nCompatibilité avec les systèmes d’exploitation : Vérifiez que les GPU sont compatibles avec les systèmes d’exploitation utilisés dans votre administration, tels que Windows, Linux ou macOS.\n\nPour le dernier point, il est commun d’acheter les GPUs par plusieurs, déjà groupés dans des serveurs. Il faut faire attention cependant au format et aux besoins spécifiques de ces serveurs, qui ne sont souvent pas standards par leur taille et par la chaleur qu’ils dégagent.\nDes GPUs reconnus peuvent être les T5, A100, V100 et leur prix d’achat est de l’ordre de milliers d’euros, mais il faut bien prendre en compte également les coûts cachés. En effet, l’intégration dans un SI pré-existant peut nécessiter des travaux. Durant leur cycle de vie, ils ont besoin de maintenance. Et enfin, tout au long de leur utilisation, ils ont besoin d’être administrés, ce qui peut représenter des Equivalents Temps Plein (ETP), dont le coût n’est pas à négliger.\n\n\n\nIl est judicieux d’utiliser un orchestrateur pour déployer des Language Models (LLMs) dans une organisation pour plusieurs raisons :\n\nSimplification de la gestion des déploiements : un orchestrateur permet de gérer de manière centralisée tous les déploiements de LLMs dans l’organisation. Cela facilite la surveillance, la maintenance et la mise à l’échelle des déploiements.\nÉvolutivité : un orchestrateur permet de mettre à l’échelle automatiquement les déploiements en fonction de la demande, ce qui est particulièrement utile pour les LLMs qui peuvent être très gourmands en ressources.\nSécurité : un orchestrateur peut aider à renforcer la sécurité en fournissant des fonctionnalités telles que l’authentification, l’autorisation et le chiffrement des données. Il peut également aider à respecter les normes de conformité en matière de traitement des données.\nGestion des versions : un orchestrateur permet de gérer les versions des LLMs et de faciliter le déploiement de nouvelles versions ou de rollbacks en cas de problème.\nIntégration avec d’autres outils : un orchestrateur peut s’intégrer facilement avec d’autres outils de développement et d’exploitation, tels que les systèmes de surveillance, les outils de débogage et les systèmes de journalisation.\nRéduction des coûts : en automatisant les déploiements et en les mettant à l’échelle de manière efficace, un orchestrateur peut aider à réduire les coûts associés aux déploiements de LLMs.\n\nEn résumé, un orchestrateur offre une gestion centralisée, une évolutivité, une sécurité renforcée, une gestion des versions, une intégration avec d’autres outils et une réduction des coûts pour les déploiements de LLMs dans une organisation. Des solutions techniques peuvent être :\n\nKubernetes\nDocker Swarm\nApache Mesos\n\n\n\n\nNous allons développer dans cette partie un exemple de déploiement d’une structure LLM avec Kubernetes. On utilise la même structure de microservices que dans la partie précedente avec FastChat mais cela peut être adapté à tout choix d’organisation et d’architecture.\nVoici un schéma résumant l’organisation proposée ici, avec le controller, l’api openai-like et deux modèles LLMs :\n\n\n\nSchéma de structure des services pour Kubernetes\n\n\nLa méthodologie générale de l’utilisation de Kubernetes est la suivante :\n\nPréparer les images Docker qui seront utilisées pour les déploiements\nCréez les fichiers de configuration YAML pour votre application\nDéployez les avec :\n\nkubectl apply -f FILENAMES.yaml\n\nSurveiller le lancement des différents services et leur bonne interconnexion\n\nAvec cela, vous avez une application plus robuste, mais cela necessite une certaine familiarité avec Kubernetes. Quelques exemples de fichiers de configuration sont proposés ci-dessous.\n\nTout d’abord les services obligatoires comme le gestionnaire de l’API et le controlleur. On fait en même temps le deployment du pod et le service permettant d’y accéder. Ils se basent sur une image Docker légère et sans requirements spécifiques.\n\nOn remarquera que les deux deploiments semblent assez similaires et que la principale différence réside dans les noms donnés aux objets et à la commande lancée dans le conteneur lancé :\n[\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\nou\n[\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1\", \"--controller-address\", \"http://svc-controller\"]\nCes commandes sont celles de FastChat mais peuvent être remplacées par votre propre solution de déploiement de modèle.\nPour le controlleur :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-controller\n  template:\n    metadata:\n      labels:\n        app: fastchat-controller\n    spec:\n      containers:\n      - name: fastchat-controller\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 21001\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-llm-mixtral,svc-llm-e5-dgfip,svc-llm-llama\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-controller2\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30001\n      port: 80\n      targetPort: 21001\n  selector:\n    app: fastchat-controller\nEt pour l’api :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-openai\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-openai\n  template:\n    metadata:\n      labels:\n        app: fastchat-openai\n    spec:\n      containers:\n      - name: fastchat-openai\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller,svc-llm-mixtral\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1\", \"--controller-address\", \"http://svc-controller\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-openai-api\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30081\n      port: 80\n      targetPort: 8000\n  selector:\n    app: fastchat-openai\n\nCréez les fichiers de configuration pour un modèle LLM, avec également le pod et le service correspondant. Cette fois-ci, l’image est plus lourde car elle contient le modèle et les modules nécessaires à son fonctionnement.\n\nOn remarquera notamment :\nimage: fastchat-mixtral:v0.3.1\nresources:\n          limits:\n            nvidia.com/gpu: 2\nEt la commande qui lance le modèle (ici Fastchat mais pourrait être n’importe quel module):\ncommand: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-mixtral\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llm-mixtral\n  template:\n    metadata:\n      labels:\n        app: llm-mixtral\n    spec:\n      containers:\n      - name: llm-mixtral\n        image: fastchat-mixtral:v0.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 2100\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0,1\"\n        command: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-llm-mixtral\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30091\n      port: 80\n      targetPort: 2100\n  selector:\n    app: llm-mixtral\nEnfin, tous ces composants se basent sur des images docker qui continennent tout le code de mise à disposition des modèles ou des APIs. Des exemples d’images utiles pour les différents services sus-mentionnés sont décrites dans ce Dockerfile :\n#################### BASE OPENAI IMAGE ####################\nFROM python:3.9-buster as llm-api-light\n\n# Set environment variables\nENV DEBIAN_FRONTEND noninteractive\n# Install dependencies\nRUN apt-get update -y && apt-get install -y curl\n# Install pip\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copy the FastChat directory into the Docker container\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n# Go into the FastChat directory and install from this directory\nWORKDIR /FastChat\nRUN pip3 install -e \".[webui]\" pydantic==1.10.13\nRUN pip3 install plotly\n\n#################### BASE LLM BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 curl\nRUN apt-get install -y python3-pip git\n# Copiez le répertoire FastChat dans le conteneur DockerEnfin, tous c\n\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install --upgrade pip\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.3.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-llama\nCOPY ./models/Upstage--Llama-2-70b-instruct-v2 /data/models/vllm/Upstage--Llama-2-70b-instruct-v2\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-mixtral\nCOPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1",
    "crumbs": [
      "III-Deploiements",
      "Socle Production"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Socle_Production.html#socle-pour-production-camille-jérôme-conrad",
    "href": "III-Deploiements/3_Socle_Production.html#socle-pour-production-camille-jérôme-conrad",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Les LLM sont des modèles de langage puissants qui nécessitent des ressources informatiques importantes pour fonctionner efficacement. Pour mettre en production une application utilisant des LLM, il est essentiel de choisir le bon matériel et les bons outils pour garantir la disponibilité des applications et des performances optimales.\nPour certains modèles de langage, les processeurs habituels appelés CPU (Central Processing Unit) peuvent suffire. Mais pour la plupart des modèles plus importants, pour que les calculs se fassent dans des temps raisonnables, il est nécessaire de se doter d’unités de traitement graphique (GPU). Nous allons donc nous intéresser ici aux différents critères à étudier pour choisir correctement des GPUs, aux outils qui permettent de suivre leurs performances et enfin le lien avec les essentiels de déploiements en termes de management de ressources (avec l’exemple du lien à Kubernetes).\n\n\nLa sélection des GPU (Graphics Processing Units) pour une installation dans une structure dépend de multiples facteurs. En voici quelques uns :\n\nPuissance de calcul : La puissance de traitement des GPU est mesurée en flops (floating-point operations per second). Un GPU plus puissant permettra d’exécuter des tâches plus rapidement et de gérer des charges de travail plus élevées.\nMémoire vive : La mémoire vive (VRAM) des GPU est essentielle pour les applications nécessitant une grande quantité de mémoire, comme les simulations scientifiques ou les applications de traitement d’image. Assurez-vous de choisir des GPU avec suffisamment de mémoire vive pour répondre aux besoins de vos applications.\nÉnergie et consommation : Les GPU consomment de l’énergie et génèrent de la chaleur. Choisissez des GPU économes en énergie et dotés de systèmes de refroidissement efficaces pour réduire les coûts énergétiques et améliorer la durée de vie des composants.\nCoût et rentabilité : Évaluez le coût total de possession (TCO) des GPU, en tenant compte des coûts d’achat, de maintenance et d’énergie. Choisissez des GPU qui offrent une bonne rentabilité pour votre administration.\nCompatibilité avec les systèmes d’exploitation : Vérifiez que les GPU sont compatibles avec les systèmes d’exploitation utilisés dans votre administration, tels que Windows, Linux ou macOS.\n\nPour le dernier point, il est commun d’acheter les GPUs par plusieurs, déjà groupés dans des serveurs. Il faut faire attention cependant au format et aux besoins spécifiques de ces serveurs, qui ne sont souvent pas standards par leur taille et par la chaleur qu’ils dégagent.\nDes GPUs reconnus peuvent être les T5, A100, V100 et leur prix d’achat est de l’ordre de milliers d’euros, mais il faut bien prendre en compte également les coûts cachés. En effet, l’intégration dans un SI pré-existant peut nécessiter des travaux. Durant leur cycle de vie, ils ont besoin de maintenance. Et enfin, tout au long de leur utilisation, ils ont besoin d’être administrés, ce qui peut représenter des Equivalents Temps Plein (ETP), dont le coût n’est pas à négliger.\n\n\n\nIl est judicieux d’utiliser un orchestrateur pour déployer des Language Models (LLMs) dans une organisation pour plusieurs raisons :\n\nSimplification de la gestion des déploiements : un orchestrateur permet de gérer de manière centralisée tous les déploiements de LLMs dans l’organisation. Cela facilite la surveillance, la maintenance et la mise à l’échelle des déploiements.\nÉvolutivité : un orchestrateur permet de mettre à l’échelle automatiquement les déploiements en fonction de la demande, ce qui est particulièrement utile pour les LLMs qui peuvent être très gourmands en ressources.\nSécurité : un orchestrateur peut aider à renforcer la sécurité en fournissant des fonctionnalités telles que l’authentification, l’autorisation et le chiffrement des données. Il peut également aider à respecter les normes de conformité en matière de traitement des données.\nGestion des versions : un orchestrateur permet de gérer les versions des LLMs et de faciliter le déploiement de nouvelles versions ou de rollbacks en cas de problème.\nIntégration avec d’autres outils : un orchestrateur peut s’intégrer facilement avec d’autres outils de développement et d’exploitation, tels que les systèmes de surveillance, les outils de débogage et les systèmes de journalisation.\nRéduction des coûts : en automatisant les déploiements et en les mettant à l’échelle de manière efficace, un orchestrateur peut aider à réduire les coûts associés aux déploiements de LLMs.\n\nEn résumé, un orchestrateur offre une gestion centralisée, une évolutivité, une sécurité renforcée, une gestion des versions, une intégration avec d’autres outils et une réduction des coûts pour les déploiements de LLMs dans une organisation. Des solutions techniques peuvent être :\n\nKubernetes\nDocker Swarm\nApache Mesos\n\n\n\n\nNous allons développer dans cette partie un exemple de déploiement d’une structure LLM avec Kubernetes. On utilise la même structure de microservices que dans la partie précedente avec FastChat mais cela peut être adapté à tout choix d’organisation et d’architecture.\nVoici un schéma résumant l’organisation proposée ici, avec le controller, l’api openai-like et deux modèles LLMs :\n\n\n\nSchéma de structure des services pour Kubernetes\n\n\nLa méthodologie générale de l’utilisation de Kubernetes est la suivante :\n\nPréparer les images Docker qui seront utilisées pour les déploiements\nCréez les fichiers de configuration YAML pour votre application\nDéployez les avec :\n\nkubectl apply -f FILENAMES.yaml\n\nSurveiller le lancement des différents services et leur bonne interconnexion\n\nAvec cela, vous avez une application plus robuste, mais cela necessite une certaine familiarité avec Kubernetes. Quelques exemples de fichiers de configuration sont proposés ci-dessous.\n\nTout d’abord les services obligatoires comme le gestionnaire de l’API et le controlleur. On fait en même temps le deployment du pod et le service permettant d’y accéder. Ils se basent sur une image Docker légère et sans requirements spécifiques.\n\nOn remarquera que les deux deploiments semblent assez similaires et que la principale différence réside dans les noms donnés aux objets et à la commande lancée dans le conteneur lancé :\n[\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\nou\n[\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1\", \"--controller-address\", \"http://svc-controller\"]\nCes commandes sont celles de FastChat mais peuvent être remplacées par votre propre solution de déploiement de modèle.\nPour le controlleur :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-controller\n  template:\n    metadata:\n      labels:\n        app: fastchat-controller\n    spec:\n      containers:\n      - name: fastchat-controller\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 21001\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-llm-mixtral,svc-llm-e5-dgfip,svc-llm-llama\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-controller2\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30001\n      port: 80\n      targetPort: 21001\n  selector:\n    app: fastchat-controller\nEt pour l’api :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-openai\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-openai\n  template:\n    metadata:\n      labels:\n        app: fastchat-openai\n    spec:\n      containers:\n      - name: fastchat-openai\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller,svc-llm-mixtral\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1\", \"--controller-address\", \"http://svc-controller\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-openai-api\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30081\n      port: 80\n      targetPort: 8000\n  selector:\n    app: fastchat-openai\n\nCréez les fichiers de configuration pour un modèle LLM, avec également le pod et le service correspondant. Cette fois-ci, l’image est plus lourde car elle contient le modèle et les modules nécessaires à son fonctionnement.\n\nOn remarquera notamment :\nimage: fastchat-mixtral:v0.3.1\nresources:\n          limits:\n            nvidia.com/gpu: 2\nEt la commande qui lance le modèle (ici Fastchat mais pourrait être n’importe quel module):\ncommand: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-mixtral\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llm-mixtral\n  template:\n    metadata:\n      labels:\n        app: llm-mixtral\n    spec:\n      containers:\n      - name: llm-mixtral\n        image: fastchat-mixtral:v0.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 2100\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0,1\"\n        command: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-llm-mixtral\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30091\n      port: 80\n      targetPort: 2100\n  selector:\n    app: llm-mixtral\nEnfin, tous ces composants se basent sur des images docker qui continennent tout le code de mise à disposition des modèles ou des APIs. Des exemples d’images utiles pour les différents services sus-mentionnés sont décrites dans ce Dockerfile :\n#################### BASE OPENAI IMAGE ####################\nFROM python:3.9-buster as llm-api-light\n\n# Set environment variables\nENV DEBIAN_FRONTEND noninteractive\n# Install dependencies\nRUN apt-get update -y && apt-get install -y curl\n# Install pip\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copy the FastChat directory into the Docker container\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n# Go into the FastChat directory and install from this directory\nWORKDIR /FastChat\nRUN pip3 install -e \".[webui]\" pydantic==1.10.13\nRUN pip3 install plotly\n\n#################### BASE LLM BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 curl\nRUN apt-get install -y python3-pip git\n# Copiez le répertoire FastChat dans le conteneur DockerEnfin, tous c\n\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n\n# Allez dans le répertoire FastChat et installez à partir de ce répertoire\nWORKDIR /FastChat\nRUN pip3 install --upgrade pip\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.3.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-llama\nCOPY ./models/Upstage--Llama-2-70b-instruct-v2 /data/models/vllm/Upstage--Llama-2-70b-instruct-v2\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-mixtral\nCOPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1",
    "crumbs": [
      "III-Deploiements",
      "Socle Production"
    ]
  },
  {
    "objectID": "Reste_a_faire.html",
    "href": "Reste_a_faire.html",
    "title": "Guide d'installation des LLM",
    "section": "",
    "text": "Format et contenu du guide :\n\nFinaliser fusion I-1 et I-2\nPartie Acculturation (I-3) ?\nAjout d’exemple d’implémentation RAG II-2\nFinaliser la partie évaluation II-3\nUniformiser le message de la partie III\nNormaliser la mise en page\nCompléter la bibliographie\nCompléter les cas d’usages dans l’administration (contacts)\n\nProposition de mise en page :\n---\ntitle: &lt;Titre de la partie&gt;(ex: RAG)\n---\n# &lt;section 1&gt;\n## &lt;sous section 1&gt;\n## &lt;sous section 2&gt;\n# &lt;section 2&gt;"
  },
  {
    "objectID": "III-Deploiements/2_Socle_avance.html",
    "href": "III-Deploiements/2_Socle_avance.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Optimisation, Monitoring, UX/UI (CODE!)\n\n\nUne fois l’infrastructure sécurisée, il est toujours utile de monitorer les performances des GPU, pour suivre l’impact de cette technologie, pour monitorer la charge et prévenir de la surcharge. Idéallement, l’on peut aussi imaginer suivre la consommation projet par projet pour reporter les lignes de budget et faire des bilans carbonne.\nSelon les technologies de GPUs utilisées, il existe différents outils qui se conncectent aux infrastructure pour fournir des statistiques (notamment la mémoire utilisée, la bande passante et la température) :\n\nnvidia-smi\nAMD Vantage\nGPU-Z\n\nVoici un exemple de résultat de statistiques extraites d’une infrastructure GPUs :\n\n\n\nResultat de la commande nvidia-smi\n\n\nIl existe également d’autres moyens d’accéder à des GPUs que l’acquisition individuelle pour les administrations (voir Partie III.4).\n\n\n\nPlusieurs initiatives permettent de déployer rapidement des interfaces de chat avec des modèles LLMs, voire des applications de RAG avec back et front. On peut remarquer :\n\nla WebUI du module FastChat\nl’application CARADOC, mise en open source par l’équipe DataScience de la DTNUM de la DGFiP, publication prévue pour fin juin 2024.\n\nAperçu de l’application CARADOC pendant ses développements :\n\n\n\nInterface de l’application RAG Caradoc\n\n\nAperçu d’un interface possible avec FastChat :\n\n\n\nInterface de l’application Chat avec FastChat\n\n\nExemple de code pour lancer l’interface Gradio de FastChat dans un Docker :\nversion: \"3.9\"\nservices:\n  fastchat-gradio-server: \n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      FASTCHAT_CONTROLLER_URL: http://0.0.0.0:21001\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    image: fastchat:latest\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.gradio_web_server_dtnum\", \"--controller-url\", \"http://0.0.0.0:21001\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--model-list-mode\", \"reload\"]\nAvec toujours l’image Docker qui contient FastChat.",
    "crumbs": [
      "III-Deploiements",
      "Socle avancé"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Socle_avance.html#socle-avancé-camille-jérôme-conrad",
    "href": "III-Deploiements/2_Socle_avance.html#socle-avancé-camille-jérôme-conrad",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Optimisation, Monitoring, UX/UI (CODE!)\n\n\nUne fois l’infrastructure sécurisée, il est toujours utile de monitorer les performances des GPU, pour suivre l’impact de cette technologie, pour monitorer la charge et prévenir de la surcharge. Idéallement, l’on peut aussi imaginer suivre la consommation projet par projet pour reporter les lignes de budget et faire des bilans carbonne.\nSelon les technologies de GPUs utilisées, il existe différents outils qui se conncectent aux infrastructure pour fournir des statistiques (notamment la mémoire utilisée, la bande passante et la température) :\n\nnvidia-smi\nAMD Vantage\nGPU-Z\n\nVoici un exemple de résultat de statistiques extraites d’une infrastructure GPUs :\n\n\n\nResultat de la commande nvidia-smi\n\n\nIl existe également d’autres moyens d’accéder à des GPUs que l’acquisition individuelle pour les administrations (voir Partie III.4).\n\n\n\nPlusieurs initiatives permettent de déployer rapidement des interfaces de chat avec des modèles LLMs, voire des applications de RAG avec back et front. On peut remarquer :\n\nla WebUI du module FastChat\nl’application CARADOC, mise en open source par l’équipe DataScience de la DTNUM de la DGFiP, publication prévue pour fin juin 2024.\n\nAperçu de l’application CARADOC pendant ses développements :\n\n\n\nInterface de l’application RAG Caradoc\n\n\nAperçu d’un interface possible avec FastChat :\n\n\n\nInterface de l’application Chat avec FastChat\n\n\nExemple de code pour lancer l’interface Gradio de FastChat dans un Docker :\nversion: \"3.9\"\nservices:\n  fastchat-gradio-server: \n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      FASTCHAT_CONTROLLER_URL: http://0.0.0.0:21001\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    image: fastchat:latest\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./FastChat:/FastChat \n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.gradio_web_server_dtnum\", \"--controller-url\", \"http://0.0.0.0:21001\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--model-list-mode\", \"reload\"]\nAvec toujours l’image Docker qui contient FastChat.",
    "crumbs": [
      "III-Deploiements",
      "Socle avancé"
    ]
  },
  {
    "objectID": "III-Deploiements/4_Infras_administrations.html",
    "href": "III-Deploiements/4_Infras_administrations.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Dans beaucoup de cas l’accès à des GPUs est un des principaux freins à l’expérimentaion et la mise en production d’un cas d’usage d’IA générative. L’acquisition d’un cluster GPUs n’est pas toujours une possibilité pour des questions budgétaires ou techniques. Cependant, plusieurs alternatives sont envisageables (ou en cours de construction) à destination des administrations pour externaliser cette infrastructure.\nCette approche est d’ailleurs en phase avec la directive “cloud au centre” qui encourage l’utilisation d’infrastructure externalisée pour les projets informatiques de l’administration.\nDans ce cadre, la principale variable a prendre en compte sont les contraintes de sécurité de l’application. Cette question va à la fois déterminer les solutions accessibles et imposer des choix architecturaux. Un exemple d’architecture d’application d’IA générative est donné dans le rapport de l’ANSSI sur l’IA générative :\n\nTout ou parti de ces éléments peuvent être externalisé en fonction de la maturité de l’administration, du besoin utilisateur et des contraintes de sécurité.\n3 principales solutions d’externalisation sont possibles :\n\nCloud Public\nCloud externe\nAPI inférence\n\n\n\nBien que l’état dispose de plusieurs offres cloud internes, la mise à disposition de GPU est encore très peu mature et peu développée\n\nSSP Cloud : A ce jour, le SSP Cloud via sa plateforme ONYXIA (hébergée et dévelopée par l’INSEE), est la principale plateforme publique mettant à disposition des GPUs à ses utilisateurs. Les ressources sont cependant très limitées et la plateforme est plus orientée autour du développement de projet que de la mise en production. cf Déploiement d’un LLM sur SSP Cloud\nCloud pi : Cloud PI est le cloud du ministère de l’intérieur, il ne semble pas proposer à date de provisionnement de GPUs. Il fournit cependant une offre IAAS et PAAS pour l’hébergement d’applications.\nNubo : Nubo est le service cloud du ministère de l’économie et des finances, qui propose un service IAAS. Via sa solution Nubonyxia (implémentation d’Onyxia sur les infrastructures Nubo), il\n\n\nPour définition de ce que recouvre les offres de service PAAS et IAAS, se référer à ce lien\n\n\n\n\nLa qualificiation SecNumCloud a été mis en place par l’ANSSI pour assurer des normes de sécurité aux utilisateurs de produits cloud. A ce jour, peu d’entreprises ont acquis cette qualification. Voici quelques exemples de fournisseurs :\n\nDassault - Outscale IAAS avec accès GPU\nThales - Sens (Implémentation de GCP sur une infrastructure sécurisée) PAAS\nCloud Temple IAAS \n\n\nPlus d’informations sur ce type de services sont disponibles ici.\n\n\n\n\n\nAPI Albert : Offre fourni à quelques partenaires de la DINUM, avec des capacités d’hébergement limitées. &gt; (ajouter contact ou moyen d’accès)\nExterne non sécurisée : Ces solutions sont envisageables où les besoins en performance sont importants et les contraintes de sécurité sont faibles. Voici quelques exemples de solutions :\n\nMistral API\nHugging face - inference endpoint\nGroq\n\n\n\nDans certains cas, il peut être aussi intéressant de mettre en place une architecture hybride Cloud + API d’inférence. Ce qui permet de bénéficier de l’agilité de développement des solutions Cloud, tout en limitant les coûts relatifs à l’approvisionnement de GPUs.\n\n\n\n\nSur le DataLab SSP Cloud, il est possible de déployer des LLM à des fins d’expérimentation. Plusieurs cas sont possibles :\nA. Utiliser des librairies d’API de LLM (vLLM, etc.) B. Déployer des containers Docker avec Kube et Helm\n\n\n\nVous pouvez lancer un service VSCode avec une GPU et installer une API de LLM\n\n\n\n\n\nCréer une image Docker et la mettre à disposition (Dockerhub) : exemple applicatif avec Streamlit\nDéployer avec Kube et Helm en utilisant un service VSCode avec les droits d’admin pour Kube\n\nExemple avec Kube :\nkubectl create deployment mon-deploiement --image=mon-image-docker\nkubectl proxy",
    "crumbs": [
      "III-Deploiements",
      "Infrastructures dans l'administration"
    ]
  },
  {
    "objectID": "III-Deploiements/4_Infras_administrations.html#infras-dispos-pour-ladministration-thibault-katia",
    "href": "III-Deploiements/4_Infras_administrations.html#infras-dispos-pour-ladministration-thibault-katia",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Dans beaucoup de cas l’accès à des GPUs est un des principaux freins à l’expérimentaion et la mise en production d’un cas d’usage d’IA générative. L’acquisition d’un cluster GPUs n’est pas toujours une possibilité pour des questions budgétaires ou techniques. Cependant, plusieurs alternatives sont envisageables (ou en cours de construction) à destination des administrations pour externaliser cette infrastructure.\nCette approche est d’ailleurs en phase avec la directive “cloud au centre” qui encourage l’utilisation d’infrastructure externalisée pour les projets informatiques de l’administration.\nDans ce cadre, la principale variable a prendre en compte sont les contraintes de sécurité de l’application. Cette question va à la fois déterminer les solutions accessibles et imposer des choix architecturaux. Un exemple d’architecture d’application d’IA générative est donné dans le rapport de l’ANSSI sur l’IA générative :\n\nTout ou parti de ces éléments peuvent être externalisé en fonction de la maturité de l’administration, du besoin utilisateur et des contraintes de sécurité.\n3 principales solutions d’externalisation sont possibles :\n\nCloud Public\nCloud externe\nAPI inférence\n\n\n\nBien que l’état dispose de plusieurs offres cloud internes, la mise à disposition de GPU est encore très peu mature et peu développée\n\nSSP Cloud : A ce jour, le SSP Cloud via sa plateforme ONYXIA (hébergée et dévelopée par l’INSEE), est la principale plateforme publique mettant à disposition des GPUs à ses utilisateurs. Les ressources sont cependant très limitées et la plateforme est plus orientée autour du développement de projet que de la mise en production. cf Déploiement d’un LLM sur SSP Cloud\nCloud pi : Cloud PI est le cloud du ministère de l’intérieur, il ne semble pas proposer à date de provisionnement de GPUs. Il fournit cependant une offre IAAS et PAAS pour l’hébergement d’applications.\nNubo : Nubo est le service cloud du ministère de l’économie et des finances, qui propose un service IAAS. Via sa solution Nubonyxia (implémentation d’Onyxia sur les infrastructures Nubo), il\n\n\nPour définition de ce que recouvre les offres de service PAAS et IAAS, se référer à ce lien\n\n\n\n\nLa qualificiation SecNumCloud a été mis en place par l’ANSSI pour assurer des normes de sécurité aux utilisateurs de produits cloud. A ce jour, peu d’entreprises ont acquis cette qualification. Voici quelques exemples de fournisseurs :\n\nDassault - Outscale IAAS avec accès GPU\nThales - Sens (Implémentation de GCP sur une infrastructure sécurisée) PAAS\nCloud Temple IAAS \n\n\nPlus d’informations sur ce type de services sont disponibles ici.\n\n\n\n\n\nAPI Albert : Offre fourni à quelques partenaires de la DINUM, avec des capacités d’hébergement limitées. &gt; (ajouter contact ou moyen d’accès)\nExterne non sécurisée : Ces solutions sont envisageables où les besoins en performance sont importants et les contraintes de sécurité sont faibles. Voici quelques exemples de solutions :\n\nMistral API\nHugging face - inference endpoint\nGroq\n\n\n\nDans certains cas, il peut être aussi intéressant de mettre en place une architecture hybride Cloud + API d’inférence. Ce qui permet de bénéficier de l’agilité de développement des solutions Cloud, tout en limitant les coûts relatifs à l’approvisionnement de GPUs.\n\n\n\n\nSur le DataLab SSP Cloud, il est possible de déployer des LLM à des fins d’expérimentation. Plusieurs cas sont possibles :\nA. Utiliser des librairies d’API de LLM (vLLM, etc.) B. Déployer des containers Docker avec Kube et Helm\n\n\n\nVous pouvez lancer un service VSCode avec une GPU et installer une API de LLM\n\n\n\n\n\nCréer une image Docker et la mettre à disposition (Dockerhub) : exemple applicatif avec Streamlit\nDéployer avec Kube et Helm en utilisant un service VSCode avec les droits d’admin pour Kube\n\nExemple avec Kube :\nkubectl create deployment mon-deploiement --image=mon-image-docker\nkubectl proxy",
    "crumbs": [
      "III-Deploiements",
      "Infrastructures dans l'administration"
    ]
  },
  {
    "objectID": "Bibliographie.html",
    "href": "Bibliographie.html",
    "title": "Bibliographie",
    "section": "",
    "text": "Qu’est ce que l’IA ?\nWhat We Learned from a Year of Building with LLMs (Part I)\nWhat We Learned from a Year of Building with LLMs (Part II)\n\n\n\n\n\nARCEP & ADEME de 2023\nAFNOR IA frugale\nGuide de recommandation de sécurité de l’ANSSI\nAI Act",
    "crumbs": [
      "Bibliographie"
    ]
  },
  {
    "objectID": "Bibliographie.html#i---accompagnement",
    "href": "Bibliographie.html#i---accompagnement",
    "title": "Bibliographie",
    "section": "",
    "text": "Qu’est ce que l’IA ?\nWhat We Learned from a Year of Building with LLMs (Part I)\nWhat We Learned from a Year of Building with LLMs (Part II)\n\n\n\n\n\nARCEP & ADEME de 2023\nAFNOR IA frugale\nGuide de recommandation de sécurité de l’ANSSI\nAI Act",
    "crumbs": [
      "Bibliographie"
    ]
  },
  {
    "objectID": "Bibliographie.html#ii---développement",
    "href": "Bibliographie.html#ii---développement",
    "title": "Bibliographie",
    "section": "II - Développement",
    "text": "II - Développement\n\nPlateforme de partage de modèles\n\nHuggingFace\n\n\n\nArticles de recherche centraux\nTransformers\n\nPapier original ‘Attention Is All You Need’\nExplication illustrée et très détaillée\n\nFine-tuning\n\nLoRA\nQLoRA\nDoRA\nDPO\nKTO\n\nBonnes pratiques du prompt engineering\n\nPrincipled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4\nGraph of Thoughts\n\n\n\nLibrairies et ressources\nPipelines et orchestration LLM - LangChain - LlamaIndex - Haystack\nRAG - [Graph RAG]https://microsoft.github.io/graphrag/\nEvaluation - SelfCheckGPT",
    "crumbs": [
      "Bibliographie"
    ]
  },
  {
    "objectID": "notebooks/autres/parse_llama31_results.html",
    "href": "notebooks/autres/parse_llama31_results.html",
    "title": "Guide d'installation des LLM",
    "section": "",
    "text": "from glob import glob\nimport pandas as pd\n\n\nTHEMATIQUES={\n    \"accord_methode_penibilite\":\"Accords de méthode (pénibilité)\",\n\"accord_methode_pse\":\"Accords de méthode (PSE)\",\n\"amenagement_temps_travail\":\"Aménagement du temps de travail (modulation, annualisation, cycles)\",\n\"autres\":\"Autre, précisez\",\n\"autres_condition_travail\":\"Autres dispositions de conditions de travail (CHSCT, médecine du travail, politique générale de prévention)\",\n\"autres_dispositions_duree\":\"Autres dispositions durée et aménagement du temps de travail \",\n\"autres_dispositions_egalite\":\"Autres dispositions Egalité professionnelle\",\n\"autres_dispositions_emploi\":\"Autres dispositions emploi\",\n\"calendrier_negociation\":\"Calendrier des négociations\",\n\"classifications\":\"Classifications\",\n\"commision_paritaire\":\"Commissions paritaires\",\n\"cet\":\"Compte épargne temps\",\n\"couverture_complementaire\":\"Couverture complémentaire santé - maladie\",\n\"don_jour\":\"Dispositifs don de jour et jour de solidarité\",\n\"distribution_actions_gratuites\":\"Distribution d'actions gratuites\",\n\"droit_deconnexion\":\"Droit à la déconnexion et outils numériques\",\n\"droit_syndical\":\"Droit syndical, IRP, expression des salariés\",\n\"duree_collective_temps_travail\":\"Durée collective du temps de travail\",\n\"egalite_salariale\":\"Egalité salariale F/H\",\n\"election_pro\":\"Elections professionnelles, prorogations de mandat et vote électronique\",\n\"evolution_prime\":\"Evolution des primes\",\n\"evolution_salariale\":\"Evolution des salaires (augmentation, gel, diminution)\",\n\"fin_conflit\":\"Fin de conflit\",\n\"conges\":\"Fixation des congés (jours fériés, ponts, RTT)\",\n\"forfait\":\"Forfaits (en heures, en jours)\",\n\"formation_pro\":\"Formation professionnelle\",\n\"gpec\":\"GPEC\",\n\"heures_supp\":\"Heures supplémentaires (contingent, majoration)\",\n\"indemnites\":\"Indemnités (dont kilométrique)\",\n\"interessement\":\"Intéressement\",\n\"mesure_age\":\"Mesures d'âge (seniors, contrat de génération...)\",\n\"mobilite\":\"Mobilité (géographique, professionnelle - promotions)\",\n\"diversite\":\"Non discrimination - Diversité\",\n\"participation\":\"Participation\",\n\"pee_peg\":\"PEE ou PEG\",\n\"pei\":\"PEI\",\n\"penibilite\":\"Pénibilité du travail (1% pénibilité, prévention, compensation/réparation)\",\n\"perco_percoi\":\"PERCO et PERCOI\",\n\"performance_collecte\":\"Performance collective (accord de compétitivité)\",\n\"prevoyance_collective\":\"Prévoyance collective, autre que santé maladie\",\n\"prime_partage_profit\":\"Prime de partage des profits\",\n\"qvt\":\"QVT, conciliation vie personnelle/vie professionnelle\",\n\"reprise_des_donnees\":\"Reprise des données\",\n\"retraite_complementaire\":\"Retraite complémentaire - supplémentaire\",\n\"rupture_conventionnelle_collective\":\"Rupture conventionnelle collective\",\n\"stress_rps\":\"Stress, risques psycho-sociaux\",\n\"supplement_participation\":\"Supplément de participation\",\n\"supplement_interessement\":\"Supplément d'intéressement\",\n\"systeme_prime\":\"Système de prime (autre qu'évolution)\",\n\"système_de_remuneration\":\"Système de rémunération (autres qu'évolution)\",\n\"teletravail\":\"Télétravail\",\n\"travail_temps_partiel\":\"Travail à temps partiel\",\n\"travail_nuit\":\"Travail de nuit\",\n\"travail_dimanche\":\"Travail du dimanche\",\n\"travailleurs_handicapes\":\"Travailleurs handicapés\"}\n\n\nlist_of_df=[]\nfor file in glob(\"results/*\"):\n    with open(file,\"r\") as f:\n        lines=f.readlines()\n    lines=[line for line in lines if \":\" in line and line.split(\" : \")[0] in set(THEMATIQUES.keys())]\n    d = dict()\n    for line in lines:\n        cle, valeur = line.split(\" : \")[0], line.split(\" : \")[1]\n        d[cle]=1 if valeur.lower().startswith(\"oui\") else 0\n    list_of_df.append(pd.DataFrame(d,index=[file.split(\"/\")[1].split(\".\")[0]]))\n\n\ndf_results=pd.concat(list_of_df)\n\n\ndf_sample=pd.read_parquet(\"./10p_accords_publics_et_thematiques_240815_sample_of_1000.parquet\")\n\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\nfor (k,v) in THEMATIQUES.items():\n    df=pd.DataFrame(df_sample[k].astype(int)).merge(df_results[k],how=\"inner\",left_index=True,right_index=True,suffixes=[\"_expected\",\"_predicted\"])\n    y_true, y_pred=df[f\"{k}_expected\"], df[f\"{k}_predicted\"]\n    cm = confusion_matrix(y_true, y_pred)\n    print(k)\n    print(cm)\n\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='macro')\n    recall = recall_score(y_true, y_pred, average='macro')\n    f1 = f1_score(y_true, y_pred, average='macro')\n    report = classification_report(y_true, y_pred)\n    \n    print(f'Accuracy: {accuracy}')\n    print(f'Precision (macro): {precision}')\n    print(f'Recall (macro): {recall}')\n    print(f'F1 Score (macro): {f1}')\n    print(\"-\"*10)\n    print('Classification Report:')\n    print(report)"
  },
  {
    "objectID": "II-Developpements/2_RAG.html#a.-principe-de-fonctionnement-général",
    "href": "II-Developpements/2_RAG.html#a.-principe-de-fonctionnement-général",
    "title": "PARTIE II. Développements autour des LLMs (pour les data scientists)",
    "section": "a. Principe de fonctionnement général",
    "text": "a. Principe de fonctionnement général\nUn système de RAG va combiner la capacité de génération de textes d’un LLM avec de la recherche d’information dans une base de connaissance interne. Le RAG se compose principalement de deux parties, une première partie de retrieval dont le rôle est d’analyser une question de l’utilisateur et de trouver des éléments qui sont pertinents pour répondre à la question dans la base, et une seconde partie de génération qui contient le LLM et qui va incorporer le contexte récupérés par la partie de retrieval dans un prompt pour permettre au LLM de générer une réponse basée sur des éléments pertinents.",
    "crumbs": [
      "II-Developpements",
      "Retrieval Augmented Generation"
    ]
  },
  {
    "objectID": "II-Developpements/2_RAG.html#b.-recherche-dinformation",
    "href": "II-Developpements/2_RAG.html#b.-recherche-dinformation",
    "title": "PARTIE II. Développements autour des LLMs (pour les data scientists)",
    "section": "b. Recherche d’information",
    "text": "b. Recherche d’information\nLa partie de retrieval qui s’occupe de la recherche d’informations pertinentes dans la base joue un rôle essentiel pour le bon fonctionnement du RAG. En effet si des informations non pertinentes sont retournées au LLM il devient très difficile pour lui de formuler une réponse adéquate.\nIl existe de nombreux algorithmes permettant d’effectuer cette recherche d’information, cependant l’une des approches les plus populaires se base sur l’utilisation de vecteurs denses. Dans cette méthode on utilise un encodeur pour transformer le texte en vecteurs de grandes dimensions que l’on stocke dans une base de données vectorielle. Cet encodeur a été entrainé a projeter des textes sémantiquement proches sur des vecteurs similaires. Lorsqu’une requête utilisateur est reçue en entrée, elle est elle aussi encodée dans un vecteur de même dimension. On peut alors comparer ce vecteur à l’ensemble des vecteurs présents en base ce qui permet de récupérer les vecteurs les plus proches qui correspondent à ceux qui sont sémantiquement proches. On peut ainsi trouver les morceaux de texte à envoyer au LLM pour la génération.",
    "crumbs": [
      "II-Developpements",
      "Retrieval Augmented Generation"
    ]
  },
  {
    "objectID": "II-Developpements/2_RAG.html#c.-génération",
    "href": "II-Developpements/2_RAG.html#c.-génération",
    "title": "PARTIE II. Développements autour des LLMs (pour les data scientists)",
    "section": "c. Génération",
    "text": "c. Génération\nLa phase de génération dans un système de RAG intervient après la récupération des documents pertinents. Une fois les documents récupérés, ils sont insérés dans un prompt que le LLM utilise pour produire des réponses contextuellement appropriées et précises. Plus le LLM est performant et correctement aligné sur les préférences humaines plus il est capable de prendre une quantité importante de document en contexte et ainsi de mieux répondre à la question.",
    "crumbs": [
      "II-Developpements",
      "Retrieval Augmented Generation"
    ]
  },
  {
    "objectID": "II-Developpements/2_RAG.html#d.-benchmark-des-différentes-bases-vectorielles-katia",
    "href": "II-Developpements/2_RAG.html#d.-benchmark-des-différentes-bases-vectorielles-katia",
    "title": "PARTIE II. Développements autour des LLMs (pour les data scientists)",
    "section": "d. Benchmark des différentes bases vectorielles (Katia)",
    "text": "d. Benchmark des différentes bases vectorielles (Katia)\n\nComparatif détaillé des solutions de stockages de données pour la recherche vectorielle approximative\nToutes les solutions testées sont open-source ou disposant d’une licence permissive, qui donne la possibilité d’héberger localement les données.\n\n\n\n\n\nWeaviate\n\n\nMilvus\n\n\nQdrant\n\n\nElasticSearch\n\n\nFAISS\n\n\n\n\nOpen source\n\n\n✅\n\n\n✅\n\n\n✅\n\n\nPartiellement\n\n\n✅\n\n\n\n\nDev-friendly\n\n\n+++\n\n\n+\n\n\n+++\n\n\n++\n\n\n+++\n\n\n\n\nDéploiement\n\n\n✅\n\n\n✅ mais difficile à mettre en place, constellation de micro-services\n\n\n✅\n\n\n✅\n\n\n❌ mais possibilité de construire une image Docker custom par exemple\n\n\n\n\nSpécifique à la recherche vectorielle\n\n\n✅\n\n\n✅\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n\n\nQualité de la documentation\n\n\n+++ [-]\n\n\n++ [-]\n\n\n+++ [-]\n\n\n++ [-]\n\n\n+ [-]\n\n\n\n\nDernière mise à jour\n\n\nmai 2024\n\n\nmai 2024\n\n\nmai 2024\n\n\nmai 2024\n\n\nmars 2024\n\n\n\n\nLatence (ms)**\n\n\n438.18\n\n\n322.63\n\n\n118.25\n\n\n338.53\n\n\n-\n\n\n\n\nRequêtes/seconde\n(RPS)**\n\n\n217.98\n\n\n281.52\n\n\n710.23\n\n\n275.11\n\n\n-\n\n\n\n\nP99 latence (ms)**\n\n\n1723.62\n\n\n436.87\n\n\n144.78\n\n\n589.61\n\n\n-\n\n\n\n\nTemps d’upload (minutes)**\n\n\n71.61\n\n\n1.41\n\n\n2.074\n\n\n14.33\n\n\n-\n\n\n\n\nTemps d’upload +\nindexation (minutes)**\n\n\n71.61\n\n\n9.53\n\n\n17.49\n\n\n122.79\n❌\n\n\n-\n\n\n\n\nPlace en mémoire\n\n\nnum_vectors * vector_dimension * 4 bytes * 2 [-]\n\n\nConséquente d’après les avis d’utilisateurs, pas de formules approximative [-]\n\n\nnum_vectors * vector_dimension * 4 bytes * 1.5 [-]\n\n\nnum_vectors * 4 * (vector_dimension + 12) [-]\n\n\n?\n\n\n\n\nType d’index\n\n\nHNSW\n\n\nFLAT, IVF_FLAT, IVF_SQ8, IVF_PQ, HNSW, BIN_FLAT, BIN_IVF_FLAT, DiskANN, GPU_IVF_FLAT, GPU_IVF_PQ, and CAGRA\n\n\nHNSW\n\n\nHNSW\n\n\nFLAT, IVS_FLAT, IVF_SQ8, IVF_PQ, HNSW, BIN_FLAT and BIN_IVF_FLAT\n\n\n\n\nRecherche hybride\n\n\n✅\n\n\n✅\n\n\n✅\n\n\n✅\n\n\n❌\n\n\n\n\nAjout d’éléments à la volée, scalabilité\n\n\nPartitionnement statique\n\n\nSegmentation\ndynamique\n\n\nPartitionnement statique\n\n\nPartitionnement statique\n\n\n❌ (index\nimmutable -&gt; vector library)\n\n\n\n\nAccès contrôlé par rôles\n\n\n❌ sur le backlog, mais n’avance beaucoup [-]\n\n\n✅ [-]\n\n\n✅ [-]\n\n\n✅ [-]\n\n\n❌\n\n\n\n\nPartions et étanchéité des bases de données\n(multi-tenancy)\n\n\n❌ pas très clair, mais il semble que ce ne soit pas encore possible [-]\n\n\n✅ Plusieurs systèmes de partitions, très flexible [-]\n\n\n✅ Plusieurs systèmes de partitions, assez flexible [-]\n\n\n✅ Possible mais pas très intuitif [-]\n\n\n❌\n\n\n\n\nAutres avantages\n\n\n\n\n\nTrès dynamique car chaque action a son propre node, facile à scaler\nPlusieurs niveaux de partitions\n\n\n\n\n\n\nStockage d’autres types de données, par exemple l’historique de conversations\nTrès commun comme solution de stockage, donc plus d’utilisateurs déjà familiers de l’outil\n\n\n\n\n\n\n\nAutres inconvénients\n\n\n\n\n\nTaille en mémoire (difficile à quantifier par rapport aux autres, mais plus importante selon les benchmarks)\n\n\n\n\nPas de stockage S3\n\n\n\n\n\n\nBibliothèque de vecteurs, pas vraiment adaptée à un usage persistant\n\n\n\n\n** Qdrant benchmark (janvier 2024), dataset = gist-960-euclidean (1M de vecteurs en dimension 960), précision à 0.95\nLes solutions présentées recouvrent en fait plusieurs cas d’usage :\n\nLes bibliothèques vectorielles (vector library) de type FAISS sont adaptées à de la rechercher sémantique à la volée, avec constitution de la base et recherche immédiate. Ici il s’agira d’un cas d’usage où l’utilisateur apporte son propre document avec un téléchargement en temps réel, et pose des questions dessus ou demande une synthèse.\nLes bases de données vectorielles (vector database) sont des dispositifs plus lourds et généralement un peu plus lents, mais avec un stockage permanent et beaucoup plus de flexibilité dans la recherche. Ils sont plus adaptés à un cas d’usage où la base de connaissance est constituée en amont et doit être mise à jour de temps en temps.\n\nPour une mise en production rapide et efficace Qdrant semble être la meilleure solution, combiné à une base de données plus traditionnelle comme ElasticSearch pour l’historique des conversations. Pour avoir une approche tout-en-en, et plus de flexibilité dans la gestion des collections, c’est ElasticSearch qui se détache des autres, malgré des temps d’indexation assez conséquents.\n\nAparté sur les intégrations Langchain : à manipuler avec précaution, les fonctions ne sont pas toujours explicites (par exemple la méthode from_documents supprime et recrée en général une collection). De plus certaines fonctionnalités comme l’utilisation de partitions ne sont pas toujours accessibles via Langchain. Il peut être utile de recréer des wrapper qui utilisent en partie Langchain et en partie les fonctions natives de la base de données.\n\n\nAnnexes\n\nDéfinitions\nDev-friendly -&gt; Note qualitative après installation de chaque solution (sauf FAISS) dans une image Docker, et utilisation avec Python (avec et sans l’intégration Langchain)\nDéploiement -&gt; Existence d’un écosystème de déploiement\nQualité de la documentation -&gt; Note qualitative après installation de chaque solution (sauf FAISS) dans une image Docker, et utilisation avec Python (avec et sans l’intégration Langchain)\nAjout d’éléments à la volée, scalabilité -&gt; Comment l’indexation se fait si la base de données est modifiée. Avec le partitionnement statique (static sharding), si la capacité du serveur est augmentée toutes les données doivent être de nouveau partitionnées, ce qui peut être long.\nRecherche hybride -&gt; Possibilité d’effectuer des recherches dans les métadonnées, avec des nombres ou des chaînes de caractères\nAccès contrôlé par rôles (RBAC)-&gt; Autorisations prédéfinies pour chaque utilisateur, avec un accès différencié aux documents\n\n\nRessources\nhttps://weaviate.io/blog/vector-library-vs-vector-database\nANN Benchmark (avril 2023)\n\n\n\n\nc. presentations de modules avec RETEX, CODE!)",
    "crumbs": [
      "II-Developpements",
      "Retrieval Augmented Generation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "Ce projet fait partie de la troisième saison du Programme 10%, co-porté par la DINUM en association étroite avec l’Insee.\n\n\nMistral, GPT4, Claude, ALBERT, CroissantLLM… Vous ne vous y retrouvez plus parmi tous ces modèles larges de language (LLM)? Ce guide permet de se poser les bonnes questions et de pouvoir répondre à un besoin d’analyse textuelle avec une solution adaptée. Ce guide est là pour vous aider du début jusqu’à la fin, de la conception du modèle après problématisation à la mise en production.\nObjectif : Comprendre son besoin et y répondre avec une solution LLM adaptée\n\n\n\nLes LLM sont des modèles de langage de grande taille, apparus vers 2018, ayant révolutionné le domaine du traitement automatique du langage naturel. Les LLMS se caractérisent par une architecture de type “transformers” permettant un traitement parallèle et contextuel du langage, un entraînement sur de très vastes corpus de données textuelles, allant de millions à des milliards de paramètres, et des capacités impressionnantes pour accomplir une grande variété de tâches linguistiques comme la génération de texte, la traduction, l’analyse de sentiment.\nPlus de 3000 LLM ont été entraînés et publiés sur des plateformes de partage de modèle comme HuggingFace, dont plus d’une cinquaine s’affronte régulièrement entre eux pour se démarquer sur différentes tâches dans une arène dédiée.\nIl est aujourd’hui difficile de savoir quel LLM est adapté pour son besoin spécifique et quelle infrastructure est nécessaire pour utiliser les différents LLM. Ce guide tente donc d’expliciter les raisonnements, les questions à se poser et des pistes de réponse quant aux choix et à la mise en oeuvre des LLM.\n\n\n\n\nl’infrastructure minimale pour faire tourner un LLM\nbenchmarking des “principaux” LLM\nexemple d’utilisation dans un cas simple (cas d’utilisation dit “fil rouge”) sous forme de tutoriel\nfinetuner un modèle LLM\nquantizer un modèle LLM\nEvaluer un LLM\nmettre en production un modèle LLM\nune bibliographie consise et non exhaustive\nd’autres exemples plus complexes de cas d’utilisation dans l’administration\nune approximation du coût environnemental et financier des différents LLM\n\n\n\n\n\nles fondements théoriques de l’optimisation\ncas spécifique d’une administration\ncomment débiaiser un LLM\n\n\n\n\n\n\n\n\n\n\n\nMembre\nAdministration\n\n\n\n\nConrad THIOUNN\nDares - Ministère du Travail\n\n\nJohnny PLATON\nSanté publique France\n\n\nThibault DUROUCHOUX\nDGDDI - Ministère de l’Economie et des Finances\n\n\nKatia JODOGNE-DEL LITTO\nIGF - Ministère de l’Economie et des Finances\n\n\nFaheem BEG\n\n\n\nCamille ANDRÉ\n\n\n\nCamille BRIER\nDTNum - DGFiP\n\n\nDaphné PERTSEKOS\nANFSI - Ministère de l’intérieur\n\n\nZhanna SANTYBAYEVA\nDGOS - Ministère du travail, de la santé et des solidarités\n\n\nBruno LENZI\nEcolab - Ministère de la Transiton Écologique et de la Cohésion des Territoires\n\n\nHélène CHARASSON-JASSON\nBanque de France\n\n\n\n\n\n\nUne remarque ? Une question ? Vous pouvez nous contacter sur le salon Tchap du Programme 10% ou …",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#préambule",
    "href": "index.html#préambule",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "Mistral, GPT4, Claude, ALBERT, CroissantLLM… Vous ne vous y retrouvez plus parmi tous ces modèles larges de language (LLM)? Ce guide permet de se poser les bonnes questions et de pouvoir répondre à un besoin d’analyse textuelle avec une solution adaptée. Ce guide est là pour vous aider du début jusqu’à la fin, de la conception du modèle après problématisation à la mise en production.\nObjectif : Comprendre son besoin et y répondre avec une solution LLM adaptée",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#etats-des-lieux-des-llm",
    "href": "index.html#etats-des-lieux-des-llm",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "Les LLM sont des modèles de langage de grande taille, apparus vers 2018, ayant révolutionné le domaine du traitement automatique du langage naturel. Les LLMS se caractérisent par une architecture de type “transformers” permettant un traitement parallèle et contextuel du langage, un entraînement sur de très vastes corpus de données textuelles, allant de millions à des milliards de paramètres, et des capacités impressionnantes pour accomplir une grande variété de tâches linguistiques comme la génération de texte, la traduction, l’analyse de sentiment.\nPlus de 3000 LLM ont été entraînés et publiés sur des plateformes de partage de modèle comme HuggingFace, dont plus d’une cinquaine s’affronte régulièrement entre eux pour se démarquer sur différentes tâches dans une arène dédiée.\nIl est aujourd’hui difficile de savoir quel LLM est adapté pour son besoin spécifique et quelle infrastructure est nécessaire pour utiliser les différents LLM. Ce guide tente donc d’expliciter les raisonnements, les questions à se poser et des pistes de réponse quant aux choix et à la mise en oeuvre des LLM.",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#ce-que-couvre-ce-guide",
    "href": "index.html#ce-que-couvre-ce-guide",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "l’infrastructure minimale pour faire tourner un LLM\nbenchmarking des “principaux” LLM\nexemple d’utilisation dans un cas simple (cas d’utilisation dit “fil rouge”) sous forme de tutoriel\nfinetuner un modèle LLM\nquantizer un modèle LLM\nEvaluer un LLM\nmettre en production un modèle LLM\nune bibliographie consise et non exhaustive\nd’autres exemples plus complexes de cas d’utilisation dans l’administration\nune approximation du coût environnemental et financier des différents LLM",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#ce-que-ne-couvre-pas-encore-ce-guide",
    "href": "index.html#ce-que-ne-couvre-pas-encore-ce-guide",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "les fondements théoriques de l’optimisation\ncas spécifique d’une administration\ncomment débiaiser un LLM",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#membres",
    "href": "index.html#membres",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "Membre\nAdministration\n\n\n\n\nConrad THIOUNN\nDares - Ministère du Travail\n\n\nJohnny PLATON\nSanté publique France\n\n\nThibault DUROUCHOUX\nDGDDI - Ministère de l’Economie et des Finances\n\n\nKatia JODOGNE-DEL LITTO\nIGF - Ministère de l’Economie et des Finances\n\n\nFaheem BEG\n\n\n\nCamille ANDRÉ\n\n\n\nCamille BRIER\nDTNum - DGFiP\n\n\nDaphné PERTSEKOS\nANFSI - Ministère de l’intérieur\n\n\nZhanna SANTYBAYEVA\nDGOS - Ministère du travail, de la santé et des solidarités\n\n\nBruno LENZI\nEcolab - Ministère de la Transiton Écologique et de la Cohésion des Territoires\n\n\nHélène CHARASSON-JASSON\nBanque de France",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#épilogue",
    "href": "index.html#épilogue",
    "title": "Projet KALLM : Kit avancé LLM",
    "section": "",
    "text": "Une remarque ? Une question ? Vous pouvez nous contacter sur le salon Tchap du Programme 10% ou …",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "I-Accompagnement/1_Besoins.html",
    "href": "I-Accompagnement/1_Besoins.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Vision high level de l'intérêt des LLMs\nLes cas d’usages des LLMs sont variés et avant de se lancer et innover grâce aux LLMs, il est nécessaire de bien identifier le besoin qui amène l’utilisation d’un LLM. Pour quoi faire ? Pour quels usages ? Est-ce pour de la génération de texte ? Pour de la classification ? L’objectif de ce chapitre est d’accompagner la réflexion autour de l’identification du besoin et de la collecte des données, avec les différents types de cas d’usages impliquant des LLMs.\nLes cas d’usages :\n\ncas d’usages autour de la génération de contenu\ncas d’usage autour de la classification et de la recherche de contenu\ncas d’usage autour des interactions conversationnelles\n\n\n\n\n\nUtilisation des SLM pour la recherche thématique simple en français (en cours, Zhanna) Malgré la disponibilité et l’attractivité des « grands » modèles langages comme GPT et Mixtral, l’utilisation des petits modèles classiques est parfois plus avantageuse, surtout quand les ressources techniques ou l’accès aux données sont restreints.\nC’est vrai dans le cas d’utilisation d’un SLM basé sur un modèle devenu classique, BERT qui donne la naissance à milliers de modèles spécialisés comme CamemBERT un modèle en français ou encore sBERT ou sentenceTransformers permettant un entraînement spécialisé pour une recherche sémantique.  **ici plus d’information sur les avantages des SLM (données, environement, spécialisation, travail en local, technique)  Nous considérons un exemple d’utilisation de CamemBERT-base et un exemple de sBERT :\ncamembert-bio-base avec ses 111M de paramètres, pour une recherche thématique dans des textes scientifiques biomédicaux. Nous utiliserons les transformers de HuggingFace\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nbiotokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-bio-base\")\nbiomodel = AutoModelForMaskedLM.from_pretrained(\"almanach/camembert-bio-base\")\n\nall-MiniLM-L6-v2\n\nimport requests\n\napi_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\nheaders = {\"Authorization\": f\"Bearer {hf_token}\"}\n\nClassifier des accords d’entreprise\n\n\n\nLes accords d’entreprise sont publiés sur LégiFrance. Ces accords peuvent concerner plusieurs thématiques (télétravail, compte épargne temps, droit à la deconnexion). Ces thématiques sont déclarés par les entreprises et sont éventuellement corrigées par la Direction Générale du Travail. Le besoin est alors de détecter automatiquement les thématiques à la lecture de l’accord. Un jeu de données est disponible à l’adresse suivante : accords_publics_xx_to_2022_themes_et_texte.parquet",
    "crumbs": [
      "I-Accompagnement",
      "Besoins"
    ]
  },
  {
    "objectID": "I-Accompagnement/1_Besoins.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/1_Besoins.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "Vision high level de l'intérêt des LLMs\nLes cas d’usages des LLMs sont variés et avant de se lancer et innover grâce aux LLMs, il est nécessaire de bien identifier le besoin qui amène l’utilisation d’un LLM. Pour quoi faire ? Pour quels usages ? Est-ce pour de la génération de texte ? Pour de la classification ? L’objectif de ce chapitre est d’accompagner la réflexion autour de l’identification du besoin et de la collecte des données, avec les différents types de cas d’usages impliquant des LLMs.\nLes cas d’usages :\n\ncas d’usages autour de la génération de contenu\ncas d’usage autour de la classification et de la recherche de contenu\ncas d’usage autour des interactions conversationnelles\n\n\n\n\n\nUtilisation des SLM pour la recherche thématique simple en français (en cours, Zhanna) Malgré la disponibilité et l’attractivité des « grands » modèles langages comme GPT et Mixtral, l’utilisation des petits modèles classiques est parfois plus avantageuse, surtout quand les ressources techniques ou l’accès aux données sont restreints.\nC’est vrai dans le cas d’utilisation d’un SLM basé sur un modèle devenu classique, BERT qui donne la naissance à milliers de modèles spécialisés comme CamemBERT un modèle en français ou encore sBERT ou sentenceTransformers permettant un entraînement spécialisé pour une recherche sémantique.  **ici plus d’information sur les avantages des SLM (données, environement, spécialisation, travail en local, technique)  Nous considérons un exemple d’utilisation de CamemBERT-base et un exemple de sBERT :\ncamembert-bio-base avec ses 111M de paramètres, pour une recherche thématique dans des textes scientifiques biomédicaux. Nous utiliserons les transformers de HuggingFace\n\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nbiotokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-bio-base\")\nbiomodel = AutoModelForMaskedLM.from_pretrained(\"almanach/camembert-bio-base\")\n\nall-MiniLM-L6-v2\n\nimport requests\n\napi_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\nheaders = {\"Authorization\": f\"Bearer {hf_token}\"}\n\nClassifier des accords d’entreprise\n\n\n\nLes accords d’entreprise sont publiés sur LégiFrance. Ces accords peuvent concerner plusieurs thématiques (télétravail, compte épargne temps, droit à la deconnexion). Ces thématiques sont déclarés par les entreprises et sont éventuellement corrigées par la Direction Générale du Travail. Le besoin est alors de détecter automatiquement les thématiques à la lecture de l’accord. Un jeu de données est disponible à l’adresse suivante : accords_publics_xx_to_2022_themes_et_texte.parquet",
    "crumbs": [
      "I-Accompagnement",
      "Besoins"
    ]
  },
  {
    "objectID": "I-Accompagnement/3_Acculturation.html",
    "href": "I-Accompagnement/3_Acculturation.html",
    "title": "Guide du LLM",
    "section": "",
    "text": "Comment embarquer les métiers/personnels moins techniques\nPoints d'attention à partager sur l'utilisation de tels outils",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  },
  {
    "objectID": "I-Accompagnement/3_Acculturation.html#partie-i.-accompagnement-au-changement",
    "href": "I-Accompagnement/3_Acculturation.html#partie-i.-accompagnement-au-changement",
    "title": "Guide du LLM",
    "section": "",
    "text": "Comment embarquer les métiers/personnels moins techniques\nPoints d'attention à partager sur l'utilisation de tels outils",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  }
]