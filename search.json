[
  {
    "objectID": "III-Deploiements/4_Deploiement_applications.html",
    "href": "III-Deploiements/4_Deploiement_applications.html",
    "title": "D√©ploiement d‚Äôapplications",
    "section": "",
    "text": "Plusieurs initiatives permettent de d√©ployer rapidement des interfaces de chat avec des mod√®les LLMs, voire des applications de RAG avec back et front. On peut remarquer :\n\nCARADOC\nWebUI du module FastChat\nopenwebui\n\n\n\nMise en open source par l‚Äô√©quipe DataScience de la DTNUM de la DGFiP : git\nAper√ßu de l‚Äôapplication CARADOC pendant ses d√©veloppements :\n\n\n\nInterface de l‚Äôapplication RAG Caradoc\n\n\n\n\n\nAper√ßu d‚Äôune interface possible avec FastChat (bas√©e sur Gradio):\n\n\n\nInterface de l‚Äôapplication Chat avec FastChat\n\n\nExemple de code pour lancer l‚Äôinterface Gradio de FastChat dans un Docker :\nversion: \"3.9\"\nservices:\n  fastchat-gradio-server:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      FASTCHAT_CONTROLLER_URL: http://0.0.0.0:21001\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    image: fastchat:latest\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./FastChat:/FastChat\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.gradio_web_server_dtnum\", \"--controller-url\", \"http://0.0.0.0:21001\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--model-list-mode\", \"reload\"]\nAvec toujours l‚Äôimage Docker qui contient FastChat.\n\n\n\nOpen webui est une librairie opensource permettant le d√©ploiement d‚Äôune interface similaire √† ChatGPT pour int√©ragir avec des API ou son propre service LLM.\n\nAu del√† du tchat, l‚Äôapplication donne acc√®s √† plusieurs fonctionnalit√©s avanc√©es (RAG, ex√©cution de fonction, ‚Ä¶). Elle est d√©ployable via Docker ou installation python.",
    "crumbs": [
      "III-Deploiements",
      ":computer: D√©ploiement d'applications"
    ]
  },
  {
    "objectID": "III-Deploiements/4_Deploiement_applications.html#des-interfaces-d√©j√†-disponibles-pour-vos-mod√®les-llms",
    "href": "III-Deploiements/4_Deploiement_applications.html#des-interfaces-d√©j√†-disponibles-pour-vos-mod√®les-llms",
    "title": "D√©ploiement d‚Äôapplications",
    "section": "",
    "text": "Plusieurs initiatives permettent de d√©ployer rapidement des interfaces de chat avec des mod√®les LLMs, voire des applications de RAG avec back et front. On peut remarquer :\n\nCARADOC\nWebUI du module FastChat\nopenwebui\n\n\n\nMise en open source par l‚Äô√©quipe DataScience de la DTNUM de la DGFiP : git\nAper√ßu de l‚Äôapplication CARADOC pendant ses d√©veloppements :\n\n\n\nInterface de l‚Äôapplication RAG Caradoc\n\n\n\n\n\nAper√ßu d‚Äôune interface possible avec FastChat (bas√©e sur Gradio):\n\n\n\nInterface de l‚Äôapplication Chat avec FastChat\n\n\nExemple de code pour lancer l‚Äôinterface Gradio de FastChat dans un Docker :\nversion: \"3.9\"\nservices:\n  fastchat-gradio-server:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      FASTCHAT_CONTROLLER_URL: http://0.0.0.0:21001\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    image: fastchat:latest\n    ports:\n      - \"8001:8001\"\n    volumes:\n      - ./FastChat:/FastChat\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.gradio_web_server_dtnum\", \"--controller-url\", \"http://0.0.0.0:21001\", \"--host\", \"0.0.0.0\", \"--port\", \"8001\", \"--model-list-mode\", \"reload\"]\nAvec toujours l‚Äôimage Docker qui contient FastChat.\n\n\n\nOpen webui est une librairie opensource permettant le d√©ploiement d‚Äôune interface similaire √† ChatGPT pour int√©ragir avec des API ou son propre service LLM.\n\nAu del√† du tchat, l‚Äôapplication donne acc√®s √† plusieurs fonctionnalit√©s avanc√©es (RAG, ex√©cution de fonction, ‚Ä¶). Elle est d√©ployable via Docker ou installation python.",
    "crumbs": [
      "III-Deploiements",
      ":computer: D√©ploiement d'applications"
    ]
  },
  {
    "objectID": "III-Deploiements/4_Deploiement_applications.html#d√©ploiement-sur-le-ssp-cloud",
    "href": "III-Deploiements/4_Deploiement_applications.html#d√©ploiement-sur-le-ssp-cloud",
    "title": "D√©ploiement d‚Äôapplications",
    "section": "D√©ploiement sur le SSP Cloud",
    "text": "D√©ploiement sur le SSP Cloud\nSur le DataLab SSP Cloud, il est possible de d√©ployer des LLM √† des fins d‚Äôexp√©rimentation. Plusieurs cas sont possibles :\n\nUtiliser des librairies d‚ÄôAPI de LLM (vLLM, etc.)\nD√©ployer des containers Docker avec Kube et Helm\n\n\nD√©ploiement par image Docker\n\nCr√©er une image Docker et la mettre √† disposition (Dockerhub) : exemple applicatif avec Streamlit\nD√©ployer avec Kube et Helm en utilisant un service VSCode avec les droits d‚Äôadmin pour Kube\n\nExemple avec Kubernetes :\nkubectl create deployment mon-deploiement --image=mon-image-docker\nkubectl proxy\n\n\nD√©ploiement par Chart Helm\nLe projet Caradoc mentionn√© pr√©c√©demment peut √©galement √™tre d√©ploy√© sur le SSP Cloud via un Chart Helm. Le proc√©dure d‚Äôinstallation est disponible ici.\n\nPour plus d‚Äôinformations sur le d√©ploiement de chart Helm sur le SSP Cloud, il existe un tutoriel qui d√©taille l‚Äôinstallation d‚Äôune application Shiny. Un exemple de d√©ploiement d‚Äôun chart Helm pour l‚Äôinstantiation d‚Äôune infrastrucuture LLM est aussi disponible au sein du guide.",
    "crumbs": [
      "III-Deploiements",
      ":computer: D√©ploiement d'applications"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Service_LLM_avance.html",
    "href": "III-Deploiements/2_Service_LLM_avance.html",
    "title": "Service LLM avanc√©",
    "section": "",
    "text": "Pour d√©ployer un grand mod√®le de langage (LLM) dans une infrastructure, il est essentiel de comprendre comment requ√™ter le mod√®le, les quelques couches techniques imm√©diates qui l‚Äôentourent et les solutions disponibles pour un d√©ploiement efficace.",
    "crumbs": [
      "III-Deploiements",
      ":wrench: Service LLM avanc√©"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Service_LLM_avance.html#ce-dont-vous-avez-besoin-pour-mettre-√†-disposition-un-llm",
    "href": "III-Deploiements/2_Service_LLM_avance.html#ce-dont-vous-avez-besoin-pour-mettre-√†-disposition-un-llm",
    "title": "Service LLM avanc√©",
    "section": "Ce dont vous avez besoin pour mettre √† disposition un LLM",
    "text": "Ce dont vous avez besoin pour mettre √† disposition un LLM\nLorsqu‚Äôil s‚Äôagit de mettre en service des applications bas√©es sur des LLM, il y a 2 composants principaux : le moteur et le serveur. Le moteur g√®re tout ce qui concerne les mod√®les et le regroupement des demandes, tandis que le serveur g√®re l‚Äôacheminement des demandes des utilisateurs.\n\nMoteurs\nLes moteurs sont les composants ex√©cutant les mod√®les et tout ce que nous avons couvert jusqu‚Äô√† pr√©sent sur le processus de g√©n√©ration avec diff√©rents types d‚Äôoptimisations. √Ä leur c≈ìur, ce sont des biblioth√®ques Python. Ils g√®rent le regroupement des demandes qui proviennent des utilisateurs vers notre chatbot et g√©n√®rent la r√©ponse √† ces demandes.\n\n\nServeurs\nLes serveurs sont responsables de l‚Äôorchestration des requ√™tes HTTP/gRPC entrantes des utilisateurs. Dans les applications du monde r√©el, nous aurons de nombreux utilisateurs qui posent des questions √† notre chatbot √† diff√©rents moments de la journ√©e. Les serveurs mettent ces demandes en file d‚Äôattente et les transf√®rent vers le moteur pour la g√©n√©ration de la r√©ponse. Les serveurs apportent √©galement les m√©triques telles que le d√©bit et la latence, qui sont importantes √† suivre pour le service de mod√®le.\n\n\nR√©sum√©\n\nMoteurs\n\nOptimisation de la m√©moire\nOptimisation sp√©cifique au mod√®le\nPrise en charge du regroupement\n\nServeurs\n\nAPI HTTP/gRPC\nMise en file d‚Äôattente des demandes\nMise en service de plusieurs mod√®les\nPrise en charge de plusieurs moteurs",
    "crumbs": [
      "III-Deploiements",
      ":wrench: Service LLM avanc√©"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Service_LLM_avance.html#exemples-doutils-de-mise-√†-disposition-de-llm",
    "href": "III-Deploiements/2_Service_LLM_avance.html#exemples-doutils-de-mise-√†-disposition-de-llm",
    "title": "Service LLM avanc√©",
    "section": "Exemples d‚Äôoutils de mise √† disposition de LLM",
    "text": "Exemples d‚Äôoutils de mise √† disposition de LLM\nQuels outils sont les mieux adapt√©s √† nos besoins ? Comment choisir ? Voici un survol rapide de grands noms du milieu pour r√©f√©rences.\n\nUne recommandation de framework rapide √† prendre en main et dont l‚Äôutilit√© a d√©j√† √©t√© prouv√©e dans une de nos administrations se trouve √† la fin et est d√©velopp√©e dans le prochain paragraphe.\n\n\nMoteurs\n\nTensorRT-LLM est une biblioth√®que open-source qui optimise les performances d‚Äôinf√©rence des grands mod√®les de langage (LLM) en utilisant les GPU NVIDIA Tensor Core. Elle utilise le parall√©lisme tensoriel, propose une API Python simple et comprend des versions optimis√©es de LLM populaires. Elle prend en charge le batching en vol et vise √† simplifier la construction et l‚Äôexp√©rimentation de nouveaux LLM. Cependant, les utilisateurs doivent sp√©cifier la longueur d‚Äôentr√©e/sortie maximale et la taille de lot avant de construire le moteur, et la gestion de la m√©moire du cache KV n‚Äôest pas open source.\nvLLM est une biblioth√®que √† hautes performances pour l‚Äôinf√©rence et le service LLM, ax√©e sur le d√©bit de service et l‚Äôefficacit√© m√©moire gr√¢ce √† son m√©canisme PagedAttention. Il prend en charge le batching continu, le parall√©lisme GPU et la sortie en streaming, ainsi que la compatibilit√© OpenAI. Cependant, la m√©moire peut devenir un goulot d‚Äô√©tranglement avec des taux de demande √©lev√©s et de grandes tailles de lot.\n\n\n\nServeurs\n\nRayLLM avec RayServe est construit sur un framework de calcul distribu√© qui simplifie le d√©veloppement et le d√©ploiement de mod√®les d‚ÄôIA √† grande √©chelle. Il prend en charge les points de terminaison multi-mod√®les, les fonctionnalit√©s serveur et les optimisations via les int√©grations avec vLLM et TGI.\nTriton avec TensorRT-LLM fournit un logiciel d‚Äôinf√©rence de serveur pour le d√©ploiement et l‚Äôex√©cution efficaces de LLM avec des techniques telles que le batching en vol et le cache KV pagin√©.\n\n\n\nMoteurs et serveurs\n\nG√©n√©ration de texte Inf√©rence (TGI) est un serveur Rust, Python et gRPC utilis√© chez HuggingFace pour HuggingChat, l‚ÄôAPI d‚Äôinf√©rence et le point de terminaison d‚Äôinf√©rence. Il prend en charge le batching continu, le parall√©lisme tensoriel, la quantification, les m√©canismes d‚Äôattention, le recuit simul√© des logits et des LLM sp√©cifiques. Cependant, la licence d‚Äôutilisation a √©t√© modifi√©e et n‚Äôest pas gratuite pour une utilisation commerciale.\nEnfin, Fastchat est une solution auto-h√©berg√©e pour h√©berger des mod√®les d‚ÄôIA g√©n√©ratifs et qui propose la gestion des mod√®les, des API OpenAI-compatibles et une web interface simple.\n\n\nNous allons d√©velopper FastChat dans la partie suivante car c‚Äôest un outil qui a √©t√© test√© et qui semble fournir beaucoup des √©l√©ments n√©cessaires pour une utilisation de premi√®re intention.",
    "crumbs": [
      "III-Deploiements",
      ":wrench: Service LLM avanc√©"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Service_LLM_avance.html#le-choix-dune-solution-technique-le-cas-dune-administration",
    "href": "III-Deploiements/2_Service_LLM_avance.html#le-choix-dune-solution-technique-le-cas-dune-administration",
    "title": "Service LLM avanc√©",
    "section": "Le choix d‚Äôune solution technique : le cas d‚Äôune administration",
    "text": "Le choix d‚Äôune solution technique : le cas d‚Äôune administration\n\nPremier cas : Les traitements par batch\nPour certains cas d‚Äôusage, l‚Äôenjeu est de traiter de nombreuses donn√©es avec le m√™me mode op√©ratoire en un coup de mani√®re ponctuelle. C‚Äôest ce qu‚Äôon appellera le traitement par batch. Cela consiste √† charger un mod√®le, le requ√™ter sur un tableau de prompt et obtenir la sortie pour pouvoir l‚Äôexporter. On peut le faire avec vLLM par exemple avec un morceau de code de ce type :\nfrom vllm import LLM, SamplingParams\nimport re\nimport pandas as pd\nimport re\nimport json\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\n\n\nlist_data = json.load(open(\"Data.json\"))\n\nlist_prompts = [ v for x,v in list_data.items()]\nlist_ids = [ x for x,v in list_data.items() ]\n\nsampling_params = SamplingParams(temperature=0.1, top_p=0.1, max_tokens=4096)\nllm = LLM(model=\"/data/models/hub/models--upstage--Llama-2-70b-instruct-v2/snapshots/36b2a974642846b40fbbafaabad936cd6f8a7632\", tensor_parallel_size=2)\nprint(\"STARTING INFERENCE\")\noutputs = llm.generate(list_prompts, sampling_params)\n\nresume = { idx:output.outputs[0].text for idx, output in zip(list_ids, outputs) }\n\njson.dump(resume, open(\"Sortie.json\", \"w\"))\nMais cette m√©thodologie a des limites, car cela n√©cessite de bloquer des gpus, ce qui entra√Æne des probl√©matiques de gestion et de partage.\n\n\nDeuxi√®me cas : Beaucoup d‚Äôutilisateurs et/ou d‚Äôapplications diff√©rents\nQue ce soit une √©quipe de plusieurs data-scientists, ou un ensemble d‚Äôapplication, si les besoins sont importants, les GPUs ont tout int√©r√™t √† √™tre partag√©s. Il ne sera donc pas possible que chaque script python charge son mod√®le en m√©moire et bloque des GPUs. Il est √©galement plus rassurant de s√©parer l‚Äôinfrastructure GPU des utilisateurs pour que chacun travaille dans son environnement, afin d‚Äô√©viter les casses accidentelles.\nLa solution qui consiste √† mettre √† disposition des APIs vient r√©pondre √† ces probl√©matiques. Les mod√®les sont cach√©s derri√®re les API, les datascientist et les applications peuvent venir les requ√™ter et n‚Äôont pas besoin de s‚Äôoccuper de l‚Äôinfrastructure. Ainsi, plut√¥t que chaque datascientist d√©ploie un m√™me mod√®le avec r√©servation de GPU, l‚Äôarchitecture en API permet la mise en commun du d√©ploiement au m√™me besoin.\n\nDans ce guide, FastChat est pr√©sent√© comme exemple pour la simplicit√© mais d‚Äôautres solutions existent, avec chacunes leurs avantages et inconv√©nients.",
    "crumbs": [
      "III-Deploiements",
      ":wrench: Service LLM avanc√©"
    ]
  },
  {
    "objectID": "III-Deploiements/2_Service_LLM_avance.html#fastchat",
    "href": "III-Deploiements/2_Service_LLM_avance.html#fastchat",
    "title": "Service LLM avanc√©",
    "section": "FastChat",
    "text": "FastChat\nFastChat propose des API OpenAI-compatibles pour ses mod√®les pris en charge, de sorte que vous puissiez utiliser FastChat comme une alternative locale aux API OpenAI. Cela permet d‚Äôutiliser la biblioth√®que openai-python et les commandes cURL, ce qui facilite le travail des datascientists.\n\nLa documentation compl√®te est disponible sur le repo du module\n\nNous allons tout de m√™me parcourir les grandes √©tapes pour pouvoir lancer son installation et ensuite l‚Äôutiliser.\n\nRESTful API Server\nTout repose sur la compl√©mentarit√© de trois services : le controller, les mod√®les et l‚ÄôAPI. Il faut commencer par lancer le controller.\npython3 -m fastchat.serve.controller\nEnsuite, les model_workers. (Un mod√®le vicuna est pris pour l‚Äôexemple.)\npython3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.5\nEt enfin, l‚ÄôAPI.\npython3 -m fastchat.serve.openai_api_server --host localhost --port 8000\n\n\nUtilisation avec l‚ÄôOpenAI Official SDK\nLe but de openai_api_server.py est d‚Äôimpl√©menter un serveur d‚ÄôAPI enti√®rement compatible avec OpenAI, de sorte que les mod√®les puissent √™tre utilis√©s directement avec la biblioth√®que openai-python.\nTout d‚Äôabord, installez le package Python OpenAI &gt;= 1.0 :\npip install --upgrade openai\nEnsuite, interagissez avec le mod√®le Vicuna :\nimport openai\n\nopenai.api_key = \"EMPTY\"\nopenai.base_url = \"http://localhost:8000/v1/\"\n\nmodel = \"vicuna-7b-v1.5\"\nprompt = \"Il √©tait une fois\"\n\n# cr√©er une compl√©tion\ncompletion = openai.completions.create(model=model, prompt=prompt, max_tokens=64)\n# imprimer la compl√©tion\nprint(prompt + completion.choices[0].text)\n\n# cr√©er une compl√©tion de chat\ncompletion = openai.chat.completions.create(\n  model=model,\n  messages=[{\"role\": \"user\", \"content\": \"Bonjour ! Quel est votre nom ?\"}]\n)\n# imprimer la compl√©tion\nprint(completion.choices[0].message.content)\n\n\nUtilisation avec curl\ncurl est un autre bon outil pour observer la sortie de l‚ÄôAPI.\nList Models:\ncurl http://localhost:8000/v1/models\nChat Completions:\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello! What is your name?\"}]\n  }'\nText Completions:\ncurl http://localhost:8000/v1/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"prompt\": \"Once upon a time\",\n    \"max_tokens\": 41,\n    \"temperature\": 0.5\n  }'\nEmbeddings:\ncurl http://localhost:8000/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"vicuna-7b-v1.5\",\n    \"input\": \"Hello world!\"\n  }'\n\n\nCombiner avec vLLM\nVous pouvez utiliser vLLM comme une mise en ≈ìuvre optimis√©e d‚Äôun travailleur dans FastChat. Il offre une mise en batch continue avanc√©e et un d√©bit beaucoup plus √©lev√© (~10x). Consultez la liste des mod√®les pris en charge ici : https://docs.vllm.ai/en/latest/models/supported_models.html\nIl suffit de remplacer le model_worker par le vllm_worker\npython3 -m fastchat.serve.vllm_worker --model-path lmsys/vicuna-7b-v1.5\n\n\nCombiner avec Docker\nPour permettre le lancement et l‚Äôarr√™t de mod√®les, et pour √©viter qu‚Äôune erreur dans un des mod√®les ne d√©regle l‚Äôensemble du syst√®me, une bonne pratique est souvent de conteneuriser les diff√©rentes parties. Cela necessite la pr√©paration de quelques fichiers et quelques tests, mais ensuite, cela assure la reproductibilit√© de votre infrastructure. Une fois que les images sont pr√©par√©es, on peut les arr√™ter, les relancer et les reproduire autant de fois que n√©cessaire.\nUne fa√ßon d‚Äôimpl√©menter vos services avec FastChat est de faire :\n\nUn conteneur pour le controller\nUn conteneur pour l‚ÄôAPI OpenAI like\nUn conteneur par mod√®le\n\nLes conteneurs pourront tous avoir la m√™me image de base o√π l‚Äôon a install√© les packages necessaires, comme vllm et notamment FastChat, que l‚Äôon a t√©l√©charg√© et copi√© dans notre arbre local :\nDockerfile :\nFROM nvidia/cuda:12.2.0-devel-ubuntu20.04\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 python3.9-distutils curl\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copiez le r√©pertoire FastChat dans le conteneur Docker\nCOPY ./FastChat_k /FastChat\n# COPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1\n# Allez dans le r√©pertoire FastChat et installez √† partir de ce r√©pertoire\nWORKDIR /FastChat\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.4.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n# nvidia/cuda:12.2.0-runtime-ubuntu20.04 docker pull nvidia/cuda:12.2.0-devel-ubuntu20.04\nEnsuite, il faut lancer les conteneurs docker avec la bonne commande pour que chaque docker remplisse bien sa fonction. Cela se g√®re avec des docker_compose.yml\nLe fichier de d√©ploiement des deux conteneurs obligatoires ressemblera √† cela :\nversion: \"3.9\"\nservices:\n  fastchat-controller:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"21001:21001\"\n    volumes:\n      - ./FastChat:/FastChat\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n  fastchat-openai:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    image: llm-api-light:1.0.0\n    network_mode: \"host\"\n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n    ports:\n      - \"8000:8000\"\n    volumes:\n      - ./FastChat:/FastChat\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1,key2,key3\", \"--controller-address\", \"http://0.0.0.0:21001\"]\nEt le fichier de d√©ploiement d‚Äôun mod√®le pourrait ressembler √† ceci :\nversion: \"3.9\"\nservices:\n  fastchat-model-mixtral-latest:\n    network_mode: \"host\"\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - /data/models:/data/models\n      - ./FastChat:/FastChat\n    environment:\n      no_proxy: localhost,127.0.0.1,0.0.0.0\n      TRANSFORMERS_OFFLINE: 1\n    image: fastchat:cudadevel-latest\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              device_ids: [\"1\", \"5\"]\n              capabilities: [gpu]\n    entrypoint: [\"python3.9\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\",\n    \"--worker-address\", \"http://0.0.0.0:26003\", \"--host\", \"0.0.0.0\", \"--port\", \"26003\", \"--controller\", \"http://0.0.0.0:21001\",\n    \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"] #  \"--quantization\", \"awq\" \"--num-gpus\", \"2\"\nPour finir, il suffit de lancer les commandes associ√©es √† chaque docker_compose pour lancer tous les services. Par exemple,\n# Define your Docker Compose files\ncompose_openai_service=\"docker-compose_openai.yml\"\ncompose_mixtral=\"docker-compose_mistral.yml\"\n\n# Execute Docker Compose commands\necho \"Executing Docker Compose for $compose_openai_service\"\ndocker compose -f $compose_openai_service up -d\n\necho \"Executing Docker Compose for $compose_mixtral\"\ndocker compose -f $compose_mixtral up -d\nA ce stade, vous avez d√©j√† une installation utilisable par plusieurs personnes (√† condition que l‚Äôurl soit accessible). Voici des exemples de code de cellules notebooks.\nimport openai\nimport requests\nimport json\n# to get proper authentication, make sure to use a valid key that's listed in\n# the --api-keys flag. if no flag value is provided, the `api_key` will be ignored.\nopenai.api_key = \"key1\" # 1rentrez l'api key\nopenai.api_base = \"host\" # mettre l'url du serveur\n#eventuellement r√©gler des probl√®mes de proxy\nmodels = openai.Model.list()\nfor d in models[\"data\"]:\n    print(d[\"id\"])\n# Instruct mode\nprompt = \"\"\"Bonjour toi. Donne moi un pays qui commence par F.\n\"\"\"\n\ncompletion = openai.Completion.create(\n    model=\"mixtral?\",\n    prompt=prompt,\n    max_tokens=25,\n    temperature=0.5,\n    top_p=1\n)\n# print the completion\nprint(completion.choices[0].text)",
    "crumbs": [
      "III-Deploiements",
      ":wrench: Service LLM avanc√©"
    ]
  },
  {
    "objectID": "III-Deploiements/0_Introduction_deploiements.html",
    "href": "III-Deploiements/0_Introduction_deploiements.html",
    "title": "PARTIE III. Deploiements",
    "section": "",
    "text": "Le d√©ploiement d‚Äôune application LLM peut prendre plusieurs formes en fonction de ses objectifs et contraintes. Comme tout projet informatique, la complexit√© du d√©ploiement d√©pendra :\n\nde la complexit√© des t√¢ches effectu√©es et des int√©ractions pr√©vues\ndu nombre d‚Äôutilisateurs total et concurrent\ndu temps de r√©ponse et de la qualit√© de service attendue\n‚Ä¶\n\nL‚Äôinfrastructure et l‚Äôimpl√©mentation de ces projets peuvent fortement vari√©es en fonction du contexte technique et de l‚Äôobjectif m√©tier. Cette probl√©matique est d‚Äôautant plus forte que les pr√©requis en terme de ressources sont importants pour les projets d‚ÄôIA g√©n√©rative.\nBien que les contextes d‚Äôimpl√©mentation soient diverses, les briques et les concepts utilis√©s sont communs. Ce chapitre introduit les principaux outils n√©cessaires, et pr√©sente diff√©rents contextes de d√©ploiement. Il est d√©coup√© en 4 parties :\n\nüöß Architecture d‚Äôun projet LLM: D√©finition g√©n√©rale des composants d‚Äôun projet LLM, ainsi que des pistes de r√©flexion pour l‚Äôinfrastructure sous-jacente\nüî© Service LLM avanc√©: Description des principaux composants d‚Äôune architecture d‚Äôinf√©rence, ainsi qu‚Äôun premier exemple d‚Äôimpl√©mentation de ces outils\nüè≠ Service LLM √† grande √©chelle: Orchestration et mise en place d‚Äôune infrastructure d‚Äôinf√©rence √† grande √©chelle\nüíª D√©ploiement d‚Äôapplications: Exemples de d√©ploiements d‚Äôinterfaces pour des fonctionnalit√©s de tchat et de RAG.",
    "crumbs": [
      "III-Deploiements"
    ]
  },
  {
    "objectID": "II-Developpements/3_RAG.html",
    "href": "II-Developpements/3_RAG.html",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Le Retrieval Augmented Generation (RAG) est une technique courante utilis√©e pour am√©liorer les performances d‚Äôun LLM et de pallier ces d√©ficiences. En effet les LLMs sont tr√®s performants pour de la g√©n√©ration de textes mais ils peuvent commettre des hallucinations, c‚Äôest-√†-dire des affirmations qui semblent plausibles mais qui n‚Äôont aucun fondement. Leur connaissance est aussi limit√©e par les donn√©es qui ont √©t√© utilis√©es pour l‚Äôentrainement, par exemple, un llm n‚Äôa pas acc√®s aux informations contenues dans une base de connaissance priv√©e car ces donn√©es ne sont pas contenues dans le corpus d‚Äôentrainement du llm.\nPour rem√©dier √† ces limitations le RAG propose de permettre √† un LLM de s‚Äôappuyer sur une base de connaissance, g√©n√©ralement sous la forme de document contenant du texte pour r√©pondre de mani√®re pertinente √† des questions, en utilisant de mani√®re ad√©quate les informations contenues dans la base de connaissance.\nLe LLM sera ainsi en mesure de r√©pondre √† des questions portant sur des points pr√©cis et sur des donn√©es sur lequel il n‚Äôa jamais √©t√© entrain√©. En exploitant la base de connaissance il aura aussi moins tendance √† inventer de fausses informations et donc √† halluciner.\n\n\nUn syst√®me de RAG va combiner la capacit√© de g√©n√©ration de textes d‚Äôun LLM avec de la recherche d‚Äôinformation dans une base de connaissance interne. Le RAG se compose principalement de deux parties, une premi√®re partie de retrieval dont le r√¥le est d‚Äôanalyser une question de l‚Äôutilisateur et de trouver des √©l√©ments qui sont pertinents pour r√©pondre √† la question dans la base, et une seconde partie de g√©n√©ration qui contient le LLM et qui va incorporer le contexte r√©cup√©r√©s par la partie de retrieval dans un prompt pour permettre au LLM de g√©n√©rer une r√©ponse bas√©e sur des √©l√©ments pertinents.\n\n\n\nLa partie de retrieval qui s‚Äôoccupe de la recherche d‚Äôinformations pertinentes dans la base joue un r√¥le essentiel pour le bon fonctionnement du RAG. En effet si des informations non pertinentes sont retourn√©es au LLM il devient tr√®s difficile pour lui de formuler une r√©ponse ad√©quate.\nIl existe de nombreux algorithmes permettant d‚Äôeffectuer cette recherche d‚Äôinformation, cependant l‚Äôune des approches les plus populaires se base sur l‚Äôutilisation de vecteurs denses. Dans cette m√©thode on utilise un encodeur pour transformer le texte en vecteurs de grandes dimensions que l‚Äôon stocke dans une base de donn√©es vectorielle. Cet encodeur a √©t√© entrain√© a projeter des textes s√©mantiquement proches sur des vecteurs similaires. Lorsqu‚Äôune requ√™te utilisateur est re√ßue en entr√©e, elle est elle aussi encod√©e dans un vecteur de m√™me dimension. On peut alors comparer ce vecteur √† l‚Äôensemble des vecteurs pr√©sents en base ce qui permet de r√©cup√©rer les vecteurs les plus proches qui correspondent √† ceux qui sont s√©mantiquement proches. On peut ainsi trouver les morceaux de texte √† envoyer au LLM pour la g√©n√©ration.\n\n\n\nLa phase de g√©n√©ration dans un syst√®me de RAG intervient apr√®s la r√©cup√©ration des documents pertinents. Une fois les documents r√©cup√©r√©s, ils sont ins√©r√©s dans un prompt que le LLM utilise pour produire des r√©ponses contextuellement appropri√©es et pr√©cises. Plus le LLM est performant et correctement align√© sur les pr√©f√©rences humaines plus il est capable de prendre une quantit√© importante de document en contexte et ainsi de mieux r√©pondre √† la question.\n\n\n\nToutes les solutions test√©es sont open-source ou disposant d‚Äôune licence permissive, qui donne la possibilit√© d‚Äôh√©berger localement les donn√©es.\n\n\n\n\n\nWeaviate\n\n\nMilvus\n\n\nQdrant\n\n\nElasticSearch\n\n\nFAISS\n\n\n\n\nOpen source\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\nPartiellement\n\n\n‚úÖ\n\n\n\n\nDev-friendly\n\n\n+++\n\n\n+\n\n\n+++\n\n\n++\n\n\n+++\n\n\n\n\nD√©ploiement\n\n\n‚úÖ\n\n\n‚úÖ mais difficile √† mettre en place, constellation de micro-services\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚ùå mais possibilit√© de construire une image Docker custom par exemple\n\n\n\n\nSp√©cifique √† la recherche vectorielle\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚ùå\n\n\n‚úÖ\n\n\n\n\nQualit√© de la documentation\n\n\n+++ [-]\n\n\n++ [-]\n\n\n+++ [-]\n\n\n++ [-]\n\n\n+ [-]\n\n\n\n\nDerni√®re mise √† jour\n\n\nmai 2024\n\n\nmai 2024\n\n\nmai 2024\n\n\nmai 2024\n\n\nmars 2024\n\n\n\n\nLatence (ms)**\n\n\n438.18\n\n\n322.63\n\n\n118.25\n\n\n338.53\n\n\n-\n\n\n\n\nRequ√™tes/seconde\n(RPS)**\n\n\n217.98\n\n\n281.52\n\n\n710.23\n\n\n275.11\n\n\n-\n\n\n\n\nP99 latence (ms)**\n\n\n1723.62\n\n\n436.87\n\n\n144.78\n\n\n589.61\n\n\n-\n\n\n\n\nTemps d‚Äôupload (minutes)**\n\n\n71.61\n\n\n1.41\n\n\n2.074\n\n\n14.33\n\n\n-\n\n\n\n\nTemps d‚Äôupload +\nindexation (minutes)**\n\n\n71.61\n\n\n9.53\n\n\n17.49\n\n\n122.79\n‚ùå\n\n\n-\n\n\n\n\nPlace en m√©moire\n\n\nnum_vectors * vector_dimension * 4 bytes * 2 [-]\n\n\nCons√©quente d‚Äôapr√®s les avis d‚Äôutilisateurs, pas de formules approximative [-]\n\n\nnum_vectors * vector_dimension * 4 bytes * 1.5 [-]\n\n\nnum_vectors * 4 * (vector_dimension + 12) [-]\n\n\n?\n\n\n\n\nType d‚Äôindex\n\n\nHNSW\n\n\nFLAT, IVF_FLAT, IVF_SQ8, IVF_PQ, HNSW, BIN_FLAT, BIN_IVF_FLAT, DiskANN, GPU_IVF_FLAT, GPU_IVF_PQ, and CAGRA\n\n\nHNSW\n\n\nHNSW\n\n\nFLAT, IVS_FLAT, IVF_SQ8, IVF_PQ, HNSW, BIN_FLAT and BIN_IVF_FLAT\n\n\n\n\nRecherche hybride\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚ùå\n\n\n\n\nAjout d‚Äô√©l√©ments √† la vol√©e, scalabilit√©\n\n\nPartitionnement statique\n\n\nSegmentation\ndynamique\n\n\nPartitionnement statique\n\n\nPartitionnement statique\n\n\n‚ùå (index\nimmutable -&gt; vector library)\n\n\n\n\nAcc√®s contr√¥l√© par r√¥les\n\n\n‚ùå sur le backlog, mais n‚Äôavance beaucoup [-]\n\n\n‚úÖ [-]\n\n\n‚úÖ [-]\n\n\n‚úÖ [-]\n\n\n‚ùå\n\n\n\n\nPartions et √©tanch√©it√© des bases de donn√©es\n(multi-tenancy)\n\n\n‚ùå pas tr√®s clair, mais il semble que ce ne soit pas encore possible [-]\n\n\n‚úÖ Plusieurs syst√®mes de partitions, tr√®s flexible [-]\n\n\n‚úÖ Plusieurs syst√®mes de partitions, assez flexible [-]\n\n\n‚úÖ Possible mais pas tr√®s intuitif [-]\n\n\n‚ùå\n\n\n\n\nAutres avantages\n\n\n\n\n\nTr√®s dynamique car chaque action a son propre node, facile √† scaler\nPlusieurs niveaux de partitions\n\n\n\n\n\n\nStockage d‚Äôautres types de donn√©es, par exemple l‚Äôhistorique de conversations\nTr√®s commun comme solution de stockage, donc plus d‚Äôutilisateurs d√©j√† familiers de l‚Äôoutil\n\n\n\n\n\n\n\nAutres inconv√©nients\n\n\n\n\n\nTaille en m√©moire (difficile √† quantifier par rapport aux autres, mais plus importante selon les benchmarks)\n\n\n\n\nPas de stockage S3\n\n\n\n\n\n\nBiblioth√®que de vecteurs, pas vraiment adapt√©e √† un usage persistant\n\n\n\n\n** Qdrant benchmark (janvier 2024), dataset = gist-960-euclidean (1M de vecteurs en dimension 960), pr√©cision √† 0.95\nLes solutions pr√©sent√©es recouvrent en fait plusieurs cas d‚Äôusage :\n\nLes biblioth√®ques vectorielles (vector library) de type FAISS sont adapt√©es √† de la rechercher s√©mantique √† la vol√©e, avec constitution de la base et recherche imm√©diate. Ici il s‚Äôagira d‚Äôun cas d‚Äôusage o√π l‚Äôutilisateur apporte son propre document avec un t√©l√©chargement en temps r√©el, et pose des questions dessus ou demande une synth√®se.\nLes bases de donn√©es vectorielles (vector database) sont des dispositifs plus lourds et g√©n√©ralement un peu plus lents, mais avec un stockage permanent et beaucoup plus de flexibilit√© dans la recherche. Ils sont plus adapt√©s √† un cas d‚Äôusage o√π la base de connaissance est constitu√©e en amont et doit √™tre mise √† jour de temps en temps.\n\nPour une mise en production rapide et efficace Qdrant semble √™tre la meilleure solution, combin√© √† une base de donn√©es plus traditionnelle comme ElasticSearch pour l‚Äôhistorique des conversations. Pour avoir une approche tout-en-en, et plus de flexibilit√© dans la gestion des collections, c‚Äôest ElasticSearch qui se d√©tache des autres, malgr√© des temps d‚Äôindexation assez cons√©quents.\n\nApart√© sur les int√©grations Langchain : √† manipuler avec pr√©caution, les fonctions ne sont pas toujours explicites (par exemple la m√©thode from_documents supprime et recr√©e en g√©n√©ral une collection). De plus certaines fonctionnalit√©s comme l‚Äôutilisation de partitions ne sont pas toujours accessibles via Langchain. Il peut √™tre utile de recr√©er des wrapper qui utilisent en partie Langchain et en partie les fonctions natives de la base de donn√©es.\n\n\n\n\n\nDev-friendly -&gt; Note qualitative apr√®s installation de chaque solution (sauf FAISS) dans une image Docker, et utilisation avec Python (avec et sans l‚Äôint√©gration Langchain)\nD√©ploiement -&gt; Existence d‚Äôun √©cosyst√®me de d√©ploiement\nQualit√© de la documentation -&gt; Note qualitative apr√®s installation de chaque solution (sauf FAISS) dans une image Docker, et utilisation avec Python (avec et sans l‚Äôint√©gration Langchain)\nAjout d‚Äô√©l√©ments √† la vol√©e, scalabilit√© -&gt; Comment l‚Äôindexation se fait si la base de donn√©es est modifi√©e. Avec le partitionnement statique (static sharding), si la capacit√© du serveur est augment√©e toutes les donn√©es doivent √™tre de nouveau partitionn√©es, ce qui peut √™tre long.\nRecherche hybride -&gt; Possibilit√© d‚Äôeffectuer des recherches dans les m√©tadonn√©es, avec des nombres ou des cha√Ænes de caract√®res\nAcc√®s contr√¥l√© par r√¥les (RBAC)-&gt; Autorisations pr√©d√©finies pour chaque utilisateur, avec un acc√®s diff√©renci√© aux documents\n\n\n\nhttps://weaviate.io/blog/vector-library-vs-vector-database\nANN Benchmark (avril 2023)",
    "crumbs": [
      "II-D√©veloppements",
      "Focus sur le RAG (Retrieval Augmented Generation)"
    ]
  },
  {
    "objectID": "II-Developpements/3_RAG.html#iii.-focus-sur-le-rag-retrieval-augmented-generation",
    "href": "II-Developpements/3_RAG.html#iii.-focus-sur-le-rag-retrieval-augmented-generation",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Le Retrieval Augmented Generation (RAG) est une technique courante utilis√©e pour am√©liorer les performances d‚Äôun LLM et de pallier ces d√©ficiences. En effet les LLMs sont tr√®s performants pour de la g√©n√©ration de textes mais ils peuvent commettre des hallucinations, c‚Äôest-√†-dire des affirmations qui semblent plausibles mais qui n‚Äôont aucun fondement. Leur connaissance est aussi limit√©e par les donn√©es qui ont √©t√© utilis√©es pour l‚Äôentrainement, par exemple, un llm n‚Äôa pas acc√®s aux informations contenues dans une base de connaissance priv√©e car ces donn√©es ne sont pas contenues dans le corpus d‚Äôentrainement du llm.\nPour rem√©dier √† ces limitations le RAG propose de permettre √† un LLM de s‚Äôappuyer sur une base de connaissance, g√©n√©ralement sous la forme de document contenant du texte pour r√©pondre de mani√®re pertinente √† des questions, en utilisant de mani√®re ad√©quate les informations contenues dans la base de connaissance.\nLe LLM sera ainsi en mesure de r√©pondre √† des questions portant sur des points pr√©cis et sur des donn√©es sur lequel il n‚Äôa jamais √©t√© entrain√©. En exploitant la base de connaissance il aura aussi moins tendance √† inventer de fausses informations et donc √† halluciner.\n\n\nUn syst√®me de RAG va combiner la capacit√© de g√©n√©ration de textes d‚Äôun LLM avec de la recherche d‚Äôinformation dans une base de connaissance interne. Le RAG se compose principalement de deux parties, une premi√®re partie de retrieval dont le r√¥le est d‚Äôanalyser une question de l‚Äôutilisateur et de trouver des √©l√©ments qui sont pertinents pour r√©pondre √† la question dans la base, et une seconde partie de g√©n√©ration qui contient le LLM et qui va incorporer le contexte r√©cup√©r√©s par la partie de retrieval dans un prompt pour permettre au LLM de g√©n√©rer une r√©ponse bas√©e sur des √©l√©ments pertinents.\n\n\n\nLa partie de retrieval qui s‚Äôoccupe de la recherche d‚Äôinformations pertinentes dans la base joue un r√¥le essentiel pour le bon fonctionnement du RAG. En effet si des informations non pertinentes sont retourn√©es au LLM il devient tr√®s difficile pour lui de formuler une r√©ponse ad√©quate.\nIl existe de nombreux algorithmes permettant d‚Äôeffectuer cette recherche d‚Äôinformation, cependant l‚Äôune des approches les plus populaires se base sur l‚Äôutilisation de vecteurs denses. Dans cette m√©thode on utilise un encodeur pour transformer le texte en vecteurs de grandes dimensions que l‚Äôon stocke dans une base de donn√©es vectorielle. Cet encodeur a √©t√© entrain√© a projeter des textes s√©mantiquement proches sur des vecteurs similaires. Lorsqu‚Äôune requ√™te utilisateur est re√ßue en entr√©e, elle est elle aussi encod√©e dans un vecteur de m√™me dimension. On peut alors comparer ce vecteur √† l‚Äôensemble des vecteurs pr√©sents en base ce qui permet de r√©cup√©rer les vecteurs les plus proches qui correspondent √† ceux qui sont s√©mantiquement proches. On peut ainsi trouver les morceaux de texte √† envoyer au LLM pour la g√©n√©ration.\n\n\n\nLa phase de g√©n√©ration dans un syst√®me de RAG intervient apr√®s la r√©cup√©ration des documents pertinents. Une fois les documents r√©cup√©r√©s, ils sont ins√©r√©s dans un prompt que le LLM utilise pour produire des r√©ponses contextuellement appropri√©es et pr√©cises. Plus le LLM est performant et correctement align√© sur les pr√©f√©rences humaines plus il est capable de prendre une quantit√© importante de document en contexte et ainsi de mieux r√©pondre √† la question.\n\n\n\nToutes les solutions test√©es sont open-source ou disposant d‚Äôune licence permissive, qui donne la possibilit√© d‚Äôh√©berger localement les donn√©es.\n\n\n\n\n\nWeaviate\n\n\nMilvus\n\n\nQdrant\n\n\nElasticSearch\n\n\nFAISS\n\n\n\n\nOpen source\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\nPartiellement\n\n\n‚úÖ\n\n\n\n\nDev-friendly\n\n\n+++\n\n\n+\n\n\n+++\n\n\n++\n\n\n+++\n\n\n\n\nD√©ploiement\n\n\n‚úÖ\n\n\n‚úÖ mais difficile √† mettre en place, constellation de micro-services\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚ùå mais possibilit√© de construire une image Docker custom par exemple\n\n\n\n\nSp√©cifique √† la recherche vectorielle\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚ùå\n\n\n‚úÖ\n\n\n\n\nQualit√© de la documentation\n\n\n+++ [-]\n\n\n++ [-]\n\n\n+++ [-]\n\n\n++ [-]\n\n\n+ [-]\n\n\n\n\nDerni√®re mise √† jour\n\n\nmai 2024\n\n\nmai 2024\n\n\nmai 2024\n\n\nmai 2024\n\n\nmars 2024\n\n\n\n\nLatence (ms)**\n\n\n438.18\n\n\n322.63\n\n\n118.25\n\n\n338.53\n\n\n-\n\n\n\n\nRequ√™tes/seconde\n(RPS)**\n\n\n217.98\n\n\n281.52\n\n\n710.23\n\n\n275.11\n\n\n-\n\n\n\n\nP99 latence (ms)**\n\n\n1723.62\n\n\n436.87\n\n\n144.78\n\n\n589.61\n\n\n-\n\n\n\n\nTemps d‚Äôupload (minutes)**\n\n\n71.61\n\n\n1.41\n\n\n2.074\n\n\n14.33\n\n\n-\n\n\n\n\nTemps d‚Äôupload +\nindexation (minutes)**\n\n\n71.61\n\n\n9.53\n\n\n17.49\n\n\n122.79\n‚ùå\n\n\n-\n\n\n\n\nPlace en m√©moire\n\n\nnum_vectors * vector_dimension * 4 bytes * 2 [-]\n\n\nCons√©quente d‚Äôapr√®s les avis d‚Äôutilisateurs, pas de formules approximative [-]\n\n\nnum_vectors * vector_dimension * 4 bytes * 1.5 [-]\n\n\nnum_vectors * 4 * (vector_dimension + 12) [-]\n\n\n?\n\n\n\n\nType d‚Äôindex\n\n\nHNSW\n\n\nFLAT, IVF_FLAT, IVF_SQ8, IVF_PQ, HNSW, BIN_FLAT, BIN_IVF_FLAT, DiskANN, GPU_IVF_FLAT, GPU_IVF_PQ, and CAGRA\n\n\nHNSW\n\n\nHNSW\n\n\nFLAT, IVS_FLAT, IVF_SQ8, IVF_PQ, HNSW, BIN_FLAT and BIN_IVF_FLAT\n\n\n\n\nRecherche hybride\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚úÖ\n\n\n‚ùå\n\n\n\n\nAjout d‚Äô√©l√©ments √† la vol√©e, scalabilit√©\n\n\nPartitionnement statique\n\n\nSegmentation\ndynamique\n\n\nPartitionnement statique\n\n\nPartitionnement statique\n\n\n‚ùå (index\nimmutable -&gt; vector library)\n\n\n\n\nAcc√®s contr√¥l√© par r√¥les\n\n\n‚ùå sur le backlog, mais n‚Äôavance beaucoup [-]\n\n\n‚úÖ [-]\n\n\n‚úÖ [-]\n\n\n‚úÖ [-]\n\n\n‚ùå\n\n\n\n\nPartions et √©tanch√©it√© des bases de donn√©es\n(multi-tenancy)\n\n\n‚ùå pas tr√®s clair, mais il semble que ce ne soit pas encore possible [-]\n\n\n‚úÖ Plusieurs syst√®mes de partitions, tr√®s flexible [-]\n\n\n‚úÖ Plusieurs syst√®mes de partitions, assez flexible [-]\n\n\n‚úÖ Possible mais pas tr√®s intuitif [-]\n\n\n‚ùå\n\n\n\n\nAutres avantages\n\n\n\n\n\nTr√®s dynamique car chaque action a son propre node, facile √† scaler\nPlusieurs niveaux de partitions\n\n\n\n\n\n\nStockage d‚Äôautres types de donn√©es, par exemple l‚Äôhistorique de conversations\nTr√®s commun comme solution de stockage, donc plus d‚Äôutilisateurs d√©j√† familiers de l‚Äôoutil\n\n\n\n\n\n\n\nAutres inconv√©nients\n\n\n\n\n\nTaille en m√©moire (difficile √† quantifier par rapport aux autres, mais plus importante selon les benchmarks)\n\n\n\n\nPas de stockage S3\n\n\n\n\n\n\nBiblioth√®que de vecteurs, pas vraiment adapt√©e √† un usage persistant\n\n\n\n\n** Qdrant benchmark (janvier 2024), dataset = gist-960-euclidean (1M de vecteurs en dimension 960), pr√©cision √† 0.95\nLes solutions pr√©sent√©es recouvrent en fait plusieurs cas d‚Äôusage :\n\nLes biblioth√®ques vectorielles (vector library) de type FAISS sont adapt√©es √† de la rechercher s√©mantique √† la vol√©e, avec constitution de la base et recherche imm√©diate. Ici il s‚Äôagira d‚Äôun cas d‚Äôusage o√π l‚Äôutilisateur apporte son propre document avec un t√©l√©chargement en temps r√©el, et pose des questions dessus ou demande une synth√®se.\nLes bases de donn√©es vectorielles (vector database) sont des dispositifs plus lourds et g√©n√©ralement un peu plus lents, mais avec un stockage permanent et beaucoup plus de flexibilit√© dans la recherche. Ils sont plus adapt√©s √† un cas d‚Äôusage o√π la base de connaissance est constitu√©e en amont et doit √™tre mise √† jour de temps en temps.\n\nPour une mise en production rapide et efficace Qdrant semble √™tre la meilleure solution, combin√© √† une base de donn√©es plus traditionnelle comme ElasticSearch pour l‚Äôhistorique des conversations. Pour avoir une approche tout-en-en, et plus de flexibilit√© dans la gestion des collections, c‚Äôest ElasticSearch qui se d√©tache des autres, malgr√© des temps d‚Äôindexation assez cons√©quents.\n\nApart√© sur les int√©grations Langchain : √† manipuler avec pr√©caution, les fonctions ne sont pas toujours explicites (par exemple la m√©thode from_documents supprime et recr√©e en g√©n√©ral une collection). De plus certaines fonctionnalit√©s comme l‚Äôutilisation de partitions ne sont pas toujours accessibles via Langchain. Il peut √™tre utile de recr√©er des wrapper qui utilisent en partie Langchain et en partie les fonctions natives de la base de donn√©es.\n\n\n\n\n\nDev-friendly -&gt; Note qualitative apr√®s installation de chaque solution (sauf FAISS) dans une image Docker, et utilisation avec Python (avec et sans l‚Äôint√©gration Langchain)\nD√©ploiement -&gt; Existence d‚Äôun √©cosyst√®me de d√©ploiement\nQualit√© de la documentation -&gt; Note qualitative apr√®s installation de chaque solution (sauf FAISS) dans une image Docker, et utilisation avec Python (avec et sans l‚Äôint√©gration Langchain)\nAjout d‚Äô√©l√©ments √† la vol√©e, scalabilit√© -&gt; Comment l‚Äôindexation se fait si la base de donn√©es est modifi√©e. Avec le partitionnement statique (static sharding), si la capacit√© du serveur est augment√©e toutes les donn√©es doivent √™tre de nouveau partitionn√©es, ce qui peut √™tre long.\nRecherche hybride -&gt; Possibilit√© d‚Äôeffectuer des recherches dans les m√©tadonn√©es, avec des nombres ou des cha√Ænes de caract√®res\nAcc√®s contr√¥l√© par r√¥les (RBAC)-&gt; Autorisations pr√©d√©finies pour chaque utilisateur, avec un acc√®s diff√©renci√© aux documents\n\n\n\nhttps://weaviate.io/blog/vector-library-vs-vector-database\nANN Benchmark (avril 2023)",
    "crumbs": [
      "II-D√©veloppements",
      "Focus sur le RAG (Retrieval Augmented Generation)"
    ]
  },
  {
    "objectID": "II-Developpements/1_Anatomie_LLM.html",
    "href": "II-Developpements/1_Anatomie_LLM.html",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Les LLMs reposent sur un d√©veloppement en deux voire trois √©tapes.\n\nLe pr√©-entra√Ænement consiste √† entra√Æner le mod√®le, en partant de z√©ro, de fa√ßon auto-supervis√©e, et sur un corpus d‚Äôentra√Ænement gigantesque. L‚Äôobjectif de ce pr√©-entra√Ænement d√©pend du type de mod√®le utilis√© (cf.¬†paragraphe suivant), mais la plupart apprennent √† pr√©dire le token suivant, √† partir d‚Äôune suite de tokens. C‚Äôest ce qui les rend particuli√®rement efficaces pour de la g√©n√©ration de texte.\nL‚Äôinstruction-tuning permet d‚Äôadapter le mod√®le pr√©-entra√Æn√© √† une plus grande diversit√© de t√¢ches. Dans de nombreux cas (chatbot, r√©sum√© de texte, etc.), la pr√©diction du token suivant n‚Äôest pas la bonne strat√©gie. L‚Äô√©tape d‚Äôinstruction-tuning permet ainsi, gr√¢ce √† un entra√Ænement supervis√©, de cr√©er une version ¬´¬†chat¬†¬ª du mod√®le. Pour donner un exemple connu de tous, ChatGPT est la version instruction-tun√©e de GPT-4.\nLe fine-tuning (optionnel) peut √™tre utilis√© pour adapter le mod√®le √† une t√¢che et √† des donn√©es sp√©cifiques. Les LLMs √©tant des outils multit√¢ches, souvent multilingues et multidomaines, leurs performances peuvent √™tre d√©grad√©es lorsqu‚Äôil y a des exigences pr√©cises et sp√©cifiques. Le fine-tuning est une nouvelle phase d‚Äôentra√Ænement supervis√©, n√©cessitant moins de donn√©es et de puissance de calcul, qui permet de sp√©cialiser le mod√®le.\n\nPar leur taille et les exigences techniques qu‚Äôils impliquent, seules quelques entreprises sp√©cialis√©es ont les moyens de pr√©-entra√Æner et d‚Äôinstruction-tuner des LLMs. Le fine-tuning, en revanche, peut √™tre abordable pour beaucoup plus d‚Äôacteurs, pour peu qu‚Äôils r√©pondent √† certaines exigences techniques (cf.¬†partie sur le fine-tuning).\nPour donner des ordres de grandeur, la petite version du dernier mod√®le de Meta, Llama-3 8B, a √©t√© pr√©-entra√Æn√© et instruction-tun√© sur un corpus de 15 trillions de tokens. Ces deux phases d‚Äôentra√Ænement ont n√©cessit√© 1,3 millions d‚Äôheures GPU, r√©parties sur plusieurs milliers de GPU H100.\n\nArticle r√©sumant la dualit√© pr√©-entra√Ænement/fine-tuning\n\n\n\n\n\nIntroduite en 2017 dans le papier Attention Is All You Need, l‚Äôarchitecture Transformer a r√©volutionn√© le domaine du TAL. Par rapport aux RNN, les Transformers permettent un traitement efficace des s√©quences en parall√®le, conduisant √† un temps de calcul beaucoup plus court (tant lors de l‚Äôentra√Ænement qu‚Äôen inf√©rence), tandis que les RNN, par construction, ne peuvent traiter une s√©quence que s√©quentiellement, c‚Äôest-√†-dire token par token. En outre, le m√©canisme d‚Äôauto-attention, pr√©sent√© ci-dessous, permet de capturer efficacement les d√©pendances distantes en att√©nuant le probl√®me de la disparition et de l‚Äôexplosion des gradients.\n\nL‚Äôauto-attention est le m√©canisme central des Transformers. Elle est utilis√©e pour pond√©rer, lors de l‚Äôexamen d‚Äôun token en particulier, l‚Äôimportance, relative √† ce token, de chaque autre token de la s√©quence. Concr√®tement, trois vecteurs (qui repr√©sentent chacun la s√©quence d‚Äôentr√©e dans un r√¥le diff√©rent) sont d√©duits de la s√©quence d‚Äôentr√©e \\(X\\) : les requ√™tes (\\(Q\\)), les cl√©s (\\(K\\)) et les valeurs (\\(V\\)), par des transformations lin√©aires comme exprim√©es dans l‚Äô√©quation suivantes. Les matrices \\(W_Q\\), \\(W_K\\) et \\(W_V\\) sont des param√®tres entra√Ænables du mod√®le. \\[Q = X \\cdot W_Q \\qquad K = X \\cdot W_K \\qquad V = X \\cdot W_V \\]\nLes scores d‚Äôattention sont ensuite calcul√©s selon l‚Äô√©quation suivante. \\[\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^{T}}{\\sqrt{d}} \\right) V\\]\nPour chaque token d‚Äôentr√©e \\(X_i\\), le r√©sultat \\(\\text{Attention}(Q, K, V)_i\\) est une combinaison de tous les autres √©l√©ments de la s√©quence, pond√©r√©s selon leur pertinence par rapport √† \\(X_i\\).\nL‚Äôauto-attention telle que pr√©sent√©e ci-dessus n‚Äôest cependant pas directement utilis√©e dans l‚Äôarchitecture Transformer. A la place, une extension, appel√©e attention multi-t√™tes, permet au mod√®le de capturer plusieurs aspects des relations et des d√©pendances entre les √©l√©ments de la s√©quence d‚Äôentr√©e. Cela est fait en transformant la s√©quence d‚Äôentr√©e en plusieurs t√™tes, i.e.¬†en plusieurs vecteurs de requ√™tes, de cl√©s et de valeurs, et en appliquant un m√©canisme d‚Äôauto-attention sur chacune de ces t√™tes. Les vecteurs d‚Äôattention de chaque t√™te sont ensuite concat√©n√©s et r√©duits lin√©airement √† la taille d‚Äôentr√©e d‚Äôorigine. Le calcul de l‚Äôattention multi-t√™tes est d√©taill√© dans l‚Äô√©quation suivante.\n\\[\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(\\text{head}_1 , \\cdots, \\text{head}_h)W^O\\]\no√π \\(\\text{head}_i = \\text{Attention}(X \\cdot W_Q^i, X \\cdot W_K^i, X \\cdot W_V^i)\\) pour \\(i = 1, \\cdot, h\\) avec \\(h\\) le nombre de t√™tes d‚Äôattention. Chaque t√™te d‚Äôattention peut donc se sp√©cialiser dans un aspect sp√©cifique des donn√©es, et le mod√®le peut apprendre √† combiner ces diff√©rents aspects pour une meilleure repr√©sentation. La combinaison de ce m√©canisme d‚Äôattention multi-t√™tes, de couches de normalisation et de couches √† action directes (FNN) forme un bloc Transformer.\nPlusieurs blocs (6 dans l‚Äôimpl√©mentation originale) forment ensuite l‚Äôencodeur (qui a acc√®s √† la s√©quence d‚Äôentr√©e dans son int√©gralit√©) et le d√©codeur (qui a acc√®s √† la repr√©sention encod√©e de la s√©quence d‚Äôentr√©e, et √† la s√©quence de sortie g√©n√©r√©e jusqu‚Äôalors). La combinaison de ces deux √©l√©ments composent le Transformer encodeur-d√©codeur original.\n\nPapier original ‚ÄòAttention Is All You Need‚Äô\nExplication illustr√©e et tr√®s d√©taill√©e\n\n\n\n\nLes LLMs bas√©s sur des architectures Transformers appartiennent √† l‚Äôune des 3 cat√©gories suivantes¬†:\n\nMod√®le ¬´¬†encoder-only¬†¬ª¬†: Ils sont bas√©s uniquement sur la partie encodeur des Transformers. Leur pr√©-entra√Ænement est souvent bas√© sur la reconstruction de phrases¬†: √† chaque √©tape, le mod√®le a acc√®s √† une phrase enti√®re, sauf certains mots qui ont √©t√© masqu√©s, et apprend √† retrouver ces mots masqu√©s. Ces mod√®les sont adapt√©s pour des t√¢ches de classification, de reconnaissance d‚Äôentit√©s nomm√©es (NER), de r√©ponses aux questions, etc. Ils ont aujourd‚Äôhui perdu en popularit√©, mais leurs repr√©sentants les plus connus (BERT, RoBERTa, DistilBERT, CamemBERT, etc.) sont encore tr√®s utilis√©s, et restent un choix int√©ressant selon la t√¢che, gr√¢ce √† leur compr√©hension fine du langage et √† leur petite taille.\nMod√®le ¬´¬†decoder-only¬†¬ª¬†: Ils sont bas√©s uniquement sur la partie d√©codeur des Transformers. Ces mod√®les sont aujourd‚Äôhui la norme, et l‚Äôimmense majorit√© des LLMs actuels utilisent cette architecture. Leur pr√©-entra√Ænement est bas√© sur la pr√©diction du prochain token¬†: √† chaque √©tape, le mod√®le a acc√®s au d√©but d‚Äôune phrase, et apprend √† pr√©dire le token suivant. Pour cette raison, ces mod√®les sont √©galement qualifi√©s d‚Äô¬´¬†autor√©gressifs¬†¬ª. Les mod√®les GPT (2, 3, 4), Llama (2, 3), Mistral,¬†Gemini, etc. sont tous des decoder-only.\nMod√®le ¬´¬†encoder-decoder¬†¬ª¬†: Ils utilisent les deux blocs des Transformers. L‚Äôencodeur a ainsi acc√®s √† l‚Äôint√©gralit√© de la s√©quence d‚Äôentr√©e, alors que le d√©codeur a acc√®s √† la repr√©sentation cach√©e de l‚Äôentr√©e et aux tokens g√©n√©r√©s jusqu‚Äôalors. Les mod√®les les plus connus sont par exemple BART et T5.\n\nhttps://medium.com/artificial-corner/discovering-llm-structures-decoder-only-encoder-only-or-decoder-encoder-5036b0e9e88\n\n\n\nLes architectures Mixture of Experts ne sont pas sp√©cifiques aux LLMs, mais elles ont √©t√© adapt√©es avec succ√®s sur des mod√®les comme Mixtral 8x7B, Mixtral 8x22B ou GPT-4 (supposition). Le principe est de remplacer chaque r√©seau √† propagation directe (pr√©sent dans chaque bloc de l‚Äôarchitecture Transformer) par un ensemble de r√©seaux ¬´¬†experts¬†¬ª. Au moment de passer dans cette partie du r√©seau, un routeur envoie vers un de ces experts uniquement. L‚Äôint√©r√™t est double : un seul expert √©tant utilis√© √† la fois, le temps d‚Äôinf√©rence est naturellement nettement plus court. Par ailleurs, chaque r√©seau expert est entra√Æn√© et donc sp√©cialis√© diff√©rement des autres : pour un m√™me nombre de param√®tres, les performances sont donc suppos√©es √™tre meilleures qu‚Äôavec une architecture classique. En revanche, si tous les poids du mod√®les ne sont pas utilis√©s syst√©matiquement, c‚Äôest uniquement √† l‚Äôinf√©rence et √† chaque couche du r√©seau que l‚Äôexpert est choisi : il est donc tout de m√™me n√©cessaire de charger l‚Äôint√©gralit√© des poids du mod√®le en m√©moire, ce qui peut √™tre tr√®s co√ªteux en VRAM. Pour une explication plus technique, l‚Äôarticle suivant d√©taille tr√®s bien les MoE en prenant l‚Äôexemple de Mixtral.\n\nExplication d√©taill√©e des MoE (exemple de Mixtral) : https://huggingface.co/blog/moe\n\n\n\nLe principal inconv√©nient architectural des Transformers est leur complexit√© quadratique par rapport √† la taille de l‚Äôentr√©e (qui vient du calcul quadratique de l‚Äôattention). Mamba est une architecture r√©cente (D√©cembre 2023) qui s‚Äôaffranchit du m√©canisme d‚Äôattention, au profit de briques SSM (Structured State Space Models). L‚Äôint√©r√™t principal de cette architecture est sa complexit√© lin√©aire par rapport √† la taille de l‚Äôentr√©e.\nJamba est une nouvelle architecture hybride, √† mi-chemin entre le Transformer et Mamba. Cela semble permettre un niveau de performance √©lev√©, une gestion des contextes tr√®s longs, un temps d‚Äôinf√©rence nettement plus court, et des exigences m√©moires bien moindres.\nLiens des papiers originaux :\n\nMamba\nJamba\n\n\n\n\n\nLes LLM sont des r√©seaux de neurones de taille importante et font l‚Äôobjet d‚Äôentra√Ænement avec des ressources colossales (e.g: quelques dizaines de milliers de GPUs dernier mod√®le pendant 3 mois pour GPT-4). L‚Äôentra√Ænement permet d‚Äôapprendre un jeu de donn√©es particulier, en r√©glant l‚Äôensemble des poids du mod√®les (e.g: Mixtral 8x22B est une architecture √† 141 milliards de poids; 175 milliards pour GPT-3). Les LLM sont entra√Æn√©s √† r√©pondre √† plusieurs t√¢ches g√©n√©riques et ne sont pas forc√©ment pertinent pour des cas d‚Äôutilisation particulier.\nPour r√©pondre √† ce besoin, plusieurs m√©thodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste √† reprendre un mod√®le d√©j√† entra√Æn√© et √† l‚Äôadapter sur un jeu de donn√©es particulier sur une ou plusieurs t√¢ches sp√©cifiques. En g√©n√©ral, il s‚Äôagit de modifier une partie ou l‚Äôensemble des poids pour que le mod√®le soit plus pr√©cis pour les t√¢ches voulues. Le fine-tuning garde en grande partie les b√©n√©fices de l‚Äôentra√Ænement initial, i.e les connaissances ant√©rieures d√©j√† apprises. Repartir d‚Äôun mod√®le d√©j√† entra√Æn√© pourra r√©duire le temps d‚Äôentra√Ænement requis pour le fine-tuning, en fonction de la similarit√© entre la nouvelle t√¢che souhait√©e et son jeu de donn√©es et les entra√Ænements pr√©c√©dents.\nPour des petits mod√®les de langages, il est possible de r√©-entra√Æner en modifiant l‚Äôensemble des poids. Pour des mod√®les plus grands, modifier l‚Äôensemble des poids peut s‚Äôav√©rer couteux en temps et en GPUs. Plusieurs approches permettent de r√©-entra√Æner √† moindre co√ªt :\n\nr√©entrainer seulement un sous-ensemble de poids\nmodifier la t√™te de mod√©lisation de la langue (lm_head) pour certains mod√®les, soit en r√©entrainant depuis les poids entra√Æn√©s, soit en r√©initialisant ces poids.\ngarder l‚Äôint√©gralit√© du mod√®le et rajouter des poids √† entra√Æner puis utiliser l‚Äôapproximation de bas rang avec LORA (Low-Rank Adaptation) pour l‚Äôentra√Ænement et l‚Äôinf√©rence.\nutiliser des versions quantis√©es, i.e.¬†des mod√®les o√π les poids ont √©t√© tronqu√©s √† une pr√©cision inf√©rieure (possibilit√© de combiner avec la technique pr√©c√©dente, sous le nom de qLORA).\n\n\n\n\n\n\nImpl√©mentation HuggingFace\n\n\n\n\nPEFT = Parameter-Efficient Fine-Tuning | LoRA = Low-Rank Adaptation | QLoRA = Quantized Low-Rank Adaptation | DoRA = Weight-Decomposed Low-Rank Adaptation\nR√©-entra√Æner enti√®rement un LLM est tr√®s co√ªteux en termes d‚Äôinfrastructure et de donn√©es, et n‚Äôest donc pas √† la port√©e de n‚Äôimporte quelle organisation. Des m√©thodes ¬´ efficaces ¬ª ont √©t√© cr√©√©es pour rendre le fine-tuning facilement accessible, dont la plus connue et la plus populaire est LoRA (Low-Rank Adaptation). Son fonctionnement repose sur deux √©l√©ments :\n\nL‚Äôadaptation : Les poids du mod√®le pr√©-entra√Æn√© sont gel√©s pendant l‚Äôentra√Ænement. Ce sont des poids suppl√©mentaires (ceux de l‚Äôadapteur) qui vont √™tre entra√Æn√©s. Cela permet de garder l‚Äôenti√®ret√© du mod√®le pr√©-entra√Æn√© tel quel, et de rajouter uniquement la partie sp√©cifique √† chaque t√¢che. Entre autres, il est ainsi possible, avec un seul mod√®le de base, d‚Äôh√©berger plusieurs mod√®les sp√©cialis√©s √† moindre co√ªt. Le papier LoRA Land explique d‚Äôailleurs comment faire tenir 25 versions de Mistral 7B fine-tun√©s avec LoRA sur un seul GPU A100.\nLe rang faible : Les poids additionnels peuvent √™tre choisis de beaucoup de mani√®res. Avec LoRA, certaines couches du mod√®le (les couches d‚Äôattention ou les couches lin√©aires par exemple) sont s√©lectionn√©es, et les poids de ces couches sont exprim√©s comme une multiplication de deux matrices de rangs faibles, ce qui r√©duit grandement le nombre de poids √† entra√Æner (la valeur de ce rang √©tant un hyperparam√®tre de l‚Äôentra√Ænement). En fonction de la valeur de ce rang et des couches s√©lectionn√©es, il est ainsi possible d‚Äôentra√Æner uniquement 1 ou 2 % du nombre de param√®tres global du mod√®le pr√©-entra√Æn√©, sans que cela n‚Äôaffecte trop les performances du fine-tuning.\n\n\nD‚Äôautres approches de PEFT (Parameter-Efficient Fine-Tuning) ont vu le jour, dont la plupart s‚Äôinspirent de LoRA. Parmi les plus connues, QLoRA permet d‚Äôappliquer LoRA sur des mod√®les quantifi√©s, et DoRA propose un raffinement de l‚Äôadapteur de LoRA.\n\nGuide th√©orique tr√®s clair sur le PEFT (principe, avantages, etc.) avec un focus sur LoRA\nGuide pratique / Impl√©mentation HugginFace\n\nLiens des papiers originaux : - LoRA - QLoRA - DoRA\nEntra√Ænement avec qLORA en pratique :\nEn plus de la librairie transformers et datasets, les librairies peft, bitsandbytes et trl permettent de simplifier l‚Äôentra√Ænement avec qLORA\n(inspir√© du notebook suivant )\n%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U trl\n%pip install -U sentencepiece\n%pip install -U protobuf\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import load_dataset\nimport torch\nfrom trl import SFTTrainer\n\nbase_model = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"Mistral-7b-instruct-teletravail\"\n\npath_to_training_file=\"Dataset_public_accords_teletravail_Dares_train.parquet\"\npath_to_test_file=\"Dataset_public_accords_teletravail_Dares_test.parquet\"\n\n\ndataset=load_dataset(\"parquet\", data_files={'train': path_to_training_file, 'test': path_to_test_file})\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n\n\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()\n\ntrainer.model.save_pretrained(new_model)\n\n\n\n\nLe fine-tuning supervis√© est tr√®s efficace dans de nombreux cas, mais il pr√©sente notamment l‚Äôinconv√©nient de n√©cessiter une quantit√© importante de donn√©es. La constitution d‚Äôune base de questions-r√©ponses attendues par exemple peut se r√©veler co√ªteuse. Un autre moyen d‚Äôam√©liorer un mod√®le est d‚Äôutiliser de l‚Äôapprentissage par renforcement. La premi√®re version utilis√©e pour r√©-entra√Æner un LLM est le RLHF (Reinforcement Learning from Human Feedback), qui consiste √† r√©colter des retours d‚Äôutilisateurs humains (typiquement, entre deux r√©ponses g√©n√©r√©es par un LLM, l‚Äôutilisateur va dire laquelle il pr√©f√®re), puis √† mettre √† jour les poids du mod√®le, par un algorithme d‚Äôapprentissage par renforcement, de telle sorte que la r√©ponse pr√©f√©r√©e par l‚Äôutilisateur ait plus de chances d‚Äô√™tre g√©n√©r√©e. Cette approche s‚Äôest r√©v√©l√©e particuli√®rement effiace pour ¬´¬†aligner¬†¬ª le mod√®le aux pr√©f√©rences humaines, en termes de biais, de toxicit√©, de style, etc.\nBien que la constitution d‚Äôune base de retours humains soit moins co√ªteuse que celle d‚Äôune base de questions/r√©ponses, elle reste co√ªteuse. Une solution aujourd‚Äôhui tr√®s populaire est de remplacer ces retours humains par des retours g√©n√©r√©s artificiellement, ce qui donne une approche appel√©e RLAIF (Reinforcement Learning from Artificial Intelligence Feedback). Typiquement, un LLM plus performant (par exemple GPT-4) va √™tre utilis√© pour d√©terminer la meilleure r√©ponse entre deux ou plusieurs choix, selon des crit√®res donn√©s. Ce sont ensuite ces retours qui vont √™tre utilis√©s pour am√©liorer le mod√®le gr√¢ce √† l‚Äôalgorithme d‚Äôapprentissage par renforcement.\nRLHF = Reinforcement Learning from Human Feedback | RLAIF = Reinforcement Learning from Artificial Intelligence Feedback\n\nIntroduction au RLHF\n\n\n\nLe premier algorithme de Reinforcement Learning utilis√© dans le cadre des LLM √©tait la PPO (Proximal Policy Optimization). Cet algorithme classique consiste √† entra√Æner un mod√®le de r√©compense fond√© sur les retours humains, puis √† entra√Æner le LLM √† optimiser cette r√©compense. La politique du mod√®le est donc mise √† jour it√©rativement pour maximiser cette r√©compense. Le principal inconv√©nient de la PPO, que la DPO pallie, est le besoin d‚Äôentra√Æner un mod√®le de r√©compense, en plus du LLM lui-m√™me.\n\nExplication th√©orique\nImpl√©mentation HuggingFace\n\nhttps://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200\n\n\n\nL‚Äôalgorithme de DPO (Direct Preference Optimization) permet de mettre √† jour les poids du LLM en fonctions des retours humains directement, sans passer par un mod√®le de r√©compense : la politique que le LLM apprend maximise directement la satisfaction humaine. Une variation de cet algorithme est celui de KTO (Kahneman-Tversky Optimization), dont le fonctionnement g√©n√©ral reste similaire.\n\nExplication th√©orique\nGuide pratique / Impl√©mentation HugginFace\n\nLiens des papiers originaux :\n\nDPO\nKTO",
    "crumbs": [
      "II-D√©veloppements",
      "Anatomie et conception d'un mod√®le de langage"
    ]
  },
  {
    "objectID": "II-Developpements/1_Anatomie_LLM.html#anatomie-et-conception-dun-llm",
    "href": "II-Developpements/1_Anatomie_LLM.html#anatomie-et-conception-dun-llm",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Les LLMs reposent sur un d√©veloppement en deux voire trois √©tapes.\n\nLe pr√©-entra√Ænement consiste √† entra√Æner le mod√®le, en partant de z√©ro, de fa√ßon auto-supervis√©e, et sur un corpus d‚Äôentra√Ænement gigantesque. L‚Äôobjectif de ce pr√©-entra√Ænement d√©pend du type de mod√®le utilis√© (cf.¬†paragraphe suivant), mais la plupart apprennent √† pr√©dire le token suivant, √† partir d‚Äôune suite de tokens. C‚Äôest ce qui les rend particuli√®rement efficaces pour de la g√©n√©ration de texte.\nL‚Äôinstruction-tuning permet d‚Äôadapter le mod√®le pr√©-entra√Æn√© √† une plus grande diversit√© de t√¢ches. Dans de nombreux cas (chatbot, r√©sum√© de texte, etc.), la pr√©diction du token suivant n‚Äôest pas la bonne strat√©gie. L‚Äô√©tape d‚Äôinstruction-tuning permet ainsi, gr√¢ce √† un entra√Ænement supervis√©, de cr√©er une version ¬´¬†chat¬†¬ª du mod√®le. Pour donner un exemple connu de tous, ChatGPT est la version instruction-tun√©e de GPT-4.\nLe fine-tuning (optionnel) peut √™tre utilis√© pour adapter le mod√®le √† une t√¢che et √† des donn√©es sp√©cifiques. Les LLMs √©tant des outils multit√¢ches, souvent multilingues et multidomaines, leurs performances peuvent √™tre d√©grad√©es lorsqu‚Äôil y a des exigences pr√©cises et sp√©cifiques. Le fine-tuning est une nouvelle phase d‚Äôentra√Ænement supervis√©, n√©cessitant moins de donn√©es et de puissance de calcul, qui permet de sp√©cialiser le mod√®le.\n\nPar leur taille et les exigences techniques qu‚Äôils impliquent, seules quelques entreprises sp√©cialis√©es ont les moyens de pr√©-entra√Æner et d‚Äôinstruction-tuner des LLMs. Le fine-tuning, en revanche, peut √™tre abordable pour beaucoup plus d‚Äôacteurs, pour peu qu‚Äôils r√©pondent √† certaines exigences techniques (cf.¬†partie sur le fine-tuning).\nPour donner des ordres de grandeur, la petite version du dernier mod√®le de Meta, Llama-3 8B, a √©t√© pr√©-entra√Æn√© et instruction-tun√© sur un corpus de 15 trillions de tokens. Ces deux phases d‚Äôentra√Ænement ont n√©cessit√© 1,3 millions d‚Äôheures GPU, r√©parties sur plusieurs milliers de GPU H100.\n\nArticle r√©sumant la dualit√© pr√©-entra√Ænement/fine-tuning\n\n\n\n\n\nIntroduite en 2017 dans le papier Attention Is All You Need, l‚Äôarchitecture Transformer a r√©volutionn√© le domaine du TAL. Par rapport aux RNN, les Transformers permettent un traitement efficace des s√©quences en parall√®le, conduisant √† un temps de calcul beaucoup plus court (tant lors de l‚Äôentra√Ænement qu‚Äôen inf√©rence), tandis que les RNN, par construction, ne peuvent traiter une s√©quence que s√©quentiellement, c‚Äôest-√†-dire token par token. En outre, le m√©canisme d‚Äôauto-attention, pr√©sent√© ci-dessous, permet de capturer efficacement les d√©pendances distantes en att√©nuant le probl√®me de la disparition et de l‚Äôexplosion des gradients.\n\nL‚Äôauto-attention est le m√©canisme central des Transformers. Elle est utilis√©e pour pond√©rer, lors de l‚Äôexamen d‚Äôun token en particulier, l‚Äôimportance, relative √† ce token, de chaque autre token de la s√©quence. Concr√®tement, trois vecteurs (qui repr√©sentent chacun la s√©quence d‚Äôentr√©e dans un r√¥le diff√©rent) sont d√©duits de la s√©quence d‚Äôentr√©e \\(X\\) : les requ√™tes (\\(Q\\)), les cl√©s (\\(K\\)) et les valeurs (\\(V\\)), par des transformations lin√©aires comme exprim√©es dans l‚Äô√©quation suivantes. Les matrices \\(W_Q\\), \\(W_K\\) et \\(W_V\\) sont des param√®tres entra√Ænables du mod√®le. \\[Q = X \\cdot W_Q \\qquad K = X \\cdot W_K \\qquad V = X \\cdot W_V \\]\nLes scores d‚Äôattention sont ensuite calcul√©s selon l‚Äô√©quation suivante. \\[\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^{T}}{\\sqrt{d}} \\right) V\\]\nPour chaque token d‚Äôentr√©e \\(X_i\\), le r√©sultat \\(\\text{Attention}(Q, K, V)_i\\) est une combinaison de tous les autres √©l√©ments de la s√©quence, pond√©r√©s selon leur pertinence par rapport √† \\(X_i\\).\nL‚Äôauto-attention telle que pr√©sent√©e ci-dessus n‚Äôest cependant pas directement utilis√©e dans l‚Äôarchitecture Transformer. A la place, une extension, appel√©e attention multi-t√™tes, permet au mod√®le de capturer plusieurs aspects des relations et des d√©pendances entre les √©l√©ments de la s√©quence d‚Äôentr√©e. Cela est fait en transformant la s√©quence d‚Äôentr√©e en plusieurs t√™tes, i.e.¬†en plusieurs vecteurs de requ√™tes, de cl√©s et de valeurs, et en appliquant un m√©canisme d‚Äôauto-attention sur chacune de ces t√™tes. Les vecteurs d‚Äôattention de chaque t√™te sont ensuite concat√©n√©s et r√©duits lin√©airement √† la taille d‚Äôentr√©e d‚Äôorigine. Le calcul de l‚Äôattention multi-t√™tes est d√©taill√© dans l‚Äô√©quation suivante.\n\\[\\text{MultiHeadAttention}(Q, K, V) = \\text{Concat}(\\text{head}_1 , \\cdots, \\text{head}_h)W^O\\]\no√π \\(\\text{head}_i = \\text{Attention}(X \\cdot W_Q^i, X \\cdot W_K^i, X \\cdot W_V^i)\\) pour \\(i = 1, \\cdot, h\\) avec \\(h\\) le nombre de t√™tes d‚Äôattention. Chaque t√™te d‚Äôattention peut donc se sp√©cialiser dans un aspect sp√©cifique des donn√©es, et le mod√®le peut apprendre √† combiner ces diff√©rents aspects pour une meilleure repr√©sentation. La combinaison de ce m√©canisme d‚Äôattention multi-t√™tes, de couches de normalisation et de couches √† action directes (FNN) forme un bloc Transformer.\nPlusieurs blocs (6 dans l‚Äôimpl√©mentation originale) forment ensuite l‚Äôencodeur (qui a acc√®s √† la s√©quence d‚Äôentr√©e dans son int√©gralit√©) et le d√©codeur (qui a acc√®s √† la repr√©sention encod√©e de la s√©quence d‚Äôentr√©e, et √† la s√©quence de sortie g√©n√©r√©e jusqu‚Äôalors). La combinaison de ces deux √©l√©ments composent le Transformer encodeur-d√©codeur original.\n\nPapier original ‚ÄòAttention Is All You Need‚Äô\nExplication illustr√©e et tr√®s d√©taill√©e\n\n\n\n\nLes LLMs bas√©s sur des architectures Transformers appartiennent √† l‚Äôune des 3 cat√©gories suivantes¬†:\n\nMod√®le ¬´¬†encoder-only¬†¬ª¬†: Ils sont bas√©s uniquement sur la partie encodeur des Transformers. Leur pr√©-entra√Ænement est souvent bas√© sur la reconstruction de phrases¬†: √† chaque √©tape, le mod√®le a acc√®s √† une phrase enti√®re, sauf certains mots qui ont √©t√© masqu√©s, et apprend √† retrouver ces mots masqu√©s. Ces mod√®les sont adapt√©s pour des t√¢ches de classification, de reconnaissance d‚Äôentit√©s nomm√©es (NER), de r√©ponses aux questions, etc. Ils ont aujourd‚Äôhui perdu en popularit√©, mais leurs repr√©sentants les plus connus (BERT, RoBERTa, DistilBERT, CamemBERT, etc.) sont encore tr√®s utilis√©s, et restent un choix int√©ressant selon la t√¢che, gr√¢ce √† leur compr√©hension fine du langage et √† leur petite taille.\nMod√®le ¬´¬†decoder-only¬†¬ª¬†: Ils sont bas√©s uniquement sur la partie d√©codeur des Transformers. Ces mod√®les sont aujourd‚Äôhui la norme, et l‚Äôimmense majorit√© des LLMs actuels utilisent cette architecture. Leur pr√©-entra√Ænement est bas√© sur la pr√©diction du prochain token¬†: √† chaque √©tape, le mod√®le a acc√®s au d√©but d‚Äôune phrase, et apprend √† pr√©dire le token suivant. Pour cette raison, ces mod√®les sont √©galement qualifi√©s d‚Äô¬´¬†autor√©gressifs¬†¬ª. Les mod√®les GPT (2, 3, 4), Llama (2, 3), Mistral,¬†Gemini, etc. sont tous des decoder-only.\nMod√®le ¬´¬†encoder-decoder¬†¬ª¬†: Ils utilisent les deux blocs des Transformers. L‚Äôencodeur a ainsi acc√®s √† l‚Äôint√©gralit√© de la s√©quence d‚Äôentr√©e, alors que le d√©codeur a acc√®s √† la repr√©sentation cach√©e de l‚Äôentr√©e et aux tokens g√©n√©r√©s jusqu‚Äôalors. Les mod√®les les plus connus sont par exemple BART et T5.\n\nhttps://medium.com/artificial-corner/discovering-llm-structures-decoder-only-encoder-only-or-decoder-encoder-5036b0e9e88\n\n\n\nLes architectures Mixture of Experts ne sont pas sp√©cifiques aux LLMs, mais elles ont √©t√© adapt√©es avec succ√®s sur des mod√®les comme Mixtral 8x7B, Mixtral 8x22B ou GPT-4 (supposition). Le principe est de remplacer chaque r√©seau √† propagation directe (pr√©sent dans chaque bloc de l‚Äôarchitecture Transformer) par un ensemble de r√©seaux ¬´¬†experts¬†¬ª. Au moment de passer dans cette partie du r√©seau, un routeur envoie vers un de ces experts uniquement. L‚Äôint√©r√™t est double : un seul expert √©tant utilis√© √† la fois, le temps d‚Äôinf√©rence est naturellement nettement plus court. Par ailleurs, chaque r√©seau expert est entra√Æn√© et donc sp√©cialis√© diff√©rement des autres : pour un m√™me nombre de param√®tres, les performances sont donc suppos√©es √™tre meilleures qu‚Äôavec une architecture classique. En revanche, si tous les poids du mod√®les ne sont pas utilis√©s syst√©matiquement, c‚Äôest uniquement √† l‚Äôinf√©rence et √† chaque couche du r√©seau que l‚Äôexpert est choisi : il est donc tout de m√™me n√©cessaire de charger l‚Äôint√©gralit√© des poids du mod√®le en m√©moire, ce qui peut √™tre tr√®s co√ªteux en VRAM. Pour une explication plus technique, l‚Äôarticle suivant d√©taille tr√®s bien les MoE en prenant l‚Äôexemple de Mixtral.\n\nExplication d√©taill√©e des MoE (exemple de Mixtral) : https://huggingface.co/blog/moe\n\n\n\nLe principal inconv√©nient architectural des Transformers est leur complexit√© quadratique par rapport √† la taille de l‚Äôentr√©e (qui vient du calcul quadratique de l‚Äôattention). Mamba est une architecture r√©cente (D√©cembre 2023) qui s‚Äôaffranchit du m√©canisme d‚Äôattention, au profit de briques SSM (Structured State Space Models). L‚Äôint√©r√™t principal de cette architecture est sa complexit√© lin√©aire par rapport √† la taille de l‚Äôentr√©e.\nJamba est une nouvelle architecture hybride, √† mi-chemin entre le Transformer et Mamba. Cela semble permettre un niveau de performance √©lev√©, une gestion des contextes tr√®s longs, un temps d‚Äôinf√©rence nettement plus court, et des exigences m√©moires bien moindres.\nLiens des papiers originaux :\n\nMamba\nJamba\n\n\n\n\n\nLes LLM sont des r√©seaux de neurones de taille importante et font l‚Äôobjet d‚Äôentra√Ænement avec des ressources colossales (e.g: quelques dizaines de milliers de GPUs dernier mod√®le pendant 3 mois pour GPT-4). L‚Äôentra√Ænement permet d‚Äôapprendre un jeu de donn√©es particulier, en r√©glant l‚Äôensemble des poids du mod√®les (e.g: Mixtral 8x22B est une architecture √† 141 milliards de poids; 175 milliards pour GPT-3). Les LLM sont entra√Æn√©s √† r√©pondre √† plusieurs t√¢ches g√©n√©riques et ne sont pas forc√©ment pertinent pour des cas d‚Äôutilisation particulier.\nPour r√©pondre √† ce besoin, plusieurs m√©thodes relevant du principe de fine-tuning sont possibles. Le fine-tuning consiste √† reprendre un mod√®le d√©j√† entra√Æn√© et √† l‚Äôadapter sur un jeu de donn√©es particulier sur une ou plusieurs t√¢ches sp√©cifiques. En g√©n√©ral, il s‚Äôagit de modifier une partie ou l‚Äôensemble des poids pour que le mod√®le soit plus pr√©cis pour les t√¢ches voulues. Le fine-tuning garde en grande partie les b√©n√©fices de l‚Äôentra√Ænement initial, i.e les connaissances ant√©rieures d√©j√† apprises. Repartir d‚Äôun mod√®le d√©j√† entra√Æn√© pourra r√©duire le temps d‚Äôentra√Ænement requis pour le fine-tuning, en fonction de la similarit√© entre la nouvelle t√¢che souhait√©e et son jeu de donn√©es et les entra√Ænements pr√©c√©dents.\nPour des petits mod√®les de langages, il est possible de r√©-entra√Æner en modifiant l‚Äôensemble des poids. Pour des mod√®les plus grands, modifier l‚Äôensemble des poids peut s‚Äôav√©rer couteux en temps et en GPUs. Plusieurs approches permettent de r√©-entra√Æner √† moindre co√ªt :\n\nr√©entrainer seulement un sous-ensemble de poids\nmodifier la t√™te de mod√©lisation de la langue (lm_head) pour certains mod√®les, soit en r√©entrainant depuis les poids entra√Æn√©s, soit en r√©initialisant ces poids.\ngarder l‚Äôint√©gralit√© du mod√®le et rajouter des poids √† entra√Æner puis utiliser l‚Äôapproximation de bas rang avec LORA (Low-Rank Adaptation) pour l‚Äôentra√Ænement et l‚Äôinf√©rence.\nutiliser des versions quantis√©es, i.e.¬†des mod√®les o√π les poids ont √©t√© tronqu√©s √† une pr√©cision inf√©rieure (possibilit√© de combiner avec la technique pr√©c√©dente, sous le nom de qLORA).\n\n\n\n\n\n\nImpl√©mentation HuggingFace\n\n\n\n\nPEFT = Parameter-Efficient Fine-Tuning | LoRA = Low-Rank Adaptation | QLoRA = Quantized Low-Rank Adaptation | DoRA = Weight-Decomposed Low-Rank Adaptation\nR√©-entra√Æner enti√®rement un LLM est tr√®s co√ªteux en termes d‚Äôinfrastructure et de donn√©es, et n‚Äôest donc pas √† la port√©e de n‚Äôimporte quelle organisation. Des m√©thodes ¬´ efficaces ¬ª ont √©t√© cr√©√©es pour rendre le fine-tuning facilement accessible, dont la plus connue et la plus populaire est LoRA (Low-Rank Adaptation). Son fonctionnement repose sur deux √©l√©ments :\n\nL‚Äôadaptation : Les poids du mod√®le pr√©-entra√Æn√© sont gel√©s pendant l‚Äôentra√Ænement. Ce sont des poids suppl√©mentaires (ceux de l‚Äôadapteur) qui vont √™tre entra√Æn√©s. Cela permet de garder l‚Äôenti√®ret√© du mod√®le pr√©-entra√Æn√© tel quel, et de rajouter uniquement la partie sp√©cifique √† chaque t√¢che. Entre autres, il est ainsi possible, avec un seul mod√®le de base, d‚Äôh√©berger plusieurs mod√®les sp√©cialis√©s √† moindre co√ªt. Le papier LoRA Land explique d‚Äôailleurs comment faire tenir 25 versions de Mistral 7B fine-tun√©s avec LoRA sur un seul GPU A100.\nLe rang faible : Les poids additionnels peuvent √™tre choisis de beaucoup de mani√®res. Avec LoRA, certaines couches du mod√®le (les couches d‚Äôattention ou les couches lin√©aires par exemple) sont s√©lectionn√©es, et les poids de ces couches sont exprim√©s comme une multiplication de deux matrices de rangs faibles, ce qui r√©duit grandement le nombre de poids √† entra√Æner (la valeur de ce rang √©tant un hyperparam√®tre de l‚Äôentra√Ænement). En fonction de la valeur de ce rang et des couches s√©lectionn√©es, il est ainsi possible d‚Äôentra√Æner uniquement 1 ou 2 % du nombre de param√®tres global du mod√®le pr√©-entra√Æn√©, sans que cela n‚Äôaffecte trop les performances du fine-tuning.\n\n\nD‚Äôautres approches de PEFT (Parameter-Efficient Fine-Tuning) ont vu le jour, dont la plupart s‚Äôinspirent de LoRA. Parmi les plus connues, QLoRA permet d‚Äôappliquer LoRA sur des mod√®les quantifi√©s, et DoRA propose un raffinement de l‚Äôadapteur de LoRA.\n\nGuide th√©orique tr√®s clair sur le PEFT (principe, avantages, etc.) avec un focus sur LoRA\nGuide pratique / Impl√©mentation HugginFace\n\nLiens des papiers originaux : - LoRA - QLoRA - DoRA\nEntra√Ænement avec qLORA en pratique :\nEn plus de la librairie transformers et datasets, les librairies peft, bitsandbytes et trl permettent de simplifier l‚Äôentra√Ænement avec qLORA\n(inspir√© du notebook suivant )\n%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U trl\n%pip install -U sentencepiece\n%pip install -U protobuf\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,TrainingArguments\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom datasets import load_dataset\nimport torch\nfrom trl import SFTTrainer\n\nbase_model = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"Mistral-7b-instruct-teletravail\"\n\npath_to_training_file=\"Dataset_public_accords_teletravail_Dares_train.parquet\"\npath_to_test_file=\"Dataset_public_accords_teletravail_Dares_test.parquet\"\n\n\ndataset=load_dataset(\"parquet\", data_files={'train': path_to_training_file, 'test': path_to_test_file})\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n\n\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)\n\ntrainer.train()\n\ntrainer.model.save_pretrained(new_model)\n\n\n\n\nLe fine-tuning supervis√© est tr√®s efficace dans de nombreux cas, mais il pr√©sente notamment l‚Äôinconv√©nient de n√©cessiter une quantit√© importante de donn√©es. La constitution d‚Äôune base de questions-r√©ponses attendues par exemple peut se r√©veler co√ªteuse. Un autre moyen d‚Äôam√©liorer un mod√®le est d‚Äôutiliser de l‚Äôapprentissage par renforcement. La premi√®re version utilis√©e pour r√©-entra√Æner un LLM est le RLHF (Reinforcement Learning from Human Feedback), qui consiste √† r√©colter des retours d‚Äôutilisateurs humains (typiquement, entre deux r√©ponses g√©n√©r√©es par un LLM, l‚Äôutilisateur va dire laquelle il pr√©f√®re), puis √† mettre √† jour les poids du mod√®le, par un algorithme d‚Äôapprentissage par renforcement, de telle sorte que la r√©ponse pr√©f√©r√©e par l‚Äôutilisateur ait plus de chances d‚Äô√™tre g√©n√©r√©e. Cette approche s‚Äôest r√©v√©l√©e particuli√®rement effiace pour ¬´¬†aligner¬†¬ª le mod√®le aux pr√©f√©rences humaines, en termes de biais, de toxicit√©, de style, etc.\nBien que la constitution d‚Äôune base de retours humains soit moins co√ªteuse que celle d‚Äôune base de questions/r√©ponses, elle reste co√ªteuse. Une solution aujourd‚Äôhui tr√®s populaire est de remplacer ces retours humains par des retours g√©n√©r√©s artificiellement, ce qui donne une approche appel√©e RLAIF (Reinforcement Learning from Artificial Intelligence Feedback). Typiquement, un LLM plus performant (par exemple GPT-4) va √™tre utilis√© pour d√©terminer la meilleure r√©ponse entre deux ou plusieurs choix, selon des crit√®res donn√©s. Ce sont ensuite ces retours qui vont √™tre utilis√©s pour am√©liorer le mod√®le gr√¢ce √† l‚Äôalgorithme d‚Äôapprentissage par renforcement.\nRLHF = Reinforcement Learning from Human Feedback | RLAIF = Reinforcement Learning from Artificial Intelligence Feedback\n\nIntroduction au RLHF\n\n\n\nLe premier algorithme de Reinforcement Learning utilis√© dans le cadre des LLM √©tait la PPO (Proximal Policy Optimization). Cet algorithme classique consiste √† entra√Æner un mod√®le de r√©compense fond√© sur les retours humains, puis √† entra√Æner le LLM √† optimiser cette r√©compense. La politique du mod√®le est donc mise √† jour it√©rativement pour maximiser cette r√©compense. Le principal inconv√©nient de la PPO, que la DPO pallie, est le besoin d‚Äôentra√Æner un mod√®le de r√©compense, en plus du LLM lui-m√™me.\n\nExplication th√©orique\nImpl√©mentation HuggingFace\n\nhttps://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200\n\n\n\nL‚Äôalgorithme de DPO (Direct Preference Optimization) permet de mettre √† jour les poids du LLM en fonctions des retours humains directement, sans passer par un mod√®le de r√©compense : la politique que le LLM apprend maximise directement la satisfaction humaine. Une variation de cet algorithme est celui de KTO (Kahneman-Tversky Optimization), dont le fonctionnement g√©n√©ral reste similaire.\n\nExplication th√©orique\nGuide pratique / Impl√©mentation HugginFace\n\nLiens des papiers originaux :\n\nDPO\nKTO",
    "crumbs": [
      "II-D√©veloppements",
      "Anatomie et conception d'un mod√®le de langage"
    ]
  },
  {
    "objectID": "I-Accompagnement/3_Impacts.html",
    "href": "I-Accompagnement/3_Impacts.html",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "",
    "text": "Le num√©rique est responsable de 2,5% de l‚Äôempreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de d√©chets) selon l‚Äô√©tude ARCEP & ADEME de 2023. Par contre, il n‚Äôexiste aucun r√©f√©rentiel √† ce jour pour mesurer l‚Äôimpact environnemental des projets d‚Äôintelligence artificielle. √Ä titre d‚Äôexemple, les √©missions li√©es √† l‚Äôentra√Ænement de GPT-3 sont estim√©es √† 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 repr√©senterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, m√©taux et d‚Äôautres mat√©riaux pour la fabrication et op√©ration des infrastructures sont √©galement cons√©quents.\nAfin de permettre aux acteurs du num√©rique d‚Äô√©valuer l‚Äôimpact environnemental de leurs projets d‚Äôintelligence artificielle, et de communiquer sur le caract√®re frugal de ces derniers, l‚ÄôEcolab du MTECT pr√©pare avec l‚ÄôAFNOR un document de r√©f√©rence, qui devra √™tre disponible en juillet.\n√Ä l‚Äôheure actuelle, pour estimer la consommation √©nerg√©tique et les √©missions de CO2 li√©es √† l‚Äôex√©cution du code, les data-scientists peuvent utiliser la librairie CodeCarbon, √† mettre en place avant l‚Äôusage, et/ou Green Algorithms, qui peut √™tre utilis√© pour estimer un usage futur ou pass√©.\nLe co√ªt environnementale li√© aux infrastructures de calcul est mis √† disposition par le groupe EcoInfo du CNRS √† travers l‚Äôoutil EcoDiag. Des estimations plus pr√©cises pour la fabrication de GPUs seront disponibles prochainement.\n[1] https://arxiv.org/pdf/2104.10350.pdf\n[2] Data For Good - Livre Blanc de l‚ÄôIA G√©n√©rative\n\n\n\nLa s√©curit√© des donn√©es personnelles et des mod√®les est un enjeu consid√©rable, que ce soit du point de vue personnel ou √† l‚Äô√©chelle de l‚Äôadministration. Par exemple, quand les mod√®les ne sont pas auto-h√©berg√©s, les entreprises qui les fournissent ont acc√®s aux conversations tenus avec les chatbots. De plus ces donn√©es sont r√©utilis√©es pour l‚Äôentra√Ænement et peuvent ressortir lors de conversations avec d‚Äôautres utilisateurs.\nLa CNIL propose une s√©rie de recommandations concenrant le d√©veloppement de syst√®me d‚ÄôIA impliquant un traitement des donn√©es personnelles, notamment en insistant sur la d√©finition des finalit√©s du traitement et sur prise en compte de la base l√©gale du RGPD qui autorise √† traiter des donn√©es personnelles. Dans le cas d‚Äôune administration publique, cette base l√©gale pourra √™tre par exemple selon les cas l‚Äôobligation l√©gale, la mission d‚Äôint√©r√™t public ou l‚Äôint√©r√™t l√©gitime.\nAu niveau europ√©en, le r√®glement (UE) 2024/1689 du Parlement europ√©en et du Conseil du 13 juin 2024 √©tablissant des r√®gles harmonis√©es concernant l‚Äôintelligence artificielle ou ‚ÄúAI Act‚Äù est le premier acte l√©gislatif europ√©en sur l‚ÄôIA. Il √©tablit notamment des r√®gles harmonis√©es concernant la mise sur le march√©, mise en service et utilisation de syst√®mes d‚ÄôIA dans l‚ÄôUE, avec l‚Äôinterdiction de certaines pratiques, comme la notation sociale, l‚Äô√©valuation des risques de commettre des infractions ou la cr√©ation de bases de donn√©es de reconnaissance faciale non cibl√©es. Une gradation est d√©termin√©e selon le niveau de risque, avec des syst√®mes d‚ÄôIA √† faible ou moyen risque, des syst√®mes √† haut risque, associ√©s √† des exigences sp√©cifiques, par exemple lorsqu‚Äôils traitent des donn√©es personnelles, et des pratiques interdites.\nPour aller plus loin : - Guide de la CNIL - R√©sum√© haut niveau de l‚ÄôAI Act\n\n\n\nEn plus de la s√©curisation commune aux applications produites par l‚Äôadministration, certains sujets sont sp√©cifiques aux mod√®les d‚ÄôIA. L‚ÄôANSSI a √©crit √† ce sujet un guide de recommandations de s√©curit√© pour sensibiliser aux risques et promouvoir les bonnes pratiques lors de la cr√©ation et de la mise en production d‚Äôapplications comportant des mod√®les d‚ÄôIA g√©n√©rative.\nTrois cat√©gories d‚Äôattaque sp√©cifiques au syst√®me d‚ÄôIA g√©n√©rative sont identifi√©es :\n\nles attaques par manipulation, au moyen de requ√™tes malveillantes;\nles attaques par infection, en contaminant les donn√©es lors de la phase d‚Äôentra√Ænement (‚Äúmodel poisoning‚Äù);\nles attaques par exfiltration, qui visent √† obtenir des informations sur le mod√®le en production, comme les donn√©es d‚Äôentra√Ænement ou les param√®tres.\n\nLes recommandations produites concernent √† la fois les phases d‚Äôentra√Ænement, de d√©ploiement et de mise en production.\nPour aller plus loin : Guide de l‚ÄôANSSI",
    "crumbs": [
      "I-Accompagnement",
      "Impacts"
    ]
  },
  {
    "objectID": "I-Accompagnement/3_Impacts.html#impacts",
    "href": "I-Accompagnement/3_Impacts.html#impacts",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "",
    "text": "Le num√©rique est responsable de 2,5% de l‚Äôempreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de d√©chets) selon l‚Äô√©tude ARCEP & ADEME de 2023. Par contre, il n‚Äôexiste aucun r√©f√©rentiel √† ce jour pour mesurer l‚Äôimpact environnemental des projets d‚Äôintelligence artificielle. √Ä titre d‚Äôexemple, les √©missions li√©es √† l‚Äôentra√Ænement de GPT-3 sont estim√©es √† 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 repr√©senterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, m√©taux et d‚Äôautres mat√©riaux pour la fabrication et op√©ration des infrastructures sont √©galement cons√©quents.\nAfin de permettre aux acteurs du num√©rique d‚Äô√©valuer l‚Äôimpact environnemental de leurs projets d‚Äôintelligence artificielle, et de communiquer sur le caract√®re frugal de ces derniers, l‚ÄôEcolab du MTECT pr√©pare avec l‚ÄôAFNOR un document de r√©f√©rence, qui devra √™tre disponible en juillet.\n√Ä l‚Äôheure actuelle, pour estimer la consommation √©nerg√©tique et les √©missions de CO2 li√©es √† l‚Äôex√©cution du code, les data-scientists peuvent utiliser la librairie CodeCarbon, √† mettre en place avant l‚Äôusage, et/ou Green Algorithms, qui peut √™tre utilis√© pour estimer un usage futur ou pass√©.\nLe co√ªt environnementale li√© aux infrastructures de calcul est mis √† disposition par le groupe EcoInfo du CNRS √† travers l‚Äôoutil EcoDiag. Des estimations plus pr√©cises pour la fabrication de GPUs seront disponibles prochainement.\n[1] https://arxiv.org/pdf/2104.10350.pdf\n[2] Data For Good - Livre Blanc de l‚ÄôIA G√©n√©rative\n\n\n\nLa s√©curit√© des donn√©es personnelles et des mod√®les est un enjeu consid√©rable, que ce soit du point de vue personnel ou √† l‚Äô√©chelle de l‚Äôadministration. Par exemple, quand les mod√®les ne sont pas auto-h√©berg√©s, les entreprises qui les fournissent ont acc√®s aux conversations tenus avec les chatbots. De plus ces donn√©es sont r√©utilis√©es pour l‚Äôentra√Ænement et peuvent ressortir lors de conversations avec d‚Äôautres utilisateurs.\nLa CNIL propose une s√©rie de recommandations concenrant le d√©veloppement de syst√®me d‚ÄôIA impliquant un traitement des donn√©es personnelles, notamment en insistant sur la d√©finition des finalit√©s du traitement et sur prise en compte de la base l√©gale du RGPD qui autorise √† traiter des donn√©es personnelles. Dans le cas d‚Äôune administration publique, cette base l√©gale pourra √™tre par exemple selon les cas l‚Äôobligation l√©gale, la mission d‚Äôint√©r√™t public ou l‚Äôint√©r√™t l√©gitime.\nAu niveau europ√©en, le r√®glement (UE) 2024/1689 du Parlement europ√©en et du Conseil du 13 juin 2024 √©tablissant des r√®gles harmonis√©es concernant l‚Äôintelligence artificielle ou ‚ÄúAI Act‚Äù est le premier acte l√©gislatif europ√©en sur l‚ÄôIA. Il √©tablit notamment des r√®gles harmonis√©es concernant la mise sur le march√©, mise en service et utilisation de syst√®mes d‚ÄôIA dans l‚ÄôUE, avec l‚Äôinterdiction de certaines pratiques, comme la notation sociale, l‚Äô√©valuation des risques de commettre des infractions ou la cr√©ation de bases de donn√©es de reconnaissance faciale non cibl√©es. Une gradation est d√©termin√©e selon le niveau de risque, avec des syst√®mes d‚ÄôIA √† faible ou moyen risque, des syst√®mes √† haut risque, associ√©s √† des exigences sp√©cifiques, par exemple lorsqu‚Äôils traitent des donn√©es personnelles, et des pratiques interdites.\nPour aller plus loin : - Guide de la CNIL - R√©sum√© haut niveau de l‚ÄôAI Act\n\n\n\nEn plus de la s√©curisation commune aux applications produites par l‚Äôadministration, certains sujets sont sp√©cifiques aux mod√®les d‚ÄôIA. L‚ÄôANSSI a √©crit √† ce sujet un guide de recommandations de s√©curit√© pour sensibiliser aux risques et promouvoir les bonnes pratiques lors de la cr√©ation et de la mise en production d‚Äôapplications comportant des mod√®les d‚ÄôIA g√©n√©rative.\nTrois cat√©gories d‚Äôattaque sp√©cifiques au syst√®me d‚ÄôIA g√©n√©rative sont identifi√©es :\n\nles attaques par manipulation, au moyen de requ√™tes malveillantes;\nles attaques par infection, en contaminant les donn√©es lors de la phase d‚Äôentra√Ænement (‚Äúmodel poisoning‚Äù);\nles attaques par exfiltration, qui visent √† obtenir des informations sur le mod√®le en production, comme les donn√©es d‚Äôentra√Ænement ou les param√®tres.\n\nLes recommandations produites concernent √† la fois les phases d‚Äôentra√Ænement, de d√©ploiement et de mise en production.\nPour aller plus loin : Guide de l‚ÄôANSSI",
    "crumbs": [
      "I-Accompagnement",
      "Impacts"
    ]
  },
  {
    "objectID": "I-Accompagnement/1_cas_usage.html",
    "href": "I-Accompagnement/1_cas_usage.html",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "",
    "text": "Les cas d‚Äôusages des LLMs sont vari√©s et avant de se lancer et innover gr√¢ce aux LLMs, il est n√©cessaire de bien identifier le besoin qui am√®ne l‚Äôutilisation d‚Äôun LLM. Pour quoi faire ? Pour quels usages ? Est-ce pour de la g√©n√©ration de texte ? Pour de la classification ? ou pour des interactions conversationnelles ? L‚Äôobjectif de ce chapitre est d‚Äôaccompagner la r√©flexion autour de l‚Äôidentification du besoin et de la collecte des donn√©es, avec les diff√©rents types de cas d‚Äôusages impliquant des LLMs.\nAu sein des administrations, les cas d‚Äôusage de LLM ci-dessous sont en cours d‚Äôexp√©rimentation, soit en production.",
    "crumbs": [
      "I-Accompagnement",
      "Cas d'usage"
    ]
  },
  {
    "objectID": "I-Accompagnement/1_cas_usage.html#cas-dusage",
    "href": "I-Accompagnement/1_cas_usage.html#cas-dusage",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "Cas d‚Äôusage",
    "text": "Cas d‚Äôusage\nDes LLM peuvent √™tre utilis√©s pour :\n\nLabelliser / classifier les textes d‚Äôun corpus traitant d‚Äôun sujet, selon certaines cat√©gories. Des LLMS peuvent √™tre utilis√©s pour labelliser des articles de presse traitant de d√©cisions de politique mon√©taire, selon les cat√©gories ¬´ d√©cision attendue ¬ª, ¬´ d√©cision surprenante ¬ª, ¬´ ne sait pas ¬ª. Ils peuvent √©galement classer des documents de recherche clinique selon diff√©rentes th√©matiques et disciplines, tout en permettant une recherche s√©mantique avanc√©e. Ils peuvent aussi permettre de classer des accords d‚Äôentreprise, publi√©s sur L√©giFrance. Ces accords peuvent concerner plusieurs th√©matiques (t√©l√©travail, compte √©pargne temps, droit √† la deconnexion).Ces th√©matiques sont d√©clar√©s par les entreprises et sont √©ventuellement corrig√©es par la Direction G√©n√©rale du Travail.Le besoin est alors de d√©tecter automatiquement les th√©matiques √† la lecture de l‚Äôaccord. Un jeu de donn√©es est disponible √† l‚Äôadresse suivante : accords_publics_xx_to_2022_themes_et_texte.parquet\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nMinist√®re en charge de la sant√©\nSIRANo\ndgos-sirano@sante.gouv.fr\nExp√©rimentation\n\n\nBanque de France\n√âtude de l‚Äôimpact des surprises mon√©taires sur les taux de change\njean-charles.bricongne@banque-france.fr\nRecherche\n\n\nBanque de France\nAnticipation d‚Äôinflation\njean-charles.bricongne@banque-france.fr olivier.debandt@banque-france.fr  Thomas.RENAULT.external@banque-france.fr\nRecherche\n\n\nDares - Minist√®re du Travail\nAcccords d‚Äôentreprise\nTHIOUNN, Conrad (DARES) conrad.thiounn@travail.gouv.fr\n\n\n\n\n\nIdentifier les th√©matiques trait√©es dans un corpus. Par exemple, des LLMs peuvent √™tre utilis√©s pour identifier les th√©matiques d√©velopp√©es dans le champ Commentaire d‚Äôune enqu√™te.\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nBanque de France\nEnqu√™te sur les Tendances r√©gionales\nFarid.OUKACI@banque-france.fr  Olivier.LANTRAN@banque-france.fr\nExp√©rimentation\n\n\nLabIA DNUM\nLLamandement : LLM finetun√© permettant d‚Äôacc√©lerer le traitement d‚Äôamendements et projets de loi (notamment via la synth√©tisation des textes).\nFarid.OUKACI@banque-france.fr  Olivier.LANTRAN@banque-france.fr\nExp√©rimentation\n\n\n\n\nFaire une analyse de sentiment d‚Äôun corpus traitant d‚Äôune th√©matique. Par exemple, des LLMs peuvent √™tre utilis√©s pour faire une analyse de sentiment (ex : positif, n√©gatif ou neutre) d‚Äôune th√©matique √©mergeant d‚Äôun champ ¬´ Commentaire ¬ª d‚Äôune enqu√™te et traitant d‚Äôune perception du climat des affaires.\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nBanque de France\nEnqu√™te sur les Tendances r√©gionales\nFarid.OUKACI@banque-france.fr  Olivier.LANTRAN@banque-france.fr\nExp√©rimentation\n\n\n\n\nInterroger une base de documents textuels (pdf, code, etc‚Ä¶) (retrieval augmented generation). Les documents sont d√©coup√©s en paragraphes (chunks). Les r√©ponses aux questions pos√©es sont g√©n√©r√©es sur la base de paragraphes idoines existant dans la base. Les paragraphes qui ont servi √† l‚Äô√©laboration de la r√©ponse sont indiqu√©s en regard de celle-ci, et peuvent √™tre consult√©s.\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nBanque de France\nChatbdf\nNicolas.THOMAZO@banque-france.fr  Guillaume.LOMBARDO@banque-france.fr  Alix.DECREMOUX@banque-france.fr\nPassage en production pr√©vu en d√©cembre 2025\n\n\nANFSI\nIAccueil\n daphne.pertsekos@gendarmerie.interieur.gouv.fr  jerome.laporte@gendarmerie.interieur.gouv.fr  jean-baptiste.delfau@gendarmerie.interieur.gouv.fr  malo.adler@gendarmerie.interieur.gouv.fr\nExp√©rimentation depuis Octobre 2024\n\n\n\n\nRequ√™ter sur des bases de donn√©es cod√©es en SQL : √† une interrogation exprim√©e en langage naturel sur une base en SQL, un code en SQL servant √† la requ√™te est renvoy√©. Par exemple, √† l‚Äôinterrogation ¬´ trouve-moi la date de naissance de l‚Äôindividu I ¬ª, un code SQL est renvoy√© permettant d‚Äôeffectuer la requ√™te\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nBanque de France\nText2SQL\nGuillaume.LOMBARDO@banque-france.fr\nPassage en production par la BCE en d√©cembre 2024\n\n\n\n\nExtraire des donn√©es √† partir de documents textuels Par exemple, √† partir de documents r√©glementaires extraire 15 informations-cl√©s et stocker celles-ci dans une base de donn√©es\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nBanque de France\nVeridic\nGuillaume.LOMBARDO@banque-france.fr\nPassage en production pr√©vu fin 2025\n\n\n\n\nEffectuer des synth√®ses de documents textuels\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nANFSI\nSynth√®se de proc√©dures judiciaires\nmalo.adler@gendarmerie.interieur.gouv.fr\nRecherche\n\n\n\n\nAider √† v√©rifier la conformit√© l√©gale de proc√©dures administratives\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\nANFSI\nConformit√© des proc√©dures\nmalo.adler@gendarmerie.interieur.gouv.fr\nRecherche\n\n\n\n\nAgent conversationnel\n\n\n\n\n\n\n\n\n\n\nInstitution\nNom du Projet\nContact\nExp√©rimentation/Production/Recherche\n\n\n\n\n\n\n\n\n\n\n\nProjet men√© par le LabIA de la DINUM - Albert github : Outils de d√©ploiements des mod√®les Albert - Mod√®les Albert - Albert France Services : Projet √† destination de France Service et visant √† appuyer ses conseillers dans la r√©alisation de leurs missions. Ce projet se base principalement Albert github Albert hugging face\n\nPour plus de projets IA (au sens large) dans l‚Äôadministration se r√©f√©rer au lien : https://grist.numerique.gouv.fr/o/beta-gouv-ia/9wTgwEbwqmwW/Ressources/p/1",
    "crumbs": [
      "I-Accompagnement",
      "Cas d'usage"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "Ce projet fait partie de la troisi√®me saison du Programme 10%, co-port√© par la DINUM en association √©troite avec l‚ÄôInsee.\n\n\nMistral, GPT4, Claude, ALBERT, CroissantLLM‚Ä¶ Vous ne vous y retrouvez plus parmi tous ces mod√®les larges de language (LLM)? Ce guide permet de se poser les bonnes questions et de pouvoir r√©pondre √† un besoin d‚Äôanalyse textuelle avec une solution adapt√©e. Ce guide est l√† pour vous aider du d√©but jusqu‚Äô√† la fin, de la conception du mod√®le apr√®s probl√©matisation √† la mise en production.\nObjectif : Comprendre son besoin et y r√©pondre avec une solution LLM adapt√©e\n\n\n\nLes LLM sont des mod√®les de langage de grande taille, apparus vers 2018, ayant r√©volutionn√© le domaine du traitement automatique du langage naturel. Les LLMS se caract√©risent par une architecture de type ‚Äútransformers‚Äù permettant un traitement parall√®le et contextuel du langage, un entra√Ænement sur de tr√®s vastes corpus de donn√©es textuelles, allant de millions √† des milliards de param√®tres, et des capacit√©s impressionnantes pour accomplir une grande vari√©t√© de t√¢ches linguistiques comme la g√©n√©ration de texte, la traduction, l‚Äôanalyse de sentiment.\nPlus de 3000 LLM ont √©t√© entra√Æn√©s et publi√©s sur des plateformes de partage de mod√®le comme HuggingFace, dont plus d‚Äôune cinquaine s‚Äôaffronte r√©guli√®rement entre eux pour se d√©marquer sur diff√©rentes t√¢ches dans une ar√®ne d√©di√©e.\nIl est aujourd‚Äôhui difficile de savoir quel LLM est adapt√© pour son besoin sp√©cifique et quelle infrastructure est n√©cessaire pour utiliser les diff√©rents LLM. Ce guide tente donc d‚Äôexpliciter les raisonnements, les questions √† se poser et des pistes de r√©ponse quant aux choix et √† la mise en oeuvre des LLM.\n\n\n\n\nl‚Äôinfrastructure minimale pour faire tourner un LLM\nbenchmarking des ‚Äúprincipaux‚Äù LLM\nexemple d‚Äôutilisation dans un cas simple (cas d‚Äôutilisation dit ‚Äúfil rouge‚Äù) sous forme de tutoriel\nfinetuner un mod√®le LLM\nquantizer un mod√®le LLM\nEvaluer un LLM\nmettre en production un mod√®le LLM\nune bibliographie consise et non exhaustive\nd‚Äôautres exemples plus complexes de cas d‚Äôutilisation dans l‚Äôadministration\nune approximation du co√ªt environnemental et financier des diff√©rents LLM\n\n\n\n\n\nles fondements th√©oriques de l‚Äôoptimisation\ncas sp√©cifique d‚Äôune administration\ncomment d√©biaiser un LLM\n\n\n\n\n\n\n\n\n\n\n\nMembre\nAdministration\n\n\n\n\nConrad THIOUNN\nDares - Minist√®re du Travail\n\n\nJohnny PLATON\nSant√© publique France\n\n\nThibault DUROUCHOUX\nDGDDI - Minist√®re de l‚ÄôEconomie et des Finances\n\n\nKatia JODOGNE-DEL LITTO\nIGF - Minist√®re de l‚ÄôEconomie et des Finances\n\n\nFaheem BEG\n\n\n\nCamille ANDR√â\n\n\n\nCamille BRIER\nDTNum - DGFiP\n\n\nDaphn√© PERTSEKOS\nANFSI - Minist√®re de l‚Äôint√©rieur\n\n\nZhanna SANTYBAYEVA\nDGOS - Minist√®re du travail, de la sant√© et des solidarit√©s\n\n\nBruno LENZI\nEcolab - Minist√®re de la Transiton √âcologique et de la Coh√©sion des Territoires\n\n\nH√©l√®ne CHARASSON-JASSON\nBanque de France\n\n\n\n\n\n\nUne remarque ? Une question ? Vous pouvez nous contacter sur le salon Tchap du Programme 10% ou ‚Ä¶",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#pr√©ambule",
    "href": "index.html#pr√©ambule",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "Mistral, GPT4, Claude, ALBERT, CroissantLLM‚Ä¶ Vous ne vous y retrouvez plus parmi tous ces mod√®les larges de language (LLM)? Ce guide permet de se poser les bonnes questions et de pouvoir r√©pondre √† un besoin d‚Äôanalyse textuelle avec une solution adapt√©e. Ce guide est l√† pour vous aider du d√©but jusqu‚Äô√† la fin, de la conception du mod√®le apr√®s probl√©matisation √† la mise en production.\nObjectif : Comprendre son besoin et y r√©pondre avec une solution LLM adapt√©e",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#etats-des-lieux-des-llm",
    "href": "index.html#etats-des-lieux-des-llm",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "Les LLM sont des mod√®les de langage de grande taille, apparus vers 2018, ayant r√©volutionn√© le domaine du traitement automatique du langage naturel. Les LLMS se caract√©risent par une architecture de type ‚Äútransformers‚Äù permettant un traitement parall√®le et contextuel du langage, un entra√Ænement sur de tr√®s vastes corpus de donn√©es textuelles, allant de millions √† des milliards de param√®tres, et des capacit√©s impressionnantes pour accomplir une grande vari√©t√© de t√¢ches linguistiques comme la g√©n√©ration de texte, la traduction, l‚Äôanalyse de sentiment.\nPlus de 3000 LLM ont √©t√© entra√Æn√©s et publi√©s sur des plateformes de partage de mod√®le comme HuggingFace, dont plus d‚Äôune cinquaine s‚Äôaffronte r√©guli√®rement entre eux pour se d√©marquer sur diff√©rentes t√¢ches dans une ar√®ne d√©di√©e.\nIl est aujourd‚Äôhui difficile de savoir quel LLM est adapt√© pour son besoin sp√©cifique et quelle infrastructure est n√©cessaire pour utiliser les diff√©rents LLM. Ce guide tente donc d‚Äôexpliciter les raisonnements, les questions √† se poser et des pistes de r√©ponse quant aux choix et √† la mise en oeuvre des LLM.",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#ce-que-couvre-ce-guide",
    "href": "index.html#ce-que-couvre-ce-guide",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "l‚Äôinfrastructure minimale pour faire tourner un LLM\nbenchmarking des ‚Äúprincipaux‚Äù LLM\nexemple d‚Äôutilisation dans un cas simple (cas d‚Äôutilisation dit ‚Äúfil rouge‚Äù) sous forme de tutoriel\nfinetuner un mod√®le LLM\nquantizer un mod√®le LLM\nEvaluer un LLM\nmettre en production un mod√®le LLM\nune bibliographie consise et non exhaustive\nd‚Äôautres exemples plus complexes de cas d‚Äôutilisation dans l‚Äôadministration\nune approximation du co√ªt environnemental et financier des diff√©rents LLM",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#ce-que-ne-couvre-pas-encore-ce-guide",
    "href": "index.html#ce-que-ne-couvre-pas-encore-ce-guide",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "les fondements th√©oriques de l‚Äôoptimisation\ncas sp√©cifique d‚Äôune administration\ncomment d√©biaiser un LLM",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#membres",
    "href": "index.html#membres",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "Membre\nAdministration\n\n\n\n\nConrad THIOUNN\nDares - Minist√®re du Travail\n\n\nJohnny PLATON\nSant√© publique France\n\n\nThibault DUROUCHOUX\nDGDDI - Minist√®re de l‚ÄôEconomie et des Finances\n\n\nKatia JODOGNE-DEL LITTO\nIGF - Minist√®re de l‚ÄôEconomie et des Finances\n\n\nFaheem BEG\n\n\n\nCamille ANDR√â\n\n\n\nCamille BRIER\nDTNum - DGFiP\n\n\nDaphn√© PERTSEKOS\nANFSI - Minist√®re de l‚Äôint√©rieur\n\n\nZhanna SANTYBAYEVA\nDGOS - Minist√®re du travail, de la sant√© et des solidarit√©s\n\n\nBruno LENZI\nEcolab - Minist√®re de la Transiton √âcologique et de la Coh√©sion des Territoires\n\n\nH√©l√®ne CHARASSON-JASSON\nBanque de France",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "index.html#√©pilogue",
    "href": "index.html#√©pilogue",
    "title": "Projet KALLM : Kit avanc√© LLM",
    "section": "",
    "text": "Une remarque ? Une question ? Vous pouvez nous contacter sur le salon Tchap du Programme 10% ou ‚Ä¶",
    "crumbs": [
      "Accueil"
    ]
  },
  {
    "objectID": "I-Accompagnement/0_Intro.html",
    "href": "I-Accompagnement/0_Intro.html",
    "title": "Guide d'installation des LLM",
    "section": "",
    "text": "Les Large Language Models sont des algorithmes d‚Äôintelligence artificielle con√ßus pour exploiter des documents non structur√©s (corpus de textes). Ils permettent d‚Äôen extraire des informations utiles ou de g√©n√©rer de nouvelles informations √† partir de cette base documentaires (par exemple : r√©pondre √† des questions, r√©sumer un texte, traduire, etc.).\n\n\nDans ce contexte, les documents forment les observations statistiques consid√©r√©es (√©quivalent aux ¬´ individus ¬ª en analyse de donn√©es) et leur ensemble forme un corpus (√©quivalent √† une ¬´ population ¬ª). Dans certains cas, les documents sont d√©coup√©s en paragraphes qui forment les observations statistiques. Les mots ou les cha√Ænes de caract√®res extra√Æts des documents jouent le r√¥le des variables.\nPour analyser un corpus, chaque document est repr√©sent√© sous forme d‚Äôun vecteur et le corpus entier sous forme d‚Äôune matrice, o√π les lignes correspondent aux et les colonnes repr√©sentent les mots ou les cha√Ænes de caract√®res.\n\n\n\nLes matrices en r√©sultantes sont potentiellement d‚Äôune tr√®s grande dimension (nombre de mots/cha√Ænes de caract√®res utilis√©s dans le corpus en colonnes), et en m√™me temps creuses (les mots/cha√Ænes de caract√®res employ√©s dans le corpus peuvent √™tre utilis√©s uniquement dans quelques documents du corpus).\n\n\n\nApr√®s l‚Äôimportation d‚Äôun corpus de textes, la premi√®re √©tape consiste en une phase de pr√©traitement visant √† r√©duire la dimension de cette matrice et √† en am√©liorer la pertinence. Cela inclut : - nettoyer les donn√©es : supprimer la ponctuation, mots usuels n‚Äôapportant pas d‚Äôinformation, etc.) - lemmatiser ou raciniser : simplifier des mots en gardant que leur racine commune (par example : garder exclusivement ¬´ finan ¬ª pour les mots ¬´ financer ¬ª, ¬´ financier ¬ª, ¬´ financement ¬ª, ‚Ä¶), - utiliser des techniques comme l‚Äôanalyse en composantes principales (ACP) ou Term Frequency-Inverse Document Frequency (TF-IDF)\n\n\n\nL‚Äôutilisation d‚Äôoutils de machine learning sur la matrice de dimension plus r√©duite ainsi obtenue permet - de comparer les documents pour analyser la similarit√© ou la distance entre eux - d‚Äôidentifier des th√®mes abord√©s dans le corpus - de classer et cat√©goriser les documents en fonction de th√©matiques - de filtrer les contenus ou de produire des statistiques pour comprendre la r√©partition des sujets dans l‚Äôensemble des textes.\nAinsi, les Large Language Models nous permettent de traiter, d‚Äôinterpr√©ter et de valoriser les donn√©es textuelles de mani√®re automatis√©e et √† grande √©chelle.",
    "crumbs": [
      "I-Accompagnement",
      "Introduction"
    ]
  },
  {
    "objectID": "I-Accompagnement/0_Intro.html#introduction-aux-large-language-models-llm",
    "href": "I-Accompagnement/0_Intro.html#introduction-aux-large-language-models-llm",
    "title": "Guide d'installation des LLM",
    "section": "",
    "text": "Les Large Language Models sont des algorithmes d‚Äôintelligence artificielle con√ßus pour exploiter des documents non structur√©s (corpus de textes). Ils permettent d‚Äôen extraire des informations utiles ou de g√©n√©rer de nouvelles informations √† partir de cette base documentaires (par exemple : r√©pondre √† des questions, r√©sumer un texte, traduire, etc.).\n\n\nDans ce contexte, les documents forment les observations statistiques consid√©r√©es (√©quivalent aux ¬´ individus ¬ª en analyse de donn√©es) et leur ensemble forme un corpus (√©quivalent √† une ¬´ population ¬ª). Dans certains cas, les documents sont d√©coup√©s en paragraphes qui forment les observations statistiques. Les mots ou les cha√Ænes de caract√®res extra√Æts des documents jouent le r√¥le des variables.\nPour analyser un corpus, chaque document est repr√©sent√© sous forme d‚Äôun vecteur et le corpus entier sous forme d‚Äôune matrice, o√π les lignes correspondent aux et les colonnes repr√©sentent les mots ou les cha√Ænes de caract√®res.\n\n\n\nLes matrices en r√©sultantes sont potentiellement d‚Äôune tr√®s grande dimension (nombre de mots/cha√Ænes de caract√®res utilis√©s dans le corpus en colonnes), et en m√™me temps creuses (les mots/cha√Ænes de caract√®res employ√©s dans le corpus peuvent √™tre utilis√©s uniquement dans quelques documents du corpus).\n\n\n\nApr√®s l‚Äôimportation d‚Äôun corpus de textes, la premi√®re √©tape consiste en une phase de pr√©traitement visant √† r√©duire la dimension de cette matrice et √† en am√©liorer la pertinence. Cela inclut : - nettoyer les donn√©es : supprimer la ponctuation, mots usuels n‚Äôapportant pas d‚Äôinformation, etc.) - lemmatiser ou raciniser : simplifier des mots en gardant que leur racine commune (par example : garder exclusivement ¬´ finan ¬ª pour les mots ¬´ financer ¬ª, ¬´ financier ¬ª, ¬´ financement ¬ª, ‚Ä¶), - utiliser des techniques comme l‚Äôanalyse en composantes principales (ACP) ou Term Frequency-Inverse Document Frequency (TF-IDF)\n\n\n\nL‚Äôutilisation d‚Äôoutils de machine learning sur la matrice de dimension plus r√©duite ainsi obtenue permet - de comparer les documents pour analyser la similarit√© ou la distance entre eux - d‚Äôidentifier des th√®mes abord√©s dans le corpus - de classer et cat√©goriser les documents en fonction de th√©matiques - de filtrer les contenus ou de produire des statistiques pour comprendre la r√©partition des sujets dans l‚Äôensemble des textes.\nAinsi, les Large Language Models nous permettent de traiter, d‚Äôinterpr√©ter et de valoriser les donn√©es textuelles de mani√®re automatis√©e et √† grande √©chelle.",
    "crumbs": [
      "I-Accompagnement",
      "Introduction"
    ]
  },
  {
    "objectID": "Bibliographie.html",
    "href": "Bibliographie.html",
    "title": "Bibliographie",
    "section": "",
    "text": "Qu‚Äôest ce que l‚ÄôIA ?\nWhat We Learned from a Year of Building with LLMs (Part I)\nWhat We Learned from a Year of Building with LLMs (Part II)\n\n\n\n\n\nARCEP & ADEME de 2023\nAFNOR IA frugale\nGuide de recommandation de s√©curit√© de l‚ÄôANSSI\nAI Act",
    "crumbs": [
      "Bibliographie"
    ]
  },
  {
    "objectID": "Bibliographie.html#i---accompagnement",
    "href": "Bibliographie.html#i---accompagnement",
    "title": "Bibliographie",
    "section": "",
    "text": "Qu‚Äôest ce que l‚ÄôIA ?\nWhat We Learned from a Year of Building with LLMs (Part I)\nWhat We Learned from a Year of Building with LLMs (Part II)\n\n\n\n\n\nARCEP & ADEME de 2023\nAFNOR IA frugale\nGuide de recommandation de s√©curit√© de l‚ÄôANSSI\nAI Act",
    "crumbs": [
      "Bibliographie"
    ]
  },
  {
    "objectID": "Bibliographie.html#ii---d√©veloppement",
    "href": "Bibliographie.html#ii---d√©veloppement",
    "title": "Bibliographie",
    "section": "II - D√©veloppement",
    "text": "II - D√©veloppement\n\nPlateforme de partage de mod√®les\n\nHuggingFace\n\n\n\nArticles de recherche centraux\nTransformers\n\nPapier original ‚ÄòAttention Is All You Need‚Äô\nExplication illustr√©e et tr√®s d√©taill√©e\nLes diff√©rents types de mod√®les\nLes Mixture of Experts\n\nFine-tuning\n\nLoRA\nQLoRA\nDoRA\nIntroduction au RLHF\nDPO\nKTO\n\nBonnes pratiques du prompt engineering\n\nPrincipled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4\nGraph of Thoughts\n\nEvaluation (m√©triques)\n\n\n\nBas√©e sur embeddings\nBas√©e sur mod√®le fine-tun√©\nBas√© sur LLM\n\n\n\n\nBERTScore\nUniEval\nG-Eval\n\n\nMoverScore\nLynx\nGPTScore\n\n\n\nPrometheus-eval\n\n\n\n\nEvaluation (frameworks) - Ragas (sp√©cialis√© pour le RAG) - Ares (sp√©cialis√© pour le RAG) - Giskard - DeepEval\nEvaluation (RAG) - Evaluation of Retrieval-Augmented Generation: A Survey - Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation\nEvaluation (divers) - Prompting strategies for LLM-based metrics - LLM-based NLG Evaluation: Current Status and Challenges - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\n\nLibrairies et ressources\nLLM platform - Ollama\nPipelines et orchestration LLM - LangChain - LlamaIndex - Haystack\nRAG - Graph RAG\nEvaluation - SelfCheckGPT",
    "crumbs": [
      "Bibliographie"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Acculturation.html",
    "href": "I-Accompagnement/2_Acculturation.html",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "",
    "text": "L‚ÄôIntelligence Artificielle offre une multitude de solutions √† diverses besoins des administrations, recens√©es dans la partie pr√©c√©dente. Dans un premier temps, il est essentiel de ma√Ætriser a minima les outils, techniques et technologies employ√©s pour r√©pondre √† ces besoins. Egalement, d√©mystifier l‚ÄôIA est tout aussi central dans la r√©ussite des projets IA, pour r√©ajuster les attentes des parties prenantes. D√®s lors, se pose la question de comment communiquer de mani√®re efficace autour de l‚ÄôIA ? Cette question prend davantage sens avec des acteurs moins techniques et d√©cisionnaires strat√©giques.\n\n\nPour communiquer sur l‚ÄôIA de mani√®re efficace avec des non-initi√©s, il est n√©cessaire de :\n\nD√©finir les termes (IA, LLM)\nD√©finir les concepts et les m√©canismes\nD√©finir ce que les mod√®les font et ne font pas\nD‚Äôavertir sur les hallucinations dans le cas des LLM et d‚Äôen √©valuer la performance\nD‚Äôavertir sur le co√ªt complet d‚Äôune solution IA\n\nPour mieux embarquer les non-initi√©s, quelques administrations ont mis en place des Lunch-and-Learn autour de l‚ÄôIA g√©n√©rative, que ce soit sous forme de pr√©sentation ou sous forme de quiz.\n\n\n\nPour mieux comprendre l‚ÄôIA ou faire comprendre aupr√®s des parties prenantes, les r√©seaux autour de l‚ÄôIA repr√©sentent une opportunit√©. Par exemple, le r√©seau du programme 10% et le r√©seau du SSPHub mettent en relation des experts autour de la data et de l‚ÄôIA, permettent d‚Äô√©changer sur des sujets techniques mais aussi sont ouverts aux non-initi√©s. Des √©v√©nements pour un public plus large que les experts sont organis√©s chaque ann√©e comme les journ√©es 10% ou la journ√©e du SSPHub",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Acculturation.html#acculturation-√†-lintelligence-artificielle",
    "href": "I-Accompagnement/2_Acculturation.html#acculturation-√†-lintelligence-artificielle",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "",
    "text": "L‚ÄôIntelligence Artificielle offre une multitude de solutions √† diverses besoins des administrations, recens√©es dans la partie pr√©c√©dente. Dans un premier temps, il est essentiel de ma√Ætriser a minima les outils, techniques et technologies employ√©s pour r√©pondre √† ces besoins. Egalement, d√©mystifier l‚ÄôIA est tout aussi central dans la r√©ussite des projets IA, pour r√©ajuster les attentes des parties prenantes. D√®s lors, se pose la question de comment communiquer de mani√®re efficace autour de l‚ÄôIA ? Cette question prend davantage sens avec des acteurs moins techniques et d√©cisionnaires strat√©giques.\n\n\nPour communiquer sur l‚ÄôIA de mani√®re efficace avec des non-initi√©s, il est n√©cessaire de :\n\nD√©finir les termes (IA, LLM)\nD√©finir les concepts et les m√©canismes\nD√©finir ce que les mod√®les font et ne font pas\nD‚Äôavertir sur les hallucinations dans le cas des LLM et d‚Äôen √©valuer la performance\nD‚Äôavertir sur le co√ªt complet d‚Äôune solution IA\n\nPour mieux embarquer les non-initi√©s, quelques administrations ont mis en place des Lunch-and-Learn autour de l‚ÄôIA g√©n√©rative, que ce soit sous forme de pr√©sentation ou sous forme de quiz.\n\n\n\nPour mieux comprendre l‚ÄôIA ou faire comprendre aupr√®s des parties prenantes, les r√©seaux autour de l‚ÄôIA repr√©sentent une opportunit√©. Par exemple, le r√©seau du programme 10% et le r√©seau du SSPHub mettent en relation des experts autour de la data et de l‚ÄôIA, permettent d‚Äô√©changer sur des sujets techniques mais aussi sont ouverts aux non-initi√©s. Des √©v√©nements pour un public plus large que les experts sont organis√©s chaque ann√©e comme les journ√©es 10% ou la journ√©e du SSPHub",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  },
  {
    "objectID": "I-Accompagnement/2_Acculturation.html#points-dattentions-autour-de-lia-g√©n√©rative",
    "href": "I-Accompagnement/2_Acculturation.html#points-dattentions-autour-de-lia-g√©n√©rative",
    "title": "PARTIE I. Accompagnement au changement",
    "section": "Points d‚Äôattentions autour de l‚ÄôIA g√©n√©rative",
    "text": "Points d‚Äôattentions autour de l‚ÄôIA g√©n√©rative\nIl y a quelques points suppl√©mentaires √† bien faire comprendre aupr√®s des parties prenantes quant √† l‚Äôusage d‚ÄôIA g√©n√©rative, il s‚Äôagit de :\n\nD√©mystifier les promesses faites et d‚Äôobjectiver les performances des solutions d‚ÄôIA g√©n√©rative\nBien expliquer son fonctionnement, ce qu‚Äôil g√©n√®re et comment il g√©n√®re. De mettre en exergue notamment sa nature probabiliste pour formuler une r√©ponse.\nBien d√©finir le besoin, le mod√®le utilis√©, les raisons de son choix et pourquoi une solution non IA g√©n√©rative n‚Äôest pas viable.",
    "crumbs": [
      "I-Accompagnement",
      "Acculturation"
    ]
  },
  {
    "objectID": "II-Developpements/2_Utilisation_LLM.html",
    "href": "II-Developpements/2_Utilisation_LLM.html",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Il faut avant tout garder √† l‚Äôesprit que le prompt engineering est une discipline tr√®s empirique, qui demande beaucoup d‚Äôit√©rations pour obtenir le meilleur prompt par rapport au r√©sultat souhait√©. Bien qu‚Äôil n‚Äôexiste pas de m√©thode syst√©matique et efficace pour optimiser un prompt, certaines pratiques sont devenues la norme. Par exemple, voici quelques bonnes pratiques :\n\nDonner un r√¥le au mod√®le : Par exemple, dire au mod√®le qu‚Äôil est un magistrat honn√™te et impartial pourra l‚Äôaider √† g√©n√©rer du texte formel, neutre et juridique. Le r√¥le est bien s√ªr √† adapter en fonction des exigences de chaque t√¢che.\nStructurer le prompt : Il est important de bien diff√©rencier le prompt syst√®me du prompt utilisateur. Le premier donnera des instructions g√©n√©rales quant au style, √† la t√¢che, au contexte, etc., alors que le second pourra donner des instructions sp√©cifiques ou un texte √† analyser. Il est √©galement pertinent d‚Äôorganiser ou de s√©parer clairement les instructions.\nEtre le plus pr√©cis possible : Rajouter le plus de d√©tails possibles, voire se r√©p√©ter dans les instructions en changeant de formulation permet de bien insister sur les √©l√©ments les plus importants.\nContraindre le mod√®le au maximum : Si l‚Äôon souhaite un format de sortie pr√©cis (JSON par exemple), donner un exemple concret de sortie attendue, et insister sur le besoin de se conformer √† ce format.\nDonner des exemples : Cf. paragraphe suivant (few-shot prompting).\n\nLe papier Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 donne un certains nombre de principes pour am√©liorer les prompts. Parmi ces principes (tr√®s nombreux), on trouve par exemple :\n\nNe pas etre poli avec le LLM si l‚Äôon souhaite une r√©ponse concise.\nD√©crire l‚Äôaudience souhait√©e dans le prompt (des experts techniques, des enfants, etc.).\nUtiliser des directives affirmatives (fais ceci), et √©viter les tournures n√©gatives (ne fais pas cela).\nEmployer des phrases telles que ‚ÄòTa tache est de‚Äô ou ‚ÄòTu DOIS‚Äô.\nR√©p√©ter plusieurs fois certains mots ou phrases essentielles.\n\n\n\n\nLa fa√ßon la plus intuitive d‚Äôadresser une requ√™te √† un LLM est de formuler des instructions les plus pr√©cises possibles. Ce faisant, on esp√®re que le mod√®le comprendra ces instructions et r√©pondra en cons√©quence. Pour des t√¢ches nouvelles, auxquelles le mod√®le n‚Äôa pas n√©cessairement √©t√© confront√© durant son (pr√©)-entra√Ænement, on appelle cette m√©thode du 0-shot prompting : le mod√®le n‚Äôa pas de r√©f√©rence ou d‚Äôexemple de r√©ponse attendue.\nPour pallier ce manque de r√©f√©rence, il est possible (et, en fonction de la t√¢che, souvent recommand√©) d‚Äôajouter des exemples de paires entr√©e/sortie dans le prompt que l‚Äôon adresse au mod√®le : cela donne du 1-shot (un exemple) ou du few-shot (plusieurs exemples) prompting. Plus les exemples sont proches de la requ√™te initiale, plus le mod√®le saura pr√©cis√©ment comment r√©pondre. Cela permet ainsi au mod√®le de s‚Äôadapter, √† moindre co√ªt, √† une t√¢che tr√®s sp√©cifique ou particuli√®re.\n\nGuide pratique (avec exemples)\n\n\n\n\nSur certaines t√¢ches qui demandent un raisonnement (par exemple la r√©solution d‚Äôun probl√®me math√©matique simple), les LLM naturellement ne sont pas tr√®s bons. Pour augmenter leurs capacit√©s de raisonnement, une strat√©gie classique consiste √† leur demander de raisonner et de r√©fl√©chir √©tape par √©tape.\nLes mod√®les les plus r√©cents ayant nettement progress√© en raisonnement, il est possible qu‚Äôils raisonnent naturellement √©tape par √©tape sur des questions simples. Pour des questions ou des raisonnements plus complexes, il sera cependant probablement plus efficace de proposer une logique de raisonnement au mod√®le, en explicitant les diff√©rentes √©tapes.\nIl est √©galement possible de combiner le CoT reasoning avec du few-shot prompting, i.e. de donner des exemples de raisonnement √©tape par √©tape au mod√®le.\n\nGuide d√©taill√©\n\n\n\n\nUne fa√ßon de travailler ses prompts est de profiter des capacit√©s g√©n√©ratives des LLMs pour leur faire cr√©er des prompts. L‚Äôid√©e est de donner au LLM un exemple de sortie souhait√©e, et de lui demander de g√©n√©rer le prompt le plus adapt√© possible pour produire cette sortie.\n\nGuide pratique\n\n\n\n\n\n\n\nLa premi√®re question √† se poser est la n√©cessit√© ou non d‚Äôutiliser un LLM. Certaines t√¢ches peuvent se r√©soudre avec un LLM, mais ce n‚Äôest pas toujours la solution la plus pertinente. Par exemple, un LLM est normalement capable de parser un fichier xml sans probl√®me, mais un script na√Øf sera largement aussi efficace, √† bien moindre co√ªt (environnemental, humain, financier). L‚Äôutilisation d‚Äôun LLM doit venir d‚Äôun besoin de compr√©hension fine du langage naturel.\n\n\n\nBeaucoup d‚Äô√©l√©ments sont √† prendre en compte lors du choix du mod√®le √† utiliser. Parmi les plus importants¬†:\n\nSa taille : Exprim√©e g√©n√©ralement en milliards (B) de param√®tres (Llama-3 8B poss√®de 8 milliards de param√®tres, Mistral 7B en poss√®de 7 milliards, etc.), elle influe fortement sur les performances du mod√®les et les exigences techniques. Un ¬´¬†petit¬†¬ª LLM de 8 milliards de param√®tres pourra tourner sur un GPU modeste avec une VRAM de 32 GB (voire moins si l‚Äôon utilise un mod√®le quantifi√©, cf.¬†‚Ä¶), tandis qu‚Äôun LLM de taille moyenne de 70 milliards de param√®tres n√©cessitera 2 GPU puissants avec 80 GB de VRAM.\nSon multilinguisme¬†: La plupart des mod√®les sont entra√Æn√©s sur une immense majorit√© de donn√©es anglaises (plus de 90¬†% pour Llama-2, contre moins de 0,1¬†% de donn√©es fran√ßaises). Les mod√®les incluant plus de fran√ßais (Mistral ?) dans leurs donn√©es d‚Äôentra√Ænement sont naturellement plus efficaces sur du fran√ßais.\nSon temps d‚Äôinf√©rence¬†: G√©n√©ralement directement li√© √† la taille du mod√®le, certaines architectures (MoE) permettent cependant d‚Äôavoir un temps d‚Äôinf√©rence plus court.\nSes performances g√©n√©rales¬†: Beaucoup de benchmarks publics √©valuent les LLMs sur des t√¢ches g√©n√©ralistes et vari√©es. Un bon point de d√©part est de regarder le Leaderboard qui recense la plupart des mod√®les connus.\nSes performances sp√©cifiques¬†: Les benchmarks g√©n√©ralistes ne sont pas forc√©ment pertinents pour certains cas d‚Äôusages, car ils ne sont pas sp√©cifiques √† la t√¢che, aux donn√©es, etc. Il peut √™tre int√©ressant de d√©velopper un pipeline d‚Äô√©valuation sp√©cifique (cf‚Ä¶).\n\nEn juin 2024, un bon point de d√©part est de regarder les mod√®les open-source de Meta (Llama-2 7B/13B/70B, Llama-3 8B/70B) et de Mistral AI (Mistral 7B, Mixtral 8x7B).\n\n\n\nSi vous √™tes dans l‚Äôun des cas suivants, le prompt engineering peut √™tre une bonne option :\n\nPas beaucoup de ressources disponibles\nBesoin d‚Äôun outil laiss√© √† la disposition des utilisateurs, avec une grande libert√©\nLes r√©ponses requises sont tr√®s formatt√©es ou tr√®s sp√©cifiques\n\n\n\n\nSi vous √™tes dans l‚Äôun des cas suivants, la RAG peut √™tre une bonne option :\n\nBesoin de r√©ponses √† jour, r√©guli√®rement et facilement actualis√©es\nBesoin de sourcer les r√©ponses ou de diminuer les hallucinations\nBesoin d‚Äôenrichir les r√©ponses avec des donn√©es sp√©cifiques\nBesoin d‚Äôune application qui ne d√©pend pas d‚Äôun mod√®le sp√©cifique (g√©n√©ralisabilit√©), et dont les utilisateurs ne connaissent pas l‚ÄôIA g√©n√©rative\n\n\n\n\nSi vous √™tes dans l‚Äôun des cas suivants, le fine-tuning peut √™tre une bonne option :\n\nBesoin d‚Äôune terminologie ou d‚Äôun style sp√©cifique\nBesoin d‚Äôenrichir les r√©ponses avec des donn√©es sp√©cifiques\nRessources (GPU, data scientists) disponibles\nDonn√©es disponibles en quantit√© et qualit√© suffisantes\nBesoin d‚Äôune application qui ne d√©pend pas d‚Äôun mod√®le sp√©cifique (g√©n√©ralisabilit√©), et dont les utilisateurs ne connaissent pas l‚ÄôIA g√©n√©rative\n\n\n\n\nIl est tout √† fait possible de fine-tuner un LLM sur une t√¢che de RAG par exemple. Peu de travaux ont √©t√© faits sur cette combinaison, mais le papier RAFT (Retrieval Augmented Fine-Tuning) en donne une vision d‚Äôensemble et en propose une m√©thode.",
    "crumbs": [
      "II-D√©veloppements",
      "Techniques d'utilisation d'un LLM"
    ]
  },
  {
    "objectID": "II-Developpements/2_Utilisation_LLM.html#techniques-dutilisation-dun-llm",
    "href": "II-Developpements/2_Utilisation_LLM.html#techniques-dutilisation-dun-llm",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Il faut avant tout garder √† l‚Äôesprit que le prompt engineering est une discipline tr√®s empirique, qui demande beaucoup d‚Äôit√©rations pour obtenir le meilleur prompt par rapport au r√©sultat souhait√©. Bien qu‚Äôil n‚Äôexiste pas de m√©thode syst√©matique et efficace pour optimiser un prompt, certaines pratiques sont devenues la norme. Par exemple, voici quelques bonnes pratiques :\n\nDonner un r√¥le au mod√®le : Par exemple, dire au mod√®le qu‚Äôil est un magistrat honn√™te et impartial pourra l‚Äôaider √† g√©n√©rer du texte formel, neutre et juridique. Le r√¥le est bien s√ªr √† adapter en fonction des exigences de chaque t√¢che.\nStructurer le prompt : Il est important de bien diff√©rencier le prompt syst√®me du prompt utilisateur. Le premier donnera des instructions g√©n√©rales quant au style, √† la t√¢che, au contexte, etc., alors que le second pourra donner des instructions sp√©cifiques ou un texte √† analyser. Il est √©galement pertinent d‚Äôorganiser ou de s√©parer clairement les instructions.\nEtre le plus pr√©cis possible : Rajouter le plus de d√©tails possibles, voire se r√©p√©ter dans les instructions en changeant de formulation permet de bien insister sur les √©l√©ments les plus importants.\nContraindre le mod√®le au maximum : Si l‚Äôon souhaite un format de sortie pr√©cis (JSON par exemple), donner un exemple concret de sortie attendue, et insister sur le besoin de se conformer √† ce format.\nDonner des exemples : Cf. paragraphe suivant (few-shot prompting).\n\nLe papier Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4 donne un certains nombre de principes pour am√©liorer les prompts. Parmi ces principes (tr√®s nombreux), on trouve par exemple :\n\nNe pas etre poli avec le LLM si l‚Äôon souhaite une r√©ponse concise.\nD√©crire l‚Äôaudience souhait√©e dans le prompt (des experts techniques, des enfants, etc.).\nUtiliser des directives affirmatives (fais ceci), et √©viter les tournures n√©gatives (ne fais pas cela).\nEmployer des phrases telles que ‚ÄòTa tache est de‚Äô ou ‚ÄòTu DOIS‚Äô.\nR√©p√©ter plusieurs fois certains mots ou phrases essentielles.\n\n\n\n\nLa fa√ßon la plus intuitive d‚Äôadresser une requ√™te √† un LLM est de formuler des instructions les plus pr√©cises possibles. Ce faisant, on esp√®re que le mod√®le comprendra ces instructions et r√©pondra en cons√©quence. Pour des t√¢ches nouvelles, auxquelles le mod√®le n‚Äôa pas n√©cessairement √©t√© confront√© durant son (pr√©)-entra√Ænement, on appelle cette m√©thode du 0-shot prompting : le mod√®le n‚Äôa pas de r√©f√©rence ou d‚Äôexemple de r√©ponse attendue.\nPour pallier ce manque de r√©f√©rence, il est possible (et, en fonction de la t√¢che, souvent recommand√©) d‚Äôajouter des exemples de paires entr√©e/sortie dans le prompt que l‚Äôon adresse au mod√®le : cela donne du 1-shot (un exemple) ou du few-shot (plusieurs exemples) prompting. Plus les exemples sont proches de la requ√™te initiale, plus le mod√®le saura pr√©cis√©ment comment r√©pondre. Cela permet ainsi au mod√®le de s‚Äôadapter, √† moindre co√ªt, √† une t√¢che tr√®s sp√©cifique ou particuli√®re.\n\nGuide pratique (avec exemples)\n\n\n\n\nSur certaines t√¢ches qui demandent un raisonnement (par exemple la r√©solution d‚Äôun probl√®me math√©matique simple), les LLM naturellement ne sont pas tr√®s bons. Pour augmenter leurs capacit√©s de raisonnement, une strat√©gie classique consiste √† leur demander de raisonner et de r√©fl√©chir √©tape par √©tape.\nLes mod√®les les plus r√©cents ayant nettement progress√© en raisonnement, il est possible qu‚Äôils raisonnent naturellement √©tape par √©tape sur des questions simples. Pour des questions ou des raisonnements plus complexes, il sera cependant probablement plus efficace de proposer une logique de raisonnement au mod√®le, en explicitant les diff√©rentes √©tapes.\nIl est √©galement possible de combiner le CoT reasoning avec du few-shot prompting, i.e. de donner des exemples de raisonnement √©tape par √©tape au mod√®le.\n\nGuide d√©taill√©\n\n\n\n\nUne fa√ßon de travailler ses prompts est de profiter des capacit√©s g√©n√©ratives des LLMs pour leur faire cr√©er des prompts. L‚Äôid√©e est de donner au LLM un exemple de sortie souhait√©e, et de lui demander de g√©n√©rer le prompt le plus adapt√© possible pour produire cette sortie.\n\nGuide pratique\n\n\n\n\n\n\n\nLa premi√®re question √† se poser est la n√©cessit√© ou non d‚Äôutiliser un LLM. Certaines t√¢ches peuvent se r√©soudre avec un LLM, mais ce n‚Äôest pas toujours la solution la plus pertinente. Par exemple, un LLM est normalement capable de parser un fichier xml sans probl√®me, mais un script na√Øf sera largement aussi efficace, √† bien moindre co√ªt (environnemental, humain, financier). L‚Äôutilisation d‚Äôun LLM doit venir d‚Äôun besoin de compr√©hension fine du langage naturel.\n\n\n\nBeaucoup d‚Äô√©l√©ments sont √† prendre en compte lors du choix du mod√®le √† utiliser. Parmi les plus importants¬†:\n\nSa taille : Exprim√©e g√©n√©ralement en milliards (B) de param√®tres (Llama-3 8B poss√®de 8 milliards de param√®tres, Mistral 7B en poss√®de 7 milliards, etc.), elle influe fortement sur les performances du mod√®les et les exigences techniques. Un ¬´¬†petit¬†¬ª LLM de 8 milliards de param√®tres pourra tourner sur un GPU modeste avec une VRAM de 32 GB (voire moins si l‚Äôon utilise un mod√®le quantifi√©, cf.¬†‚Ä¶), tandis qu‚Äôun LLM de taille moyenne de 70 milliards de param√®tres n√©cessitera 2 GPU puissants avec 80 GB de VRAM.\nSon multilinguisme¬†: La plupart des mod√®les sont entra√Æn√©s sur une immense majorit√© de donn√©es anglaises (plus de 90¬†% pour Llama-2, contre moins de 0,1¬†% de donn√©es fran√ßaises). Les mod√®les incluant plus de fran√ßais (Mistral ?) dans leurs donn√©es d‚Äôentra√Ænement sont naturellement plus efficaces sur du fran√ßais.\nSon temps d‚Äôinf√©rence¬†: G√©n√©ralement directement li√© √† la taille du mod√®le, certaines architectures (MoE) permettent cependant d‚Äôavoir un temps d‚Äôinf√©rence plus court.\nSes performances g√©n√©rales¬†: Beaucoup de benchmarks publics √©valuent les LLMs sur des t√¢ches g√©n√©ralistes et vari√©es. Un bon point de d√©part est de regarder le Leaderboard qui recense la plupart des mod√®les connus.\nSes performances sp√©cifiques¬†: Les benchmarks g√©n√©ralistes ne sont pas forc√©ment pertinents pour certains cas d‚Äôusages, car ils ne sont pas sp√©cifiques √† la t√¢che, aux donn√©es, etc. Il peut √™tre int√©ressant de d√©velopper un pipeline d‚Äô√©valuation sp√©cifique (cf‚Ä¶).\n\nEn juin 2024, un bon point de d√©part est de regarder les mod√®les open-source de Meta (Llama-2 7B/13B/70B, Llama-3 8B/70B) et de Mistral AI (Mistral 7B, Mixtral 8x7B).\n\n\n\nSi vous √™tes dans l‚Äôun des cas suivants, le prompt engineering peut √™tre une bonne option :\n\nPas beaucoup de ressources disponibles\nBesoin d‚Äôun outil laiss√© √† la disposition des utilisateurs, avec une grande libert√©\nLes r√©ponses requises sont tr√®s formatt√©es ou tr√®s sp√©cifiques\n\n\n\n\nSi vous √™tes dans l‚Äôun des cas suivants, la RAG peut √™tre une bonne option :\n\nBesoin de r√©ponses √† jour, r√©guli√®rement et facilement actualis√©es\nBesoin de sourcer les r√©ponses ou de diminuer les hallucinations\nBesoin d‚Äôenrichir les r√©ponses avec des donn√©es sp√©cifiques\nBesoin d‚Äôune application qui ne d√©pend pas d‚Äôun mod√®le sp√©cifique (g√©n√©ralisabilit√©), et dont les utilisateurs ne connaissent pas l‚ÄôIA g√©n√©rative\n\n\n\n\nSi vous √™tes dans l‚Äôun des cas suivants, le fine-tuning peut √™tre une bonne option :\n\nBesoin d‚Äôune terminologie ou d‚Äôun style sp√©cifique\nBesoin d‚Äôenrichir les r√©ponses avec des donn√©es sp√©cifiques\nRessources (GPU, data scientists) disponibles\nDonn√©es disponibles en quantit√© et qualit√© suffisantes\nBesoin d‚Äôune application qui ne d√©pend pas d‚Äôun mod√®le sp√©cifique (g√©n√©ralisabilit√©), et dont les utilisateurs ne connaissent pas l‚ÄôIA g√©n√©rative\n\n\n\n\nIl est tout √† fait possible de fine-tuner un LLM sur une t√¢che de RAG par exemple. Peu de travaux ont √©t√© faits sur cette combinaison, mais le papier RAFT (Retrieval Augmented Fine-Tuning) en donne une vision d‚Äôensemble et en propose une m√©thode.",
    "crumbs": [
      "II-D√©veloppements",
      "Techniques d'utilisation d'un LLM"
    ]
  },
  {
    "objectID": "II-Developpements/4_Evaluations.html",
    "href": "II-Developpements/4_Evaluations.html",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Tous les LLM visent le m√™me objectif : ma√Ætriser le langage naturel et par l√† m√™me, √©galer l‚Äôhumain dans des t√¢ches telles que le r√©sum√©, la traduction, la reconnaissance des entit√©s nomm√©es, etc.\nCependant, tous les LLM souffrent des m√™mes d√©fauts, de fa√ßon plus ou moins prononc√©e:\n* Tr√®s grande sensibilit√© du mod√®le au prompt utilis√© \n* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)\n* Les LLM peuvent avoir des comportements inattendus et dangereux suite √† l'usage de prompts malveillants, de donn√©es \nd'entra√Ænement biais√©es, au recours √† des agents trop permissifs, etc.\nOn souhaite donc se doter d‚Äôun cadre de comparaison qui permette d‚Äôaffirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir √† diff√©rentes m√©triques pour mesurer differents aspects du probl√®me (fiabilit√©, s√©curit√©, absence de biais‚Ä¶)\nSi de nombreux bancs d‚Äôessai existent aujourd‚Äôhui, permettant de distinguer certains LLM, il ne faut pas oublier que de bonnes performances dans un banc d‚Äôessai ne sont pas suffisantes, et qu‚Äôil est primordial de mettre en place un syst√®me d‚Äô√©valuation quasi temps r√©√©l du LLM une fois en production.\n\n\n\na) Scenario\nUn sc√©nario est un ensemble de conditions dans lesquelles la performance du LLM est √©valu√©e. Il s‚Äôagit par exemple de\n\nR√©ponse aux questions\nRaisonnement\nTraduction\nG√©n√©ration de texte\n‚Ä¶\n\nb) T√¢che\nUne t√¢che constitue une forme plus granulaire d‚Äôun sc√©nario. Elle conditionne plus sp√©cifiquement sur quelle base le LLM est √©valu√©. Une t√¢che peut √™tre une composition de plusieurs sous-t√¢ches.\n\nCombinaisons de sous-t√¢ches de difficult√© vari√©e\n\nPar exemple, l‚Äôarithm√©tique peut √™tre consid√©r√©e comme une t√¢che constitu√©e des sous-t√¢ches arithm√©tique niveau 1er degr√©, arithm√©tique niveau coll√®ge, arithm√©tique niveau lyc√©e, etc.\n\nCombinaison de sous-t√¢che de domaines vari√©s\n\nLa t√¢che de type QCM peut √™tre vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.\nc) M√©trique\nUne m√©trique est une mesure qualitative utilis√©e pour √©valuer la performance d‚Äôun mod√®le de langage dans certaines t√¢ches/sc√©narios. Une m√©trique peut √™tre :\n\nune fonction statistique/math√©matique d√©terministe simple (par exemple, pr√©cision ou rappel)\nun score produit par un r√©seau neuronal ou un mod√®le de Machine Learning (ex. : score BERT)\nun score g√©n√©r√© √† l‚Äôaide d‚Äôun LLM (ex. : G-Eval)\n\nNotons que dans le dernier cas, √©valuer un LLM √† l‚Äôaide d‚Äôun LLM peut donner l‚Äôimpression du serpent qui se mord la queue. Cependant, ce type de ‚Äòd√©pendances circulaires‚Äô existe couramment et est bien accept√©e dans d‚Äôautres dommaines. Lors d‚Äôun entretien d‚Äôembauche par exemple, l‚Äôintellect humain √©value le potentiel d‚Äôun autre √™tre humain.\nd) Benchmarks\nLes benchmarks sont des collections standardis√©es de tests utilis√©es pour √©valuer les LLM sur une t√¢che ou un sc√©nario donn√©. On trouvera par exemple :\n\nSQuAD pour le sc√©nario de r√©ponse aux questions de l‚Äôutilisateur √† partir d‚Äôextraction de parties d‚Äôun corpus (En anglais)\nPIAF semblable √† SQuAD mais en fran√ßais\nIMDB pour l‚Äôanalyse des sentiments\n‚Ä¶\n\nA titre d‚Äôexemple, l‚Äôimage ci-dessous pr√©sente le corpus PIAF. Il est compos√© de paragraphes issus d‚Äôarticles de Wikipedia, et d‚Äôune liste de questions portant sur ces paragraphes.\n\n\n\n\nLes performances des mod√®les peuvent √™tre √©valu√©es qu‚Äôen comparaison avec les connaissances existantes. Pour ce faire, il est n√©cessaire de disposer d‚Äôensembles de donn√©es de r√©f√©rence dont les r√©sultats sont connus et v√©rifi√©s. Au cours des derni√®res ann√©es, de tels ensembles de donn√©es ont √©t√© collect√©s pour un certain nombre d‚Äôapplications. Pour √©valuer les LLM, il existe des ‚Äúbenchmark datasets‚Äù qui peuvent √™tre utilis√©s pour entra√Æner et pour tester des mod√®les.\n\nCoQA (Conversational Question Answering) est un set de donn√©es avec plus de 127 000 questions-r√©ponses dans 7 domaines sont 5 sont publiques. https://stanfordnlp.github.io/coqa/ Pour √©valuer votre mod√®le, il suffit de lancer ce script\n\npython evaluate-v1.0.py --data-file &lt;chemin_vers_dev-v1.0.json&gt; --pred-file &lt;chemin_vers_predictions&gt;\n\nGLUE (General Language Understanding Evaluation) https://gluebenchmark.com/ et  SuperGLUE** https://super.gluebenchmark.com/ sont des collections des t√¢ches pour √©valuer la compr√©hension du langage naturel. jiant est un PyTorch toolkit qui permet faire cette √©valuation. Installez avec pip :\n\npip install jiant\nIci, un exemple d‚Äôaffinage du mod√®le RoBERTa sur les donn√©es MRPC :\nfrom jiant.proj.simple import runscript as run\nimport jiant.scripts.download_data.runscript as downloader\n\nEXP_DIR = \"/path/to/exp\"\n\n# T√©l√©charger les donn√©es\ndownloader.download_data([\"mrpc\"], f\"{EXP_DIR}/tasks\")\n\n# Configurer les arguments pour l'API simple\nargs = run.RunConfiguration(\n   run_name=\"simple\",\n   exp_dir=EXP_DIR,\n   data_dir=f\"{EXP_DIR}/tasks\",\n   hf_pretrained_model_name_or_path=\"roberta-base\",\n   tasks=\"mrpc\",\n   train_batch_size=16,\n   num_train_epochs=3\n)\n\n# Lancer\nrun.run_simple(args)\n\nSQuAD (Stanford Question Answering Dataset) est un set de donn√©es pour √©valuer la compr√©hension de la lecture. Il est constitu√© des questions bas√©es sur un ensemble d‚Äôarticles de Wikip√©dia avec 100000 questions avec des r√©ponses, et 50000 questions qui ne peuvent pas √™tre r√©pondues. Les mod√®les doivent Pour √©valuer votre mod√®le, il suffit de lancer ce script\n\npython evaluate-v2.0.py &lt;chemin_vers_dev-v2.0&gt; &lt;chemin_vers_predictions&gt;\nPlus d‚Äôinformations sur Git : https://github.com/nyu-mll/jiant\nhttps://github.com/leobeeson/llm_benchmarks.\n\n\n\nL‚Äôexpression √©valuation de LLM peut recouvrir diff√©rentes pratiques et diff√©rents objectifs. On doit ainsi distinguer l‚Äô√©valuations de mod√®les LLM de l‚Äô√©valuations de syst√®mes LLM. Les √©valuations de mod√®les LLM s‚Äôint√©ressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacit√© sur un ensemble de t√¢ches diff√©rentes.\nIl existe de nombreux benchmarks qui permettent d‚Äôillustrer les performances des mod√®les sur des aspects pr√©cis, comme HellaSwag (qui √©value la capacit√© d‚Äôun LLM √† compl√©ter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la v√©racit√© des r√©ponses du mod√®le) et MMLU (qui mesure la capacit√© de compr√©hension et de r√©solution de probl√®mes ).\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nL‚Äô√©valuation de syst√®mes LLM couvre l‚Äô√©valuation de tous les composants de la cha√Æne, pour un mod√®le donn√©. En effet, un mod√®le de LLM est rarement utilis√© seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le r√©sultat du mod√®le. On pourra s‚Äôint√©resser par exemple √† l‚Äôeffet du prompt sur la politesse de la r√©ponse, le style, le niveau de d√©tail, etc. Un mod√®le peut √©galement recevoir un contexte (ensemble de documents, tableaux, images‚Ä¶) et son influence doit √©galement √™tre mesur√©e. On pourrait par exemple s‚Äôapercevoir que le mod√®le produit des r√©sum√©s de qualit√© quand on lui fournit des documents litt√©raires, mais pas des documents techniques.\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nEn pratique, la comparaison des mod√®les sur des benchmarks est r√©alis√©e par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communaut√© universitaire. L‚Äô√©valuation de mod√®le reste cependant int√©ressante pour mesurer le gain de performance apport√© par un fine-tuning sur un corpus interne par exemple. Cependant, ce sont les √©valuations de syst√®mes LLM qui int√©resseront la majorit√© des √©quipes souhaitant d√©ployer un LLM dans leur administration.\n\n\nIl n‚Äôexiste pas de r√©ponse simple √† la question de savoir quelles m√©triques utiliser pour √©valuer son syst√®me LLM. Cela d√©pendra du type de t√¢che, de la population cible, de la nature des donn√©es, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l‚Äôapprentissage machine, on √©value un mod√®le en se dotant d‚Äôun ensemble annot√© d‚Äôentr√©es/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes r√©ponses.\nLa difficult√© de l‚Äô√©valuation en IA g√©n√©rative r√©side dans le fait que nous ne disposons g√©n√©ralement pas de valeur de r√©f√©rence √† laquelle comparer la sortie du mod√®le. M√™me dans les cas o√π l‚Äôon disposerait d‚Äôun exemple de bonne r√©ponse, les donn√©es de sortie √©tant non structur√©es (texte en language naturel), il est difficile de comparer la distance entre deux objets.\nOn reprend ici l‚Äôid√©e de classer les m√©triques en fonction de leur approche du probl√®me, c‚Äôest √† dire selon la fa√ßon dont elles √©valuent la pertinence de la r√©ponse obtenue. Certaines techniques supposent que l‚Äôon dispose d‚Äôune r√©ponse de r√©f√©rence, et la question est alors de savoir comment elles √©valuent la distance entre la r√©ponse obtenue et une r√©ponse de r√©f√©rence. D‚Äôautres techniques plus r√©centes ne font pas cette hypoth√®se, et cherchent √† √©valuer la qualit√© de la r√©ponse dans l‚Äôabsolu. \nOn ne d√©taillera pas toutes les m√©triques dans le cadre de ce guide, il existe pl√©thore de documentation disponible sur le sujet (cf.¬†Bibliographie en fin de guide). L‚Äôobjectif ici est plut√¥t de fournir une grille d‚Äôanalyse.\nM√©triques traditionnelles du machine learning\nDans les m√©triques classiques, on trouve des m√©triques g√©n√©rales de classification qui sont couramment utilis√©es en apprentissage machine et ne sont pas propres aux donn√©es textuelles (Accuracy, Precision, Recall, F1‚Ä¶). Ces m√©triques restent pertinentes pour certaines t√¢ches confi√©es aux LLM, typiquement l‚Äôextraction d‚Äôentit√©s nomm√©es. Parmi les classiques, il existe √©galement des m√©triques sp√©cifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases pr√©dites et les phrases de r√©f√©rence. Le recouvrement peut √™tre calcul√© au niveau des mots, ou au niveau des lettres. Ces m√©thodes ont √©t√© critiqu√©es pour leur faible corr√©lation avec le jugement humain, ce qui est n‚Äôest pas √©tonnant dans la mesure o√π elles ne s‚Äôint√©ressent qu‚Äôa la forme de surface du texte.\nIntroduction de crit√®res s√©mantiques\nLes m√©triques bas√©es sur le Deep Learning permettent de palier ce probl√®me en introduisant des crit√®res s√©mantiques. On distingue en premier lieu les m√©triques qui ont besoin d‚Äôune valeur de r√©f√©rence et les autres.\nD√©tenir une valeur de r√©f√©rence peut repr√©senter une grosse contrainte. Tout d‚Äôabord, ce n‚Äôest pas toujours pertinent; si on demande au LLM d‚Äô√©crire un po√®me sur la mer par exemple, il serait vain de chercher √† le comparer √† un autre po√®me. Deuxi√®mement, les textes de r√©f√©rences ne sont pas toujours de meilleure qualit√© que les textes g√©n√©r√©s par des LLM. Dans le cas du r√©sum√© automatique par exemple, [Fabbri, 2020] illustre les probl√®mes du dataset CNN/DailyMail [Hermann, 2015] dans lequel les r√©sum√©s de r√©f√©rence sont pollu√©s par des r√©f√©rences et click-baits vers d‚Äôautres articles, ou souffrent d‚Äôun manque de coh√©rence suite √† la concat√©nation de r√©sum√©s sous forme de liste √† puces. Ces r√©sum√©s, √©valu√©s par des annotateurs humains, obtiennent parfois de moins bons scores que des r√©sum√©s g√©n√©r√©s par LLM. Enfin, m√™me si l‚Äôon dispose de r√©f√©rences de qualit√©, il faut s‚Äôassurer que la distribution des documents est la m√™me que celle qui sera utilis√©e en production. Si l‚Äôon entra√Æne un mod√®le √† r√©sumer des articles de presse par exemple, il ne sera pas n√©cessairement performants sur des documents d‚Äôune autre nature. D√®s lors se pose la question de comment constituer un dataset d‚Äô√©valuation pour un mod√®le de r√©sum√© √† vocation g√©n√©raliste.\nToujours est-il que si l‚Äôon dispose de valeurs de r√©f√©rence, on peut recourir √† des m√©triques bas√©s sur les embeddings ou des m√©triques bas√©es sur des mod√®les fine-tun√©s.\nLes m√©triques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexit√© peut les rendre int√©ressantes. Elles exigent toutes des r√©ponse de r√©f√©rence. On peut citer BERTScore ou MoverScore.\nLes m√©triques bas√©es sur des LLM utilisent un LLM qui joue le r√¥le de juge pour √©valuer les sorties d‚Äôun LLM. Elles peuvent fonctionner avec ou sans r√©f√©rences, selon des modalit√©s vari√©es (scoring, ranking, classification, r√©ponse √† des question ferm√©es, etc.) Ces techniques n√©cessitent en g√©n√©ral de d√©crire:\n\nla t√¢che confi√©e au LLM initial (r√©sum√©, traduction, ‚Ä¶)\nles aspects √† √©valuer (fluidit√©, factualit√©, citation des sources‚Ä¶)\neventuellement, les √©tapes de raisonnement permettant de d√©terminer le respect des crit√®res\ndonner quelques exemples\n\nCes m√©thodes sont aujourd‚Äôhui √† l‚Äô√©tat de l‚Äôart pour l‚Äô√©valuation des LLM, si tant est que l‚Äôon utilise des LLM propri√©taires comme juge (Constat valable pour le fran√ßais du moins). L‚Äôinconv√©nient est qu‚Äôelles peuvent vite devenir co√ªteuses, et que le r√©sultat n‚Äôest pas forc√©ment reproductible.\nSi notre use case n‚Äôest pas compatible avec l‚Äôusage d‚Äôun LLM propri√©taire, on peut alors recourir aux m√©triques bas√©es sur le fine-tuning de LLM de taille moyenne, open-source. Il peut √™tre n√©cessaire de fine-tuner encore les mod√®les sur vos propres corpus. De plus, certains de ces mod√®les ne sont capables de r√©pondre qu‚Äô√† la question pour laquelle ils ont √©t√© entra√Æn√©s. Par exemple, le mod√®le Lynx (Patronus AI ) est entra√Æn√© √† d√©tecter les hallucinations dans les environnements RAG. √Ä partir d‚Äôun document, d‚Äôune question et d‚Äôune r√©ponse, le mod√®le tente d‚Äô√©valuer si la r√©ponse est fid√®le au document. En revanche, il ne peut se prononcer sur d‚Äôautres aspects. Le mod√®le Prometheus-Eval (LG AI Research, KAIST AI) est lui capable de r√©pondre √† des questions sur diff√©rents aspects, tant que l‚Äôutilisateur les d√©finit explicitement dans le prompt.\n```code python\nrubric_data = { ‚Äúcriteria‚Äù:‚ÄúIs the model proficient in applying empathy and emotional intelligence to its responses when the user conveys emotions or faces challenging circumstances?‚Äù, ‚Äúscore1_description‚Äù:‚ÄúThe model neglects to identify or react to the emotional tone of user inputs, giving responses that are unfitting or emotionally insensitive.‚Äù, ‚Äúscore2_description‚Äù:‚ÄúThe model intermittently acknowledges emotional context but often responds without sufficient empathy or emotional understanding.‚Äù, ‚Äúscore3_description‚Äù:‚ÄúThe model typically identifies emotional context and attempts to answer with empathy, yet the responses might sometimes miss the point or lack emotional profundity.‚Äù, ‚Äúscore4_description‚Äù:‚ÄúThe model consistently identifies and reacts suitably to emotional context, providing empathetic responses. Nonetheless, there may still be sporadic oversights or deficiencies in emotional depth.‚Äù, ‚Äúscore5_description‚Äù:‚ÄúThe model excels in identifying emotional context and persistently offers empathetic, emotionally aware responses that demonstrate a profound comprehension of the user‚Äôs emotions or situation.‚Äù }\n```\nQAG (Question Answer Generation) Score est un √©valuateur qui exploite les forte capacit√©s de raisonnement des LLM pour √©valuer de mani√®re fiable les r√©sultats des LLM. Ici, on utilise les r√©ponses √† des questions ferm√©es (g√©n√©r√©es ou pr√©d√©finies) pour calculer un score final. Cette approche est relativement fiable parce qu‚Äôelle n‚Äôutilise pas les LLM pour g√©n√©rer directement les scores. Par exemple, si vous voulez calculer un score de fid√©lit√© (qui mesure si une sortie du LLM a √©t√© hallucin√©e ou non), vous devez :\n* Utiliser un LLM pour extraire toutes les affirmations faites dans une sortie LLM. * Convertir ces affirmations en question. * Pour chaque question, demandez au LLM si la r√©ponse de r√©f√©rence concorde avec l‚Äôaffirmation faite.\nEn novembre 2024, on constate n√©anmoins que les LLM open-source ne sont pas encore assez puissants pour mener √† bien la t√¢che de d√©coupage du texte en affirmations √©l√©mentaires. Si les donn√©es √† √©valuer ne sont pas sensibles, nous conseillons, pour l‚Äôheure, de privil√©gier un LLM propri√©taire.\n\n\n\n\n\n\n\n\n\n\n\nCat√©gorie\nM√©triques\nD√©tails\n\n\n\n\nM√©triques d‚Äôengagement des utilisateurs et g√©n√©ralit√©s\n\n\n\n\n\nVisit√©\nNombre d‚Äôutilisateurs qui ont visit√© l‚Äôapplication\n\n\n\nSoumis\nNombre d‚Äôutilisateurs qui soumettent des messages\n\n\n\nR√©pondu\nLe LLM g√©n√®re des r√©ponses sans erreurs\n\n\n\nVu\nL‚Äôutilisateur consulte les r√©ponses du LLM\n\n\n\nClics\nL‚Äôutilisateur clique sur la documentation r√©f√©renc√©e dans la r√©ponse du LLM, le cas √©ch√©ant\n\n\nInteraction avec l‚Äôutilisateur\n\n\n\n\n\nTaux d‚Äôacceptation\nFr√©quence d‚Äôacceptation des r√©ponses du LLM par l‚Äôutilisateur\n\n\n\nConversation LLM\nNombre moyen de conversations LLM par utilisateur\n\n\n\nJours d‚Äôactivit√©\nNombre de jours actifs d‚Äôutilisation du LLM (par utilisateur)\n\n\n\nTiming\nDur√©e moyenne entre les prompts et les r√©ponses, et temps consacr√© √† chacune\n\n\nFeedback utilisateur et r√©tention\n\n\n\n\n\nFeedback utilisateur\nNombre de r√©ponses avec des commentaires positifs ou n√©gatifs\n\n\n\nUtilisateurs actifs par p√©riode\nNombre d‚Äôutilisateurs ayant visit√© l‚Äôapplication LLM au cours d‚Äôune p√©riode donn√©e\n\n\n\nTaux de retour\nPourcentage d‚Äôutilisateurs qui ont utilis√© cette fonction la semaine/mois pr√©c√©dente et qui continuent √† l‚Äôutiliser cette semaine/mois.\n\n\nPerformance\n\n\n\n\n\nRequ√™tes par seconde (Concurrence)\n\n\n\n\nTokens par seconde\nCompte les tokens g√©n√©r√©s par seconde lors de la diffusion de la r√©ponse LLM.\n\n\n\nD√©lai avant le premier rendu de jeton\n\n\n\n\nTaux d‚Äôerreur\nTaux d‚Äôerreur pour diff√©rents types d‚Äôerreurs tels que l‚Äôerreur 401, l‚Äôerreur 429.\n\n\n\nFiabilit√©\nLe pourcentage de demandes satisfaites par rapport au nombre total de demandes, y compris celles qui comportent des erreurs ou des √©checs.\n\n\n\nLatence\nDur√©e moyenne du temps de traitement entre la soumission d‚Äôune requ√™te et la r√©ception d‚Äôune r√©ponse.\n\n\n\nutilisation GPU/CPU\nUtilisation en termes de nombre total de tokens et nombre de code erreur 429 re√ßus (‚ÄòRate limit reached for requests‚Äô dans l‚ÄôAPI OpenAI)\n\n\nCo√ªts\n\n\n\n\n\nCo√ªt des appels LLM\nCe qui est factur√© par le fournisseur du LLM si vous passez par un LLM heberg√© par un tiers\n\n\n\nCo√ªt de l‚Äôinfrastucture\nCo√ªt du stockage, √©nergie, si vous h√©bergez votre LLM\n\n\n\nCo√ªt op√©rationnel\nCo√ªt de la maintenance, du monitoring, de la mise en place des mesures de s√©curit√©, du support, etc si vous h√©bergez votre LLM\n\n\n\n\n\n\n\nA ce jour, il existe des nombreux outils et librairies pour effectuer l‚Äô√©valuation des mod√®les LLM. Chaque de ces librairies et frameworks est taill√©e pour une utilisation de mod√®le concr√®te avec des exemples pour vous aider √† d√©marrer l‚Äô√©valuation de votre mod√®le.\nHugging Face Transformers fournissent des API et des outils pour √©valuer des mod√®les pr√©-entra√Æn√©s sur diff√©rentes t√¢ches en utilisant des m√©triques telles que la pr√©cision, le score F1 ou encore le score BLEU. Ils prennent en charge aussi l‚Äôint√©gration les donn√©es de la biblioth√®que Hugging Face Datasets. https://huggingface.co/docs/transformers/index\nScikit-learn, c‚Äôest un projet Open source avec des librairies principalement ax√©es sur l‚Äôapprentissage automatique traditionnel. Elle comprend de nombreux outils de m√©triques et utilitaires qui peuvent √™tre utilis√©s pour √©valuer les mod√®les de langage. https://scikit-learn.org/stable/\nEvalML est une biblioth√®que sp√©cifique pour l‚Äô√©valuation des mod√®les d‚Äôapprentissage automatique, y compris les LLM. Elle fournit des m√©triques, des visualisations et des outils de s√©lection de mod√®les. https://evalml.alteryx.com/en/stable/\nNLTK et SpaCy - ces deux biblioth√®ques offrent des fonctionnalit√©s pour le traitement du langage naturel et incluent des m√©triques pour √©valuer des t√¢ches telles que la tokenisation, l‚Äôanalyse syntaxique et l‚Äôanalyse des sentiments. https://www.nltk.org/ https://spacy.io/\nAllenNLP est une biblioth√®que con√ßue pour construire et √©valuer des mod√®les de NLP. Elle fournit des outils pour faciliter la mise en ≈ìuvre de mesures d‚Äô√©valuation et de visualisation personnalis√©es. https://docs.allennlp.org/models/main/\nTransformers-Interpret Une biblioth√®que qui se concentre sur l‚Äôinterpr√©tabilit√© des mod√®les, permettant de mieux comprendre les pr√©dictions et les performances des mod√®les. https://pypi.org/project/transformers-interpret/0.3.0/\nLangChain est principalement destin√©e √† la construction d‚Äôapplications avec des LLM. Elle comprend des outils d‚Äô√©valuation pour √©valuer la performance des mod√®les de langage dans le contexte. https://www.langchain.com/\nOpenAI Evals est une bo√Æte √† outils d‚Äô√©valuation de l‚ÄôOpenAI qui fournit les outils et lignes directrices pour √©valuer la performance et la s√©curit√© de leurs mod√®les. https://github.com/openai/evals\nAutres sources : ‚Ä¶\n\n\n\n\nUn arbre de d√©cision pour l‚Äô√©valuation des LLM peut vous aider √† guider votre processus d‚Äô√©valuation en fonction de crit√®res et d‚Äôobjectifs sp√©cifiques de votre mod√®le et son √©valuation. Voici un exemple de tel arbre de d√©cision qui pourrait vous aider en cas de doute. v1",
    "crumbs": [
      "II-D√©veloppements",
      "Evaluations"
    ]
  },
  {
    "objectID": "II-Developpements/4_Evaluations.html#√©valuations-dun-mod√®le",
    "href": "II-Developpements/4_Evaluations.html#√©valuations-dun-mod√®le",
    "title": "PARTIE II. D√©veloppements autour des LLMs (pour les data scientists)",
    "section": "",
    "text": "Tous les LLM visent le m√™me objectif : ma√Ætriser le langage naturel et par l√† m√™me, √©galer l‚Äôhumain dans des t√¢ches telles que le r√©sum√©, la traduction, la reconnaissance des entit√©s nomm√©es, etc.\nCependant, tous les LLM souffrent des m√™mes d√©fauts, de fa√ßon plus ou moins prononc√©e:\n* Tr√®s grande sensibilit√© du mod√®le au prompt utilis√© \n* Les affirmations produites par les LLM ne sont pas toujours factuellement correctes (on parle d'hallucinations)\n* Les LLM peuvent avoir des comportements inattendus et dangereux suite √† l'usage de prompts malveillants, de donn√©es \nd'entra√Ænement biais√©es, au recours √† des agents trop permissifs, etc.\nOn souhaite donc se doter d‚Äôun cadre de comparaison qui permette d‚Äôaffirmer que tel LLM est plus performant ou plus fiable que tel autre. On devra recourir √† diff√©rentes m√©triques pour mesurer differents aspects du probl√®me (fiabilit√©, s√©curit√©, absence de biais‚Ä¶)\nSi de nombreux bancs d‚Äôessai existent aujourd‚Äôhui, permettant de distinguer certains LLM, il ne faut pas oublier que de bonnes performances dans un banc d‚Äôessai ne sont pas suffisantes, et qu‚Äôil est primordial de mettre en place un syst√®me d‚Äô√©valuation quasi temps r√©√©l du LLM une fois en production.\n\n\n\na) Scenario\nUn sc√©nario est un ensemble de conditions dans lesquelles la performance du LLM est √©valu√©e. Il s‚Äôagit par exemple de\n\nR√©ponse aux questions\nRaisonnement\nTraduction\nG√©n√©ration de texte\n‚Ä¶\n\nb) T√¢che\nUne t√¢che constitue une forme plus granulaire d‚Äôun sc√©nario. Elle conditionne plus sp√©cifiquement sur quelle base le LLM est √©valu√©. Une t√¢che peut √™tre une composition de plusieurs sous-t√¢ches.\n\nCombinaisons de sous-t√¢ches de difficult√© vari√©e\n\nPar exemple, l‚Äôarithm√©tique peut √™tre consid√©r√©e comme une t√¢che constitu√©e des sous-t√¢ches arithm√©tique niveau 1er degr√©, arithm√©tique niveau coll√®ge, arithm√©tique niveau lyc√©e, etc.\n\nCombinaison de sous-t√¢che de domaines vari√©s\n\nLa t√¢che de type QCM peut √™tre vue comme la combinaison de QCM histoire, QCM anglais, QCM logique, etc.\nc) M√©trique\nUne m√©trique est une mesure qualitative utilis√©e pour √©valuer la performance d‚Äôun mod√®le de langage dans certaines t√¢ches/sc√©narios. Une m√©trique peut √™tre :\n\nune fonction statistique/math√©matique d√©terministe simple (par exemple, pr√©cision ou rappel)\nun score produit par un r√©seau neuronal ou un mod√®le de Machine Learning (ex. : score BERT)\nun score g√©n√©r√© √† l‚Äôaide d‚Äôun LLM (ex. : G-Eval)\n\nNotons que dans le dernier cas, √©valuer un LLM √† l‚Äôaide d‚Äôun LLM peut donner l‚Äôimpression du serpent qui se mord la queue. Cependant, ce type de ‚Äòd√©pendances circulaires‚Äô existe couramment et est bien accept√©e dans d‚Äôautres dommaines. Lors d‚Äôun entretien d‚Äôembauche par exemple, l‚Äôintellect humain √©value le potentiel d‚Äôun autre √™tre humain.\nd) Benchmarks\nLes benchmarks sont des collections standardis√©es de tests utilis√©es pour √©valuer les LLM sur une t√¢che ou un sc√©nario donn√©. On trouvera par exemple :\n\nSQuAD pour le sc√©nario de r√©ponse aux questions de l‚Äôutilisateur √† partir d‚Äôextraction de parties d‚Äôun corpus (En anglais)\nPIAF semblable √† SQuAD mais en fran√ßais\nIMDB pour l‚Äôanalyse des sentiments\n‚Ä¶\n\nA titre d‚Äôexemple, l‚Äôimage ci-dessous pr√©sente le corpus PIAF. Il est compos√© de paragraphes issus d‚Äôarticles de Wikipedia, et d‚Äôune liste de questions portant sur ces paragraphes.\n\n\n\n\nLes performances des mod√®les peuvent √™tre √©valu√©es qu‚Äôen comparaison avec les connaissances existantes. Pour ce faire, il est n√©cessaire de disposer d‚Äôensembles de donn√©es de r√©f√©rence dont les r√©sultats sont connus et v√©rifi√©s. Au cours des derni√®res ann√©es, de tels ensembles de donn√©es ont √©t√© collect√©s pour un certain nombre d‚Äôapplications. Pour √©valuer les LLM, il existe des ‚Äúbenchmark datasets‚Äù qui peuvent √™tre utilis√©s pour entra√Æner et pour tester des mod√®les.\n\nCoQA (Conversational Question Answering) est un set de donn√©es avec plus de 127 000 questions-r√©ponses dans 7 domaines sont 5 sont publiques. https://stanfordnlp.github.io/coqa/ Pour √©valuer votre mod√®le, il suffit de lancer ce script\n\npython evaluate-v1.0.py --data-file &lt;chemin_vers_dev-v1.0.json&gt; --pred-file &lt;chemin_vers_predictions&gt;\n\nGLUE (General Language Understanding Evaluation) https://gluebenchmark.com/ et  SuperGLUE** https://super.gluebenchmark.com/ sont des collections des t√¢ches pour √©valuer la compr√©hension du langage naturel. jiant est un PyTorch toolkit qui permet faire cette √©valuation. Installez avec pip :\n\npip install jiant\nIci, un exemple d‚Äôaffinage du mod√®le RoBERTa sur les donn√©es MRPC :\nfrom jiant.proj.simple import runscript as run\nimport jiant.scripts.download_data.runscript as downloader\n\nEXP_DIR = \"/path/to/exp\"\n\n# T√©l√©charger les donn√©es\ndownloader.download_data([\"mrpc\"], f\"{EXP_DIR}/tasks\")\n\n# Configurer les arguments pour l'API simple\nargs = run.RunConfiguration(\n   run_name=\"simple\",\n   exp_dir=EXP_DIR,\n   data_dir=f\"{EXP_DIR}/tasks\",\n   hf_pretrained_model_name_or_path=\"roberta-base\",\n   tasks=\"mrpc\",\n   train_batch_size=16,\n   num_train_epochs=3\n)\n\n# Lancer\nrun.run_simple(args)\n\nSQuAD (Stanford Question Answering Dataset) est un set de donn√©es pour √©valuer la compr√©hension de la lecture. Il est constitu√© des questions bas√©es sur un ensemble d‚Äôarticles de Wikip√©dia avec 100000 questions avec des r√©ponses, et 50000 questions qui ne peuvent pas √™tre r√©pondues. Les mod√®les doivent Pour √©valuer votre mod√®le, il suffit de lancer ce script\n\npython evaluate-v2.0.py &lt;chemin_vers_dev-v2.0&gt; &lt;chemin_vers_predictions&gt;\nPlus d‚Äôinformations sur Git : https://github.com/nyu-mll/jiant\nhttps://github.com/leobeeson/llm_benchmarks.\n\n\n\nL‚Äôexpression √©valuation de LLM peut recouvrir diff√©rentes pratiques et diff√©rents objectifs. On doit ainsi distinguer l‚Äô√©valuations de mod√®les LLM de l‚Äô√©valuations de syst√®mes LLM. Les √©valuations de mod√®les LLM s‚Äôint√©ressent aux performances globales. Les entreprises/centres de recherche qui lance leurs LLM ont besoin de quantifier leur efficacit√© sur un ensemble de t√¢ches diff√©rentes.\nIl existe de nombreux benchmarks qui permettent d‚Äôillustrer les performances des mod√®les sur des aspects pr√©cis, comme HellaSwag (qui √©value la capacit√© d‚Äôun LLM √† compl√©ter une phrase et faire preuve de bon sens), TruthfulQA (qui mesure la v√©racit√© des r√©ponses du mod√®le) et MMLU (qui mesure la capacit√© de compr√©hension et de r√©solution de probl√®mes ).\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nL‚Äô√©valuation de syst√®mes LLM couvre l‚Äô√©valuation de tous les composants de la cha√Æne, pour un mod√®le donn√©. En effet, un mod√®le de LLM est rarement utilis√© seul. A minima, il faut lui fournir un prompt, et celui-ci aura un fort impact sur le r√©sultat du mod√®le. On pourra s‚Äôint√©resser par exemple √† l‚Äôeffet du prompt sur la politesse de la r√©ponse, le style, le niveau de d√©tail, etc. Un mod√®le peut √©galement recevoir un contexte (ensemble de documents, tableaux, images‚Ä¶) et son influence doit √©galement √™tre mesur√©e. On pourrait par exemple s‚Äôapercevoir que le mod√®le produit des r√©sum√©s de qualit√© quand on lui fournit des documents litt√©raires, mais pas des documents techniques.\n\n(Source https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)\nEn pratique, la comparaison des mod√®les sur des benchmarks est r√©alis√©e par les grands fournisseurs de LLM (OpenAI, Facebook, Google, etc) ou par la communaut√© universitaire. L‚Äô√©valuation de mod√®le reste cependant int√©ressante pour mesurer le gain de performance apport√© par un fine-tuning sur un corpus interne par exemple. Cependant, ce sont les √©valuations de syst√®mes LLM qui int√©resseront la majorit√© des √©quipes souhaitant d√©ployer un LLM dans leur administration.\n\n\nIl n‚Äôexiste pas de r√©ponse simple √† la question de savoir quelles m√©triques utiliser pour √©valuer son syst√®me LLM. Cela d√©pendra du type de t√¢che, de la population cible, de la nature des donn√©es, des ressources materielles disponibles, etc Traditionnellement, dans le domaine de l‚Äôapprentissage machine, on √©value un mod√®le en se dotant d‚Äôun ensemble annot√© d‚Äôentr√©es/sorties attendues, et on compare ensuite la distance entre la sortie obtenue et la sortie attendue. Dans le cas de la classification, on peut par exemple mesurer le taux de bonnes r√©ponses.\nLa difficult√© de l‚Äô√©valuation en IA g√©n√©rative r√©side dans le fait que nous ne disposons g√©n√©ralement pas de valeur de r√©f√©rence √† laquelle comparer la sortie du mod√®le. M√™me dans les cas o√π l‚Äôon disposerait d‚Äôun exemple de bonne r√©ponse, les donn√©es de sortie √©tant non structur√©es (texte en language naturel), il est difficile de comparer la distance entre deux objets.\nOn reprend ici l‚Äôid√©e de classer les m√©triques en fonction de leur approche du probl√®me, c‚Äôest √† dire selon la fa√ßon dont elles √©valuent la pertinence de la r√©ponse obtenue. Certaines techniques supposent que l‚Äôon dispose d‚Äôune r√©ponse de r√©f√©rence, et la question est alors de savoir comment elles √©valuent la distance entre la r√©ponse obtenue et une r√©ponse de r√©f√©rence. D‚Äôautres techniques plus r√©centes ne font pas cette hypoth√®se, et cherchent √† √©valuer la qualit√© de la r√©ponse dans l‚Äôabsolu. \nOn ne d√©taillera pas toutes les m√©triques dans le cadre de ce guide, il existe pl√©thore de documentation disponible sur le sujet (cf.¬†Bibliographie en fin de guide). L‚Äôobjectif ici est plut√¥t de fournir une grille d‚Äôanalyse.\nM√©triques traditionnelles du machine learning\nDans les m√©triques classiques, on trouve des m√©triques g√©n√©rales de classification qui sont couramment utilis√©es en apprentissage machine et ne sont pas propres aux donn√©es textuelles (Accuracy, Precision, Recall, F1‚Ä¶). Ces m√©triques restent pertinentes pour certaines t√¢ches confi√©es aux LLM, typiquement l‚Äôextraction d‚Äôentit√©s nomm√©es. Parmi les classiques, il existe √©galement des m√©triques sp√©cifiques au texte, qui reposent sur le principe du recouvrement maximal entre les phrases pr√©dites et les phrases de r√©f√©rence. Le recouvrement peut √™tre calcul√© au niveau des mots, ou au niveau des lettres. Ces m√©thodes ont √©t√© critiqu√©es pour leur faible corr√©lation avec le jugement humain, ce qui est n‚Äôest pas √©tonnant dans la mesure o√π elles ne s‚Äôint√©ressent qu‚Äôa la forme de surface du texte.\nIntroduction de crit√®res s√©mantiques\nLes m√©triques bas√©es sur le Deep Learning permettent de palier ce probl√®me en introduisant des crit√®res s√©mantiques. On distingue en premier lieu les m√©triques qui ont besoin d‚Äôune valeur de r√©f√©rence et les autres.\nD√©tenir une valeur de r√©f√©rence peut repr√©senter une grosse contrainte. Tout d‚Äôabord, ce n‚Äôest pas toujours pertinent; si on demande au LLM d‚Äô√©crire un po√®me sur la mer par exemple, il serait vain de chercher √† le comparer √† un autre po√®me. Deuxi√®mement, les textes de r√©f√©rences ne sont pas toujours de meilleure qualit√© que les textes g√©n√©r√©s par des LLM. Dans le cas du r√©sum√© automatique par exemple, [Fabbri, 2020] illustre les probl√®mes du dataset CNN/DailyMail [Hermann, 2015] dans lequel les r√©sum√©s de r√©f√©rence sont pollu√©s par des r√©f√©rences et click-baits vers d‚Äôautres articles, ou souffrent d‚Äôun manque de coh√©rence suite √† la concat√©nation de r√©sum√©s sous forme de liste √† puces. Ces r√©sum√©s, √©valu√©s par des annotateurs humains, obtiennent parfois de moins bons scores que des r√©sum√©s g√©n√©r√©s par LLM. Enfin, m√™me si l‚Äôon dispose de r√©f√©rences de qualit√©, il faut s‚Äôassurer que la distribution des documents est la m√™me que celle qui sera utilis√©e en production. Si l‚Äôon entra√Æne un mod√®le √† r√©sumer des articles de presse par exemple, il ne sera pas n√©cessairement performants sur des documents d‚Äôune autre nature. D√®s lors se pose la question de comment constituer un dataset d‚Äô√©valuation pour un mod√®le de r√©sum√© √† vocation g√©n√©raliste.\nToujours est-il que si l‚Äôon dispose de valeurs de r√©f√©rence, on peut recourir √† des m√©triques bas√©s sur les embeddings ou des m√©triques bas√©es sur des mod√®les fine-tun√©s.\nLes m√©triques qui calculent la distance entre embeddings sont parmi les moins fines, mais leur faible complexit√© peut les rendre int√©ressantes. Elles exigent toutes des r√©ponse de r√©f√©rence. On peut citer BERTScore ou MoverScore.\nLes m√©triques bas√©es sur des LLM utilisent un LLM qui joue le r√¥le de juge pour √©valuer les sorties d‚Äôun LLM. Elles peuvent fonctionner avec ou sans r√©f√©rences, selon des modalit√©s vari√©es (scoring, ranking, classification, r√©ponse √† des question ferm√©es, etc.) Ces techniques n√©cessitent en g√©n√©ral de d√©crire:\n\nla t√¢che confi√©e au LLM initial (r√©sum√©, traduction, ‚Ä¶)\nles aspects √† √©valuer (fluidit√©, factualit√©, citation des sources‚Ä¶)\neventuellement, les √©tapes de raisonnement permettant de d√©terminer le respect des crit√®res\ndonner quelques exemples\n\nCes m√©thodes sont aujourd‚Äôhui √† l‚Äô√©tat de l‚Äôart pour l‚Äô√©valuation des LLM, si tant est que l‚Äôon utilise des LLM propri√©taires comme juge (Constat valable pour le fran√ßais du moins). L‚Äôinconv√©nient est qu‚Äôelles peuvent vite devenir co√ªteuses, et que le r√©sultat n‚Äôest pas forc√©ment reproductible.\nSi notre use case n‚Äôest pas compatible avec l‚Äôusage d‚Äôun LLM propri√©taire, on peut alors recourir aux m√©triques bas√©es sur le fine-tuning de LLM de taille moyenne, open-source. Il peut √™tre n√©cessaire de fine-tuner encore les mod√®les sur vos propres corpus. De plus, certains de ces mod√®les ne sont capables de r√©pondre qu‚Äô√† la question pour laquelle ils ont √©t√© entra√Æn√©s. Par exemple, le mod√®le Lynx (Patronus AI ) est entra√Æn√© √† d√©tecter les hallucinations dans les environnements RAG. √Ä partir d‚Äôun document, d‚Äôune question et d‚Äôune r√©ponse, le mod√®le tente d‚Äô√©valuer si la r√©ponse est fid√®le au document. En revanche, il ne peut se prononcer sur d‚Äôautres aspects. Le mod√®le Prometheus-Eval (LG AI Research, KAIST AI) est lui capable de r√©pondre √† des questions sur diff√©rents aspects, tant que l‚Äôutilisateur les d√©finit explicitement dans le prompt.\n```code python\nrubric_data = { ‚Äúcriteria‚Äù:‚ÄúIs the model proficient in applying empathy and emotional intelligence to its responses when the user conveys emotions or faces challenging circumstances?‚Äù, ‚Äúscore1_description‚Äù:‚ÄúThe model neglects to identify or react to the emotional tone of user inputs, giving responses that are unfitting or emotionally insensitive.‚Äù, ‚Äúscore2_description‚Äù:‚ÄúThe model intermittently acknowledges emotional context but often responds without sufficient empathy or emotional understanding.‚Äù, ‚Äúscore3_description‚Äù:‚ÄúThe model typically identifies emotional context and attempts to answer with empathy, yet the responses might sometimes miss the point or lack emotional profundity.‚Äù, ‚Äúscore4_description‚Äù:‚ÄúThe model consistently identifies and reacts suitably to emotional context, providing empathetic responses. Nonetheless, there may still be sporadic oversights or deficiencies in emotional depth.‚Äù, ‚Äúscore5_description‚Äù:‚ÄúThe model excels in identifying emotional context and persistently offers empathetic, emotionally aware responses that demonstrate a profound comprehension of the user‚Äôs emotions or situation.‚Äù }\n```\nQAG (Question Answer Generation) Score est un √©valuateur qui exploite les forte capacit√©s de raisonnement des LLM pour √©valuer de mani√®re fiable les r√©sultats des LLM. Ici, on utilise les r√©ponses √† des questions ferm√©es (g√©n√©r√©es ou pr√©d√©finies) pour calculer un score final. Cette approche est relativement fiable parce qu‚Äôelle n‚Äôutilise pas les LLM pour g√©n√©rer directement les scores. Par exemple, si vous voulez calculer un score de fid√©lit√© (qui mesure si une sortie du LLM a √©t√© hallucin√©e ou non), vous devez :\n* Utiliser un LLM pour extraire toutes les affirmations faites dans une sortie LLM. * Convertir ces affirmations en question. * Pour chaque question, demandez au LLM si la r√©ponse de r√©f√©rence concorde avec l‚Äôaffirmation faite.\nEn novembre 2024, on constate n√©anmoins que les LLM open-source ne sont pas encore assez puissants pour mener √† bien la t√¢che de d√©coupage du texte en affirmations √©l√©mentaires. Si les donn√©es √† √©valuer ne sont pas sensibles, nous conseillons, pour l‚Äôheure, de privil√©gier un LLM propri√©taire.\n\n\n\n\n\n\n\n\n\n\n\nCat√©gorie\nM√©triques\nD√©tails\n\n\n\n\nM√©triques d‚Äôengagement des utilisateurs et g√©n√©ralit√©s\n\n\n\n\n\nVisit√©\nNombre d‚Äôutilisateurs qui ont visit√© l‚Äôapplication\n\n\n\nSoumis\nNombre d‚Äôutilisateurs qui soumettent des messages\n\n\n\nR√©pondu\nLe LLM g√©n√®re des r√©ponses sans erreurs\n\n\n\nVu\nL‚Äôutilisateur consulte les r√©ponses du LLM\n\n\n\nClics\nL‚Äôutilisateur clique sur la documentation r√©f√©renc√©e dans la r√©ponse du LLM, le cas √©ch√©ant\n\n\nInteraction avec l‚Äôutilisateur\n\n\n\n\n\nTaux d‚Äôacceptation\nFr√©quence d‚Äôacceptation des r√©ponses du LLM par l‚Äôutilisateur\n\n\n\nConversation LLM\nNombre moyen de conversations LLM par utilisateur\n\n\n\nJours d‚Äôactivit√©\nNombre de jours actifs d‚Äôutilisation du LLM (par utilisateur)\n\n\n\nTiming\nDur√©e moyenne entre les prompts et les r√©ponses, et temps consacr√© √† chacune\n\n\nFeedback utilisateur et r√©tention\n\n\n\n\n\nFeedback utilisateur\nNombre de r√©ponses avec des commentaires positifs ou n√©gatifs\n\n\n\nUtilisateurs actifs par p√©riode\nNombre d‚Äôutilisateurs ayant visit√© l‚Äôapplication LLM au cours d‚Äôune p√©riode donn√©e\n\n\n\nTaux de retour\nPourcentage d‚Äôutilisateurs qui ont utilis√© cette fonction la semaine/mois pr√©c√©dente et qui continuent √† l‚Äôutiliser cette semaine/mois.\n\n\nPerformance\n\n\n\n\n\nRequ√™tes par seconde (Concurrence)\n\n\n\n\nTokens par seconde\nCompte les tokens g√©n√©r√©s par seconde lors de la diffusion de la r√©ponse LLM.\n\n\n\nD√©lai avant le premier rendu de jeton\n\n\n\n\nTaux d‚Äôerreur\nTaux d‚Äôerreur pour diff√©rents types d‚Äôerreurs tels que l‚Äôerreur 401, l‚Äôerreur 429.\n\n\n\nFiabilit√©\nLe pourcentage de demandes satisfaites par rapport au nombre total de demandes, y compris celles qui comportent des erreurs ou des √©checs.\n\n\n\nLatence\nDur√©e moyenne du temps de traitement entre la soumission d‚Äôune requ√™te et la r√©ception d‚Äôune r√©ponse.\n\n\n\nutilisation GPU/CPU\nUtilisation en termes de nombre total de tokens et nombre de code erreur 429 re√ßus (‚ÄòRate limit reached for requests‚Äô dans l‚ÄôAPI OpenAI)\n\n\nCo√ªts\n\n\n\n\n\nCo√ªt des appels LLM\nCe qui est factur√© par le fournisseur du LLM si vous passez par un LLM heberg√© par un tiers\n\n\n\nCo√ªt de l‚Äôinfrastucture\nCo√ªt du stockage, √©nergie, si vous h√©bergez votre LLM\n\n\n\nCo√ªt op√©rationnel\nCo√ªt de la maintenance, du monitoring, de la mise en place des mesures de s√©curit√©, du support, etc si vous h√©bergez votre LLM\n\n\n\n\n\n\n\nA ce jour, il existe des nombreux outils et librairies pour effectuer l‚Äô√©valuation des mod√®les LLM. Chaque de ces librairies et frameworks est taill√©e pour une utilisation de mod√®le concr√®te avec des exemples pour vous aider √† d√©marrer l‚Äô√©valuation de votre mod√®le.\nHugging Face Transformers fournissent des API et des outils pour √©valuer des mod√®les pr√©-entra√Æn√©s sur diff√©rentes t√¢ches en utilisant des m√©triques telles que la pr√©cision, le score F1 ou encore le score BLEU. Ils prennent en charge aussi l‚Äôint√©gration les donn√©es de la biblioth√®que Hugging Face Datasets. https://huggingface.co/docs/transformers/index\nScikit-learn, c‚Äôest un projet Open source avec des librairies principalement ax√©es sur l‚Äôapprentissage automatique traditionnel. Elle comprend de nombreux outils de m√©triques et utilitaires qui peuvent √™tre utilis√©s pour √©valuer les mod√®les de langage. https://scikit-learn.org/stable/\nEvalML est une biblioth√®que sp√©cifique pour l‚Äô√©valuation des mod√®les d‚Äôapprentissage automatique, y compris les LLM. Elle fournit des m√©triques, des visualisations et des outils de s√©lection de mod√®les. https://evalml.alteryx.com/en/stable/\nNLTK et SpaCy - ces deux biblioth√®ques offrent des fonctionnalit√©s pour le traitement du langage naturel et incluent des m√©triques pour √©valuer des t√¢ches telles que la tokenisation, l‚Äôanalyse syntaxique et l‚Äôanalyse des sentiments. https://www.nltk.org/ https://spacy.io/\nAllenNLP est une biblioth√®que con√ßue pour construire et √©valuer des mod√®les de NLP. Elle fournit des outils pour faciliter la mise en ≈ìuvre de mesures d‚Äô√©valuation et de visualisation personnalis√©es. https://docs.allennlp.org/models/main/\nTransformers-Interpret Une biblioth√®que qui se concentre sur l‚Äôinterpr√©tabilit√© des mod√®les, permettant de mieux comprendre les pr√©dictions et les performances des mod√®les. https://pypi.org/project/transformers-interpret/0.3.0/\nLangChain est principalement destin√©e √† la construction d‚Äôapplications avec des LLM. Elle comprend des outils d‚Äô√©valuation pour √©valuer la performance des mod√®les de langage dans le contexte. https://www.langchain.com/\nOpenAI Evals est une bo√Æte √† outils d‚Äô√©valuation de l‚ÄôOpenAI qui fournit les outils et lignes directrices pour √©valuer la performance et la s√©curit√© de leurs mod√®les. https://github.com/openai/evals\nAutres sources : ‚Ä¶\n\n\n\n\nUn arbre de d√©cision pour l‚Äô√©valuation des LLM peut vous aider √† guider votre processus d‚Äô√©valuation en fonction de crit√®res et d‚Äôobjectifs sp√©cifiques de votre mod√®le et son √©valuation. Voici un exemple de tel arbre de d√©cision qui pourrait vous aider en cas de doute. v1",
    "crumbs": [
      "II-D√©veloppements",
      "Evaluations"
    ]
  },
  {
    "objectID": "III-Deploiements/1_Architecture_projet_llm.html",
    "href": "III-Deploiements/1_Architecture_projet_llm.html",
    "title": "3. Infrastructures externalis√©es pour l‚Äôadministration",
    "section": "",
    "text": "Dans la plupart des cas, un projet d‚ÄôIA g√©n√©rative ne se limite pas au d√©ploiement d‚Äôun LLM. Souvent, le projet n√©cessitera la pr√©sence d‚Äôune interface utilisateur, et parfois m√™me d‚Äôune architecture RAG.\nLes diff√©rents √©lements du d√©veloppement d‚Äôune application d‚ÄôIA g√©n√©rative sont donn√©s dans le rapport de l‚ÄôANSSI sur l‚ÄôIA g√©n√©rative :\n\nPour le d√©ploiement, les principales briques peuvent se d√©composent en 3 grandes familles :\n\nService LLM\nInterface utilisateur\nGestion d‚Äôune base de connaissance (RAG)\n\n\n\nLe service LLM est la brique qui permet de faire tourner un mod√®le de language. Il peut √™tre d√©ploy√© sur une infrastructure en propre, ou externalis√©.\n\n\nDans le cas o√π il est possible d‚Äôavoir acc√®s √† des GPUs, il existe de nombreux outils pour d√©ployer des LLMs sur sa propre infrastructure. Ces outils permettent de d√©ployer √† de multiples √©chelles des mod√®les LLM libres.\nCette option est d√©taill√©e dans les parties Service LLM avanc√© et Service LLM √† grande √©chelle.\n\n\n\nCette option pr√©sente l‚Äôint√©r√™t d‚Äô√™tre pr√™te √† l‚Äôemploi et de ne pas n√©cessiter de poss√©der infrastructure d√©di√©e.\n\nAPI Albert : Offre fournie par la DINUM pour les administrations. \nExterne non s√©curis√©e : Ces solutions sont envisageables o√π les besoins en performance sont importants et les contraintes de s√©curit√© sont faibles. Voici quelques exemples de solutions :\n\nMistral API\nAPI Openai\nHugging face - inference endpoint\nAPI Claude - Anthropic\n\n\n\n\n\nollama permet le d√©ploiement en local de nombreux mod√®les d‚ÄôIA g√©n√©rative (LLM, multimodal LLM, embedings, Code completion, ‚Ä¶), √† l‚Äôaide d‚Äôune unique ligne de commande :\nollama run llama3.2\nPour permettre l‚Äôex√©cution dans le plus de contexte possible, ollama donne le choix de la taille du mod√®le et du niveau de quantization.\nollama run gemma2:27b-instruct-q2_K\n\nollama n‚Äôest pas adapt√© dans les cas o√π le projet a des contraintes de performance et/ou doit g√©rer les requ√™tes simultan√©es de plusieurs utilisateurs.\n\n\n\n\n\nDans le cas o√π le projet n√©cessite l‚Äôutilisation de Retrieval Augmented Generation (RAG), il est n√©cessaire de mettre en place une infrastructure permettant de stocker et de r√©cup√©rer les donn√©es n√©cessaires au mod√®le.\nLes architectures RAG se concentrent principalement autour d‚Äôune base de donn√©es vectorielles (cf Benchmark des diff√©rentes bases vectorielles.\nUn exemple d‚Äôinfrastructure pour un projet RAG est donn√© dans le cadre du projet CARADOC (cf. d√©ploiement CARADOC):\n\n\n\n\nL‚Äôinterface utilisateur est la brique qui permet √† l‚Äôutilisateur d‚Äôinteragir avec le mod√®le et/ou les r√©sultats du mod√®le. Elle peut √™tre d√©velopp√©e sp√©cifiquement pour le projet, ou utiliser un outil existant.\nPour des applications simples, streamlit et Gradio propose des interfaces de chat. Un exemple d‚Äôoutil d√©velopp√© et d√©ploy√© avec Gradio est Compar:IA.\nDans les cas o√π les int√©ractions sont plus complexes, il peut √™tre n√©cessaire de passer sur une application plus lourde. Des exemples d‚Äôapplications sont abord√©es dans la partie D√©ploiement d‚Äôapplications.",
    "crumbs": [
      "III-Deploiements",
      ":construction: Architecture d'un projet LLM"
    ]
  },
  {
    "objectID": "III-Deploiements/1_Architecture_projet_llm.html#architecture-de-projets-llm",
    "href": "III-Deploiements/1_Architecture_projet_llm.html#architecture-de-projets-llm",
    "title": "3. Infrastructures externalis√©es pour l‚Äôadministration",
    "section": "",
    "text": "Dans la plupart des cas, un projet d‚ÄôIA g√©n√©rative ne se limite pas au d√©ploiement d‚Äôun LLM. Souvent, le projet n√©cessitera la pr√©sence d‚Äôune interface utilisateur, et parfois m√™me d‚Äôune architecture RAG.\nLes diff√©rents √©lements du d√©veloppement d‚Äôune application d‚ÄôIA g√©n√©rative sont donn√©s dans le rapport de l‚ÄôANSSI sur l‚ÄôIA g√©n√©rative :\n\nPour le d√©ploiement, les principales briques peuvent se d√©composent en 3 grandes familles :\n\nService LLM\nInterface utilisateur\nGestion d‚Äôune base de connaissance (RAG)\n\n\n\nLe service LLM est la brique qui permet de faire tourner un mod√®le de language. Il peut √™tre d√©ploy√© sur une infrastructure en propre, ou externalis√©.\n\n\nDans le cas o√π il est possible d‚Äôavoir acc√®s √† des GPUs, il existe de nombreux outils pour d√©ployer des LLMs sur sa propre infrastructure. Ces outils permettent de d√©ployer √† de multiples √©chelles des mod√®les LLM libres.\nCette option est d√©taill√©e dans les parties Service LLM avanc√© et Service LLM √† grande √©chelle.\n\n\n\nCette option pr√©sente l‚Äôint√©r√™t d‚Äô√™tre pr√™te √† l‚Äôemploi et de ne pas n√©cessiter de poss√©der infrastructure d√©di√©e.\n\nAPI Albert : Offre fournie par la DINUM pour les administrations. \nExterne non s√©curis√©e : Ces solutions sont envisageables o√π les besoins en performance sont importants et les contraintes de s√©curit√© sont faibles. Voici quelques exemples de solutions :\n\nMistral API\nAPI Openai\nHugging face - inference endpoint\nAPI Claude - Anthropic\n\n\n\n\n\nollama permet le d√©ploiement en local de nombreux mod√®les d‚ÄôIA g√©n√©rative (LLM, multimodal LLM, embedings, Code completion, ‚Ä¶), √† l‚Äôaide d‚Äôune unique ligne de commande :\nollama run llama3.2\nPour permettre l‚Äôex√©cution dans le plus de contexte possible, ollama donne le choix de la taille du mod√®le et du niveau de quantization.\nollama run gemma2:27b-instruct-q2_K\n\nollama n‚Äôest pas adapt√© dans les cas o√π le projet a des contraintes de performance et/ou doit g√©rer les requ√™tes simultan√©es de plusieurs utilisateurs.\n\n\n\n\n\nDans le cas o√π le projet n√©cessite l‚Äôutilisation de Retrieval Augmented Generation (RAG), il est n√©cessaire de mettre en place une infrastructure permettant de stocker et de r√©cup√©rer les donn√©es n√©cessaires au mod√®le.\nLes architectures RAG se concentrent principalement autour d‚Äôune base de donn√©es vectorielles (cf Benchmark des diff√©rentes bases vectorielles.\nUn exemple d‚Äôinfrastructure pour un projet RAG est donn√© dans le cadre du projet CARADOC (cf. d√©ploiement CARADOC):\n\n\n\n\nL‚Äôinterface utilisateur est la brique qui permet √† l‚Äôutilisateur d‚Äôinteragir avec le mod√®le et/ou les r√©sultats du mod√®le. Elle peut √™tre d√©velopp√©e sp√©cifiquement pour le projet, ou utiliser un outil existant.\nPour des applications simples, streamlit et Gradio propose des interfaces de chat. Un exemple d‚Äôoutil d√©velopp√© et d√©ploy√© avec Gradio est Compar:IA.\nDans les cas o√π les int√©ractions sont plus complexes, il peut √™tre n√©cessaire de passer sur une application plus lourde. Des exemples d‚Äôapplications sont abord√©es dans la partie D√©ploiement d‚Äôapplications.",
    "crumbs": [
      "III-Deploiements",
      ":construction: Architecture d'un projet LLM"
    ]
  },
  {
    "objectID": "III-Deploiements/1_Architecture_projet_llm.html#pistes-dinfrastructure",
    "href": "III-Deploiements/1_Architecture_projet_llm.html#pistes-dinfrastructure",
    "title": "3. Infrastructures externalis√©es pour l‚Äôadministration",
    "section": "Pistes d‚Äôinfrastructure",
    "text": "Pistes d‚Äôinfrastructure\nDans beaucoup de cas l‚Äôacc√®s √† des GPUs est un des principaux freins √† l‚Äôexp√©rimentaion et la mise en production d‚Äôun cas d‚Äôusage d‚ÄôIA g√©n√©rative. L‚Äôacquisition d‚Äôun cluster GPUs n‚Äôest pas toujours une possibilit√© pour des questions budg√©taires ou techniques. Cependant, plusieurs alternatives sont envisageables (ou en cours de construction) par les administrations pour externaliser cette infrastructure.\nDans ce cadre, les principales variables √† prendre en compte sont les contraintes de s√©curit√© de l‚Äôapplication. Cette question va √† la fois d√©terminer les solutions accessibles et imposer des choix architecturaux.\n2 principales solutions d‚Äôexternalisation sont possibles :\n\nCloud Public\nCloud externe\n\n\nDans certains cas, il peut √™tre int√©ressant de mettre en place une architecture hybride Cloud + API d‚Äôinf√©rence. Ce qui permet de b√©n√©ficier de l‚Äôagilit√© de d√©veloppement des solutions Cloud, tout en r√©duisant les co√ªts relatifs √† l‚Äôapprovisionnement de GPUs.\n\n\nCloud Public\n\nSSP Cloud : A ce jour, le SSP Cloud via sa plateforme ONYXIA (h√©berg√©e et d√©velop√©e par l‚ÄôINSEE), est la principale plateforme publique mettant √† disposition des GPUs √† ses utilisateurs. Les ressources sont cependant tr√®s limit√©es et la plateforme est plus orient√©e autour du d√©veloppement de projet que de la mise en production. cf D√©ploiement d‚Äôun LLM sur SSP Cloud\nCloud pi : Cloud PI est le cloud du minist√®re de l‚Äôint√©rieur, il ne semble pas proposer √† date de provisionnement de GPUs. Il fournit cependant une offre IAAS (Infrastructure As A Service) et PAAS (Platform As A Service) pour l‚Äôh√©bergement d‚Äôapplications.\nNubo : Nubo est le service cloud du minist√®re de l‚Äô√©conomie et des finances, qui propose un service IAAS. Via sa solution Nubonyxia, il est possible d‚Äôint√©ragir avec l‚Äôinterface Onyxia et le service Kubernetes sous jacents. Nubo ne propose pas √† date de provisionnement de GPUs.\n\n\nPour d√©finition de ce que recouvre les offres de service PAAS et IAAS, se r√©f√©rer √† ce lien\n\n\n\nCloud externe\nLa qualificiation SecNumCloud a √©t√© mis en place par l‚ÄôANSSI pour assurer des normes de s√©curit√© aux utilisateurs de produits cloud. A ce jour, peu d‚Äôentreprises ont acquis cette qualification. Voici quelques exemples de fournisseurs :\n\nDassault - Outscale IAAS avec acc√®s GPU\nThales - Sens (Impl√©mentation de GCP sur une infrastructure s√©curis√©e) PAAS\nCloud Temple IAAS \n\n\nPlus d‚Äôinformations sur ce type de services sont disponibles ici.",
    "crumbs": [
      "III-Deploiements",
      ":construction: Architecture d'un projet LLM"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Service_LLM_production.html",
    "href": "III-Deploiements/3_Service_LLM_production.html",
    "title": "4. Service LLM √† grande √©chelle",
    "section": "",
    "text": "Les LLM sont des mod√®les de langage puissants qui n√©cessitent des ressources informatiques importantes pour fonctionner efficacement. Pour mettre en production une application utilisant des LLM, il est essentiel de choisir le bon mat√©riel et les bons outils pour garantir la disponibilit√© des applications et des performances optimales.\nPour certains mod√®les de langage, les processeurs habituels appel√©s CPU (Central Processing Unit) peuvent suffire. Mais pour la plupart des mod√®les plus importants, pour que les calculs se fassent dans des temps raisonnables, il est n√©cessaire de se doter d‚Äôunit√©s de traitement graphique (GPU). Nous allons donc nous int√©resser ici aux diff√©rents crit√®res √† √©tudier pour choisir correctement des GPUs, aux outils qui permettent de suivre leurs performances et enfin le lien avec les essentiels de d√©ploiements en termes de management de ressources (avec l‚Äôexemple du lien √† Kubernetes).",
    "crumbs": [
      "III-Deploiements",
      ":factory: Service LLM √† grande √©chelle"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Service_LLM_production.html#gpu-pour-les-llm",
    "href": "III-Deploiements/3_Service_LLM_production.html#gpu-pour-les-llm",
    "title": "4. Service LLM √† grande √©chelle",
    "section": "GPU pour les LLM",
    "text": "GPU pour les LLM\nLa s√©lection des GPU (Graphics Processing Units) pour une installation dans une structure d√©pend de multiples facteurs. En voici quelques uns :\n\nPuissance de calcul : La puissance de traitement des GPU est mesur√©e en flops (floating-point operations per second). Un GPU plus puissant permettra d‚Äôex√©cuter des t√¢ches plus rapidement et de g√©rer des charges de travail plus √©lev√©es.\nM√©moire vive : La m√©moire vive (VRAM) des GPU est essentielle pour les applications n√©cessitant une grande quantit√© de m√©moire, comme les simulations scientifiques ou les applications de traitement d‚Äôimage. Assurez-vous de choisir des GPU avec suffisamment de m√©moire vive pour r√©pondre aux besoins de vos applications.\n√ânergie et consommation : Les GPU consomment de l‚Äô√©nergie et g√©n√®rent de la chaleur. Choisissez des GPU √©conomes en √©nergie et dot√©s de syst√®mes de refroidissement efficaces pour r√©duire les co√ªts √©nerg√©tiques et am√©liorer la dur√©e de vie des composants.\nCo√ªt et rentabilit√© : √âvaluez le co√ªt total de possession (TCO) des GPU, en tenant compte des co√ªts d‚Äôachat, de maintenance et d‚Äô√©nergie. Choisissez des GPU qui offrent une bonne rentabilit√© pour votre administration.\nCompatibilit√© avec les syst√®mes d‚Äôexploitation : V√©rifiez que les GPU sont compatibles avec les syst√®mes d‚Äôexploitation utilis√©s dans votre administration, tels que Windows, Linux ou macOS.\n\nPour le dernier point, il est commun d‚Äôacheter les GPUs par lots, d√©j√† group√©s dans des serveurs. Il faut faire attention cependant au format et aux besoins sp√©cifiques de ces serveurs, qui ne sont souvent pas standards par leur taille et par la chaleur qu‚Äôils d√©gagent.\nDes GPUs reconnus peuvent √™tre les T5, A100, V100 et leur prix d‚Äôachat est de l‚Äôordre de milliers d‚Äôeuros, mais il faut bien prendre en compte √©galement les co√ªts cach√©s. En effet, l‚Äôint√©gration dans un SI pr√©-existant peut n√©cessiter des travaux. Durant leur cycle de vie, ils ont besoin de maintenance. Et enfin, tout au long de leur utilisation, ils ont besoin d‚Äô√™tre administr√©s, ce qui peut repr√©senter des Equivalents Temps Plein (ETP), dont le co√ªt n‚Äôest pas √† n√©gliger.",
    "crumbs": [
      "III-Deploiements",
      ":factory: Service LLM √† grande √©chelle"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Service_LLM_production.html#orchestration-avec-des-gpus",
    "href": "III-Deploiements/3_Service_LLM_production.html#orchestration-avec-des-gpus",
    "title": "4. Service LLM √† grande √©chelle",
    "section": "Orchestration avec des GPUs",
    "text": "Orchestration avec des GPUs\nIl est judicieux d‚Äôutiliser un orchestrateur pour d√©ployer des Language Models (LLMs) dans une organisation pour plusieurs raisons :\n\nSimplification de la gestion des d√©ploiements : un orchestrateur permet de g√©rer de mani√®re centralis√©e tous les d√©ploiements de LLMs dans l‚Äôorganisation. Cela facilite la surveillance, la maintenance et la mise √† l‚Äô√©chelle des d√©ploiements.\n√âvolutivit√© : un orchestrateur permet de mettre √† l‚Äô√©chelle automatiquement les d√©ploiements en fonction de la demande, ce qui est particuli√®rement utile pour les LLMs qui peuvent √™tre tr√®s gourmands en ressources.\nS√©curit√© : un orchestrateur peut aider √† renforcer la s√©curit√© en fournissant des fonctionnalit√©s telles que l‚Äôauthentification, l‚Äôautorisation et le chiffrement des donn√©es. Il peut √©galement aider √† respecter les normes de conformit√© en mati√®re de traitement des donn√©es.\nGestion des versions : un orchestrateur permet de g√©rer les versions des LLMs et de faciliter le d√©ploiement de nouvelles versions ou de rollbacks en cas de probl√®me.\nInt√©gration avec d‚Äôautres outils : un orchestrateur peut s‚Äôint√©grer facilement avec d‚Äôautres outils de d√©veloppement et d‚Äôexploitation, tels que les syst√®mes de surveillance, les outils de d√©bogage et les syst√®mes de journalisation.\nR√©duction des co√ªts : en automatisant les d√©ploiements et en les mettant √† l‚Äô√©chelle de mani√®re efficace, un orchestrateur peut aider √† r√©duire les co√ªts associ√©s aux d√©ploiements de LLMs.\n\nEn r√©sum√©, un orchestrateur offre une gestion centralis√©e, une √©volutivit√©, une s√©curit√© renforc√©e, une gestion des versions, une int√©gration avec d‚Äôautres outils et une r√©duction des co√ªts pour les d√©ploiements de LLMs dans une organisation. Des solutions techniques peuvent √™tre :\n\nKubernetes\nDocker Swarm\nApache Mesos",
    "crumbs": [
      "III-Deploiements",
      ":factory: Service LLM √† grande √©chelle"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Service_LLM_production.html#outils-pour-surveiller-les-performances-des-gpu",
    "href": "III-Deploiements/3_Service_LLM_production.html#outils-pour-surveiller-les-performances-des-gpu",
    "title": "4. Service LLM √† grande √©chelle",
    "section": "Outils pour surveiller les performances des GPU",
    "text": "Outils pour surveiller les performances des GPU\nUne fois l‚Äôinfrastructure s√©curis√©e, il est toujours utile de monitorer les performances des GPU, pour suivre l‚Äôimpact de cette technologie, pour monitorer la charge et pr√©venir de la surcharge. Id√©alement, l‚Äôon peut aussi imaginer suivre la consommation projet par projet pour reporter les lignes de budget et faire des bilans carbonne.\nSelon les technologies de GPUs utilis√©es, il existe diff√©rents outils qui se connectent aux infrastructure pour fournir des statistiques (notamment la m√©moire utilis√©e, la bande passante et la temp√©rature) :\n\nnvidia-smi\nAMD Vantage\nGPU-Z\n\nVoici un exemple de r√©sultat de statistiques extraites d‚Äôune infrastructure GPUs :\n\n\n\nResultat de la commande nvidia-smi\n\n\nIl existe √©galement d‚Äôautres moyens d‚Äôacc√©der √† des GPUs que l‚Äôacquisition individuelle pour les administrations (voir Partie III.4).",
    "crumbs": [
      "III-Deploiements",
      ":factory: Service LLM √† grande √©chelle"
    ]
  },
  {
    "objectID": "III-Deploiements/3_Service_LLM_production.html#exemple-de-d√©ploiement",
    "href": "III-Deploiements/3_Service_LLM_production.html#exemple-de-d√©ploiement",
    "title": "4. Service LLM √† grande √©chelle",
    "section": "Exemple de d√©ploiement",
    "text": "Exemple de d√©ploiement\nNous allons d√©velopper dans cette partie un exemple de d√©ploiement d‚Äôune structure LLM avec Kubernetes. On utilise la m√™me structure de microservices que dans la partie pr√©cedente avec FastChat mais cela peut √™tre adapt√© √† tout choix d‚Äôorganisation et d‚Äôarchitecture.\nVoici un sch√©ma r√©sumant l‚Äôorganisation propos√©e ici, avec le controller, l‚Äôapi openai-like et deux mod√®les LLMs :\n\n\n\nSch√©ma de structure des services pour Kubernetes\n\n\nLa m√©thodologie g√©n√©rale de l‚Äôutilisation de Kubernetes est la suivante :\n\nPr√©parer les images Docker qui seront utilis√©es pour les d√©ploiements\nCr√©ez les fichiers de configuration YAML pour votre application\nD√©ployez les avec :\n\nkubectl apply -f FILENAMES.yaml\n\nSurveiller le lancement des diff√©rents services et leur bonne interconnexion\n\nAvec cela, vous avez une application plus robuste, mais cela necessite une certaine familiarit√© avec Kubernetes. Quelques exemples de fichiers de configuration sont propos√©s ci-dessous.\n\nTout d‚Äôabord les services obligatoires comme le gestionnaire de l‚ÄôAPI et le controlleur. On fait en m√™me temps le deployment du pod et le service permettant d‚Äôy acc√©der. Ils se basent sur une image Docker l√©g√®re et sans requirements sp√©cifiques.\n\nOn remarquera que les deux deploiments semblent assez similaires et que la principale diff√©rence r√©side dans les noms donn√©s aux objets et √† la commande lanc√©e dans le conteneur lanc√© :\n[\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\nou\n[\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1\", \"--controller-address\", \"http://svc-controller\"]\nCes commandes sont celles de FastChat mais peuvent √™tre remplac√©es par votre propre solution de d√©ploiement de mod√®le.\nPour le controlleur :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-controller\n  template:\n    metadata:\n      labels:\n        app: fastchat-controller\n    spec:\n      containers:\n      - name: fastchat-controller\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 21001\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-llm-mixtral,svc-llm-e5-dgfip,svc-llm-llama\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.controller\", \"--host\", \"0.0.0.0\", \"--port\", \"21001\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-controller2\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30001\n      port: 80\n      targetPort: 21001\n  selector:\n    app: fastchat-controller\nEt pour l‚Äôapi :\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fastchat-openai\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: fastchat-openai\n  template:\n    metadata:\n      labels:\n        app: fastchat-openai\n    spec:\n      containers:\n      - name: fastchat-openai\n        image: llm-api-light:1.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8000\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller,svc-llm-mixtral\n        command: [\"python3.9\", \"-m\", \"fastchat.serve.openai_api_server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--api-keys\", \"key1\", \"--controller-address\", \"http://svc-controller\"]\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-openai-api\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30081\n      port: 80\n      targetPort: 8000\n  selector:\n    app: fastchat-openai\n\nCr√©ez les fichiers de configuration pour un mod√®le LLM, avec √©galement le pod et le service correspondant. Cette fois-ci, l‚Äôimage est plus lourde car elle contient le mod√®le et les modules n√©cessaires √† son fonctionnement.\n\nOn remarquera notamment :\nimage: fastchat-mixtral:v0.3.1\nresources:\n          limits:\n            nvidia.com/gpu: 2\nEt la commande qui lance le mod√®le (ici Fastchat mais pourrait √™tre n‚Äôimporte quel module):\ncommand: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: llm-mixtral\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: llm-mixtral\n  template:\n    metadata:\n      labels:\n        app: llm-mixtral\n    spec:\n      containers:\n      - name: llm-mixtral\n        image: fastchat-mixtral:v0.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 2100\n        env:\n        - name: no_proxy\n          value: localhost,127.0.0.1,0.0.0.0,svc-controller\n        - name: CUDA_VISIBLE_DEVICES\n          value: \"0,1\"\n        command: [\"python3\", \"-m\", \"fastchat.serve.vllm_worker\", \"--model-path\", \"/data/models/vllm/Mixtral-8x7B-Instruct-v0.1\", \"--worker-address\", \"http://svc-llm-mixtral\", \"--host\", \"0.0.0.0\", \"--port\", \"2100\", \"--controller\", \"http://svc-controller\", \"--trust-remote-code\", \"--model-names\", \"mixtral-instruct\", \"--num-gpus\", \"2\"]\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n      imagePullSecrets:\n      - name : regcred\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-llm-mixtral\nspec:\n  type: NodePort\n  ports:\n    - nodePort: 30091\n      port: 80\n      targetPort: 2100\n  selector:\n    app: llm-mixtral\nEnfin, tous ces composants se basent sur des images docker qui continennent tout le code de mise √† disposition des mod√®les ou des APIs. Des exemples d‚Äôimages utiles pour les diff√©rents services sus-mentionn√©s sont d√©crites dans ce Dockerfile :\n#################### BASE OPENAI IMAGE ####################\nFROM python:3.9-buster as llm-api-light\n\n# Set environment variables\nENV DEBIAN_FRONTEND noninteractive\n# Install dependencies\nRUN apt-get update -y && apt-get install -y curl\n# Install pip\nRUN curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\nRUN python3.9 get-pip.py\n# Copy the FastChat directory into the Docker container\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n# Go into the FastChat directory and install from this directory\nWORKDIR /FastChat\nRUN pip3 install -e \".[webui]\" pydantic==1.10.13\nRUN pip3 install plotly\n\n#################### BASE LLM BUILD IMAGE ####################\nFROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base\nENV DEBIAN_FRONTEND noninteractive\nRUN apt-get update -y && apt-get install -y python3.9 curl\nRUN apt-get install -y python3-pip git\n# Copiez le r√©pertoire FastChat dans le conteneur DockerEnfin, tous c\n\nWORKDIR /\nRUN git clone -n https://github.com/lm-sys/FastChat.git  && \\\n    cd FastChat  && \\\n    git checkout ed6735d\n\n# Allez dans le r√©pertoire FastChat et installez √† partir de ce r√©pertoire\nWORKDIR /FastChat\nRUN pip3 install --upgrade pip\nRUN pip3 install -e \".[model_worker]\" pydantic==1.10.13\nRUN pip3 install plotly==5.18.0\nRUN pip3 install accelerate==0.25.0\nRUN pip3 install vllm==0.3.1\nRUN pip3 install minio==7.2.2\nRUN pip3 install pynvml==11.5.0\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-llama\nCOPY ./models/Upstage--Llama-2-70b-instruct-v2 /data/models/vllm/Upstage--Llama-2-70b-instruct-v2\n\n#################### EXTENSION LLAMA  ####################\nFROM base AS llm-mixtral\nCOPY ./models/Mixtral-8x7B-Instruct-v0.1 /data/models/vllm/Mixtral-8x7B-Instruct-v0.1",
    "crumbs": [
      "III-Deploiements",
      ":factory: Service LLM √† grande √©chelle"
    ]
  }
]