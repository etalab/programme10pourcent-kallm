---
title: Bibliographie
---

## I - Accompagnement

### Généralité

- [Qu'est ce que l'IA ?](https://www.technologyreview.com/2024/07/10/1094475/what-is-artificial-intelligence-ai-definitive-guide/)

- [What We Learned from a Year of Building with LLMs (Part I)](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/)
- [What We Learned from a Year of Building with LLMs (Part II)](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-ii/)

### Guides pour la prise en compte de l'impact de l'IA

- [ARCEP & ADEME de 2023](https://www.arcep.fr/uploads/tx_gspublication/note-synthese-au-gouvernement-prospective-2030-2050_mars2023.pdf)
- [AFNOR IA frugale](https://normalisation.afnor.org/nos-solutions/afnor-spec/intelligence-artificielle-frugale/)
- [Guide de recommandation de sécurité de l'ANSSI](https://cyber.gouv.fr/publications/recommandations-de-securite-pour-un-systeme-dia-generative)

- [AI Act](https://artificialintelligenceact.eu/fr/high-level-summary/)

## II - Développement

### Plateforme de partage de modèles

- [HuggingFace](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)

### Articles de recherche centraux

**Transformers**

- [Papier original **'Attention Is All You Need'**](https://arxiv.org/abs/1706.03762)
- [Explication illustrée et très détaillée](http://jalammar.github.io/illustrated-transformer/)
- [Les différents types de modèles](https://medium.com/artificial-corner/discovering-llm-structures-decoder-only-encoder-only-or-decoder-encoder-5036b0e9e88)
- [Les Mixture of Experts](https://huggingface.co/blog/moe)

**Fine-tuning**

- [LoRA](https://arxiv.org/abs/2106.09685)
- [QLoRA](https://arxiv.org/abs/2305.14314)
- [DoRA](https://arxiv.org/abs/2402.09353)

- [Introduction au RLHF](https://huggingface.co/blog/rlhf)
- [DPO](https://arxiv.org/abs/2305.18290)
- [KTO](https://arxiv.org/abs/2402.01306)

**Bonnes pratiques du prompt engineering**

- [Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4](https://arxiv.org/abs/2312.16171)
- [Graph of Thoughts](https://arxiv.org/pdf/2308.09687)

**Evaluation (métriques)**

| Basée sur embeddings | Basée sur modèle fine-tuné  | Basé sur LLM |
|--|--|--|
| [BERTScore](https://arxiv.org/abs/1904.09675) |[UniEval](https://arxiv.org/abs/2210.07197) | [G-Eval](https://arxiv.org/abs/2303.16634)|
|[MoverScore](https://arxiv.org/abs/1909.02622)   | [Lynx](https://www.patronus.ai/blog/lynx-state-of-the-art-open-source-hallucination-detection-model)   |   [GPTScore](https://arxiv.org/abs/2302.04166)|
| |  [Prometheus-eval](https://github.com/prometheus-eval/prometheus-eval)  |  |

**Evaluation (frameworks)**
- [Ragas](https://github.com/explodinggradients/ragas) (spécialisé pour le RAG)
- [Ares](https://github.com/stanford-futuredata/ARES) (spécialisé pour le RAG)
- [Giskard](https://github.com/Giskard-AI/giskard)
- [DeepEval](https://github.com/confident-ai/deepeval) 

**Evaluation (RAG)**
- [Evaluation of Retrieval-Augmented Generation: A Survey](https://arxiv.org/abs/2405.07437)
- [Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation](https://arxiv.org/abs/2405.13622)


**Evaluation (divers)**
- [Prompting strategies for LLM-based metrics](https://arxiv.org/abs/2311.03754)
- [LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/abs/2402.01383)
- [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://arxiv.org/abs/2306.05685)


### Librairies et ressources


**LLM platform**
- [Ollama](https://ollama.com/)

**Pipelines et orchestration LLM**
- [LangChain](https://www.langchain.com/)
- [LlamaIndex](https://www.llamaindex.ai/)
- [Haystack](https://haystack.deepset.ai/)

**RAG**
- [Graph RAG](https://microsoft.github.io/graphrag/)

**Evaluation**
- [SelfCheckGPT](https://github.com/potsawee/selfcheckgpt)
