# Guide du LLM

## PARTIE I. Accompagnement au changement

### 4. Impacts (Katia Bruno Hélène)

	a. Environnementaux (poids de l’entraînement, poids de l’utilisation)

Le numérique est responsable de 2,5% de l’empreinte carbone de la France (17,2 Mt de CO2e & 20 millions de tonnes de déchets) selon l’étude [ARCEP & ADEME de 2023](https://www.arcep.fr/uploads/tx_gspublication/note-synthese-au-gouvernement-prospective-2030-2050_mars2023.pdf). Par contre, il n’existe aucun référentiel à ce jour pour mesurer l’impact environnemental des projets d’intelligence artificielle. À titre d'exemple, les émissions liées à l'entraînement de GPT-3 sont estimées à 552 tonnes de CO2eq [1] et son utilisation en janvier 2023 représenterait 10 113 tonnes de CO2eq [2]. Les ressources en eau, métaux et d'autres matériaux pour la fabrication et opération des infrastructures sont également conséquents. 

Afin de permettre aux acteurs du numérique d’évaluer l’impact environnemental de leurs projets d’intelligence artificielle, et de communiquer sur le caractère frugal de ces derniers, l'Ecolab du MTECT prépare avec l'AFNOR un [document de référence](https://normalisation.afnor.org/nos-solutions/afnor-spec/intelligence-artificielle-frugale/), qui devra être disponible en juillet.

À l'heure actuelle, pour estimer la consommation énergétique et les émissions de CO2 liées à l’exécution du code, les data-scientists peuvent utiliser la librairie [CodeCarbon](https://github.com/mlco2/codecarbon), à mettre en place avant l'usage, et/ou [Green Algorithms](https://www.green-algorithms.org/), qui peut être utilisé pour estimer un usage futur ou passé.

Le coût environnementale lié aux infrastructures de calcul est mis à disposition par le groupe EcoInfo du CNRS à travers l'outil [EcoDiag](https://ecoinfo.cnrs.fr/ecodiag-calcul/). Des estimations plus précises pour la fabrication de GPUs seront disponibles prochainement.

[1] https://arxiv.org/pdf/2104.10350.pdf

[2] [Data For Good - Livre Blanc de l'IA Générative](https://issuu.com/dataforgood/docs/dataforgood_livreblanc_iagenerative_v1.0?fr=sZGE0MjYyNjE5MTU)


 
	b. Légaux (RGPD, chartes de l’IA, IA Act, ...)

La sécurité des données personnelles et des modèles est un enjeu considérable, que ce soit du point de vue personnel ou à l'échelle de l'administration. Par exemple, quand les modèles ne sont pas auto-hébergés, les entreprises qui les fournissent ont accès aux conversations tenus avec les chatbots. De plus ces données sont réutilisées pour l'entraînement et peuvent ressortir lors de conversations avec d'autres utilisateurs.

La CNIL propose une série de recommandations concenrant le développement de système d'IA impliquant un traitement des données personnelles, notamment en insistant sur la définition des finalités du traitement et sur prise en compte de la [base légale du RGPD](https://www.cnil.fr/fr/les-bases-legales/liceite-essentiel-sur-les-bases-legales) qui autorise à traiter des données personnelles. Dans le cas d'une administration publique, cette base légale pourra être par exemple selon les cas l'obligation légale, la mission d'intérêt public ou l'intérêt légitime.

Au niveau européen, le [règlement (UE) 2024/1689 du Parlement européen et du Conseil du 13 juin 2024 établissant des règles harmonisées concernant l'intelligence artificielle](https://eur-lex.europa.eu/legal-content/FR/TXT/?uri=OJ:L_202401689) ou "AI Act" est le premier acte législatif européen sur l'IA. Il établit notamment des règles harmonisées concernant la mise sur le marché, mise en service et utilisation de systèmes d'IA dans l'UE, avec l'interdiction de certaines pratiques, comme la notation sociale, l'évaluation des risques de commettre des infractions ou la création de bases de données de reconnaissance faciale non ciblées. Une gradation est déterminée selon le niveau de risque, avec des systèmes d'IA à faible ou moyen risque, des systèmes à haut risque, associés à des exigences spécifiques, par exemple lorsqu'ils traitent des données personnelles, et des pratiques interdites.

Pour aller plus loin : 
- [Guide de la CNIL](https://www.cnil.fr/fr/ia-la-cnil-publie-ses-premieres-recommandations-sur-le-developpement-des-systemes-dintelligence)
- [Résumé haut niveau de l'AI Act](https://artificialintelligenceact.eu/fr/high-level-summary/)
 
	c. Sécurité (Renvoyer vers le guide de l’ANSSI)

En plus de la sécurisation commune aux applications produites par l'administration, certains sujets sont spécifiques aux modèles d'IA. L'ANSSI a écrit à ce sujet un guide de recommandations de sécurité pour sensibiliser aux risques et promouvoir les bonnes pratiques lors de la création et de la mise en production d'applications comportant des modèles d'IA générative.

Trois catégories d'attaque spécifiques au système d'IA générative sont identifiées :

- les attaques par manipulation, au moyen de requêtes malveillantes;
- les attaques par infection, en contaminant les données lors de la phase d'entraînement ("model poisoning");
- les attaques par exfiltration, qui visent à obtenir des informations sur le modèle en production, comme les données d'entraînement ou les paramètres.

Les recommandations produites concernent à la fois les phases d'entraînement, de déploiement et de mise en production.

Pour aller plus loin : [Guide de l'ANSSI](https://cyber.gouv.fr/publications/recommandations-de-securite-pour-un-systeme-dia-generative)